{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#hello","title":"\ud83d\udc4b Hello","text":"<p>We write your reusable computer vision tools. Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us!</p>"},{"location":"#install","title":"\ud83d\udcbb Install","text":"<p>You can install <code>supervision</code> with pip in a Python&gt;=3.8 environment.</p> <p>pip install (recommended)</p> headlessdesktop <p>The headless installation of <code>supervision</code> is designed for environments where graphical user interfaces (GUI) are not needed, making it more lightweight and suitable for server-side applications.</p> <pre><code>pip install supervision\n</code></pre> <p>If you require the full version of <code>supervision</code> with GUI support you can install the desktop version. This version includes the GUI components of OpenCV, allowing you to display images and videos on the screen.</p> <pre><code>pip install supervision[desktop]\n</code></pre> <p>git clone (for development)</p> virtualenvpoetry <pre><code># clone repository and navigate to root directory\ngit clone https://github.com/roboflow/supervision.git\ncd supervision\n\n# setup python environment and activate it\npython3 -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\n\n# headless install\npip install -e \".\"\n\n# desktop install\npip install -e \".[desktop]\"\n</code></pre> <pre><code># clone repository and navigate to root directory\ngit clone https://github.com/roboflow/supervision.git\ncd supervision\n\n# setup python environment and activate it\npoetry env use python3.10\npoetry shell\n\n# headless install\npoetry install\n\n# desktop install\npoetry install --extras \"desktop\"\n</code></pre>"},{"location":"annotators/","title":"Annotators","text":"BoundingBoxBoxCornerColorCircleDotTriangleEllipseHaloMaskPolygonLabelBlurPixelateTraceHeatMap <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; bounding_box_annotator = sv.BoundingBoxAnnotator()\n&gt;&gt;&gt; annotated_frame = bounding_box_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; corner_annotator = sv.BoxCornerAnnotator()\n&gt;&gt;&gt; annotated_frame = corner_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; color_annotator = sv.ColorAnnotator()\n&gt;&gt;&gt; annotated_frame = color_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; circle_annotator = sv.CircleAnnotator()\n&gt;&gt;&gt; annotated_frame = circle_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; dot_annotator = sv.DotAnnotator()\n&gt;&gt;&gt; annotated_frame = dot_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; triangle_annotator = sv.TriangleAnnotator()\n&gt;&gt;&gt; annotated_frame = triangle_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; ellipse_annotator = sv.EllipseAnnotator()\n&gt;&gt;&gt; annotated_frame = ellipse_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; halo_annotator = sv.HaloAnnotator()\n&gt;&gt;&gt; annotated_frame = halo_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; mask_annotator = sv.MaskAnnotator()\n&gt;&gt;&gt; annotated_frame = mask_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; polygon_annotator = sv.PolygonAnnotator()\n&gt;&gt;&gt; annotated_frame = polygon_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; label_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n&gt;&gt;&gt; annotated_frame = label_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; blur_annotator = sv.BlurAnnotator()\n&gt;&gt;&gt; annotated_frame = blur_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; pixelate_annotator = sv.PixelateAnnotator()\n&gt;&gt;&gt; annotated_frame = pixelate_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; model = YOLO('yolov8x.pt')\n\n&gt;&gt;&gt; trace_annotator = sv.TraceAnnotator()\n\n&gt;&gt;&gt; video_info = sv.VideoInfo.from_video_path(video_path='...')\n&gt;&gt;&gt; frames_generator = get_video_frames_generator(source_path='...')\n&gt;&gt;&gt; tracker = sv.ByteTrack()\n\n&gt;&gt;&gt; with sv.VideoSink(target_path='...', video_info=video_info) as sink:\n...    for frame in frames_generator:\n...        result = model(frame)[0]\n...        detections = sv.Detections.from_ultralytics(result)\n...        detections = tracker.update_with_detections(detections)\n...        annotated_frame = trace_annotator.annotate(\n...            scene=frame.copy(),\n...            detections=detections)\n...        sink.write_frame(frame=annotated_frame)\n</code></pre> <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; model = YOLO('yolov8x.pt')\n\n&gt;&gt;&gt; heat_map_annotator = sv.HeatMapAnnotator()\n\n&gt;&gt;&gt; video_info = sv.VideoInfo.from_video_path(video_path='...')\n&gt;&gt;&gt; frames_generator = get_video_frames_generator(source_path='...')\n\n&gt;&gt;&gt; with sv.VideoSink(target_path='...', video_info=video_info) as sink:\n...    for frame in frames_generator:\n...        result = model(frame)[0]\n...        detections = sv.Detections.from_ultralytics(result)\n...        annotated_frame = heat_map_annotator.annotate(\n...            scene=frame.copy(),\n...            detections=detections)\n...        sink.write_frame(frame=annotated_frame)\n</code></pre>"},{"location":"annotators/#boundingboxannotator","title":"BoundingBoxAnnotator","text":"<p>             Bases: <code>BaseAnnotator</code></p> <p>A class for drawing bounding boxes on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class BoundingBoxAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing bounding boxes on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        thickness: int = 2,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the bounding box lines.\n            color_lookup (str): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.color_lookup: ColorLookup = color_lookup\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with bounding boxes based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where bounding boxes will be drawn.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; bounding_box_annotator = sv.BoundingBoxAnnotator()\n            &gt;&gt;&gt; annotated_frame = bounding_box_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n        ![bounding-box-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/bounding-box-annotator-example-purple.png)\n        \"\"\"\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            cv2.rectangle(\n                img=scene,\n                pt1=(x1, y1),\n                pt2=(x2, y2),\n                color=color.as_bgr(),\n                thickness=self.thickness,\n            )\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.BoundingBoxAnnotator.__init__","title":"<code>__init__(color=ColorPalette.default(), thickness=2, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>default()</code> <code>thickness</code> <code>int</code> <p>Thickness of the bounding box lines.</p> <code>2</code> <code>color_lookup</code> <code>str</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.default(),\n    thickness: int = 2,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the bounding box lines.\n        color_lookup (str): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.BoundingBoxAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with bounding boxes based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where bounding boxes will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; bounding_box_annotator = sv.BoundingBoxAnnotator()\n&gt;&gt;&gt; annotated_frame = bounding_box_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with bounding boxes based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where bounding boxes will be drawn.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; bounding_box_annotator = sv.BoundingBoxAnnotator()\n        &gt;&gt;&gt; annotated_frame = bounding_box_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n    ![bounding-box-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/bounding-box-annotator-example-purple.png)\n    \"\"\"\n    for detection_idx in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        cv2.rectangle(\n            img=scene,\n            pt1=(x1, y1),\n            pt2=(x2, y2),\n            color=color.as_bgr(),\n            thickness=self.thickness,\n        )\n    return scene\n</code></pre>"},{"location":"annotators/#boxcornerannotator","title":"BoxCornerAnnotator","text":"<p>             Bases: <code>BaseAnnotator</code></p> <p>A class for drawing box corners on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class BoxCornerAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing box corners on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        thickness: int = 4,\n        corner_length: int = 15,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the corner lines.\n            corner_length (int): Length of each corner line.\n            color_lookup (str): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.corner_length: int = corner_length\n        self.color_lookup: ColorLookup = color_lookup\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with box corners based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where box corners will be drawn.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; corner_annotator = sv.BoxCornerAnnotator()\n            &gt;&gt;&gt; annotated_frame = corner_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n        ![box-corner-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/box-corner-annotator-example-purple.png)\n        \"\"\"\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            corners = [(x1, y1), (x2, y1), (x1, y2), (x2, y2)]\n\n            for x, y in corners:\n                x_end = x + self.corner_length if x == x1 else x - self.corner_length\n                cv2.line(\n                    scene, (x, y), (x_end, y), color.as_bgr(), thickness=self.thickness\n                )\n\n                y_end = y + self.corner_length if y == y1 else y - self.corner_length\n                cv2.line(\n                    scene, (x, y), (x, y_end), color.as_bgr(), thickness=self.thickness\n                )\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.BoxCornerAnnotator.__init__","title":"<code>__init__(color=ColorPalette.default(), thickness=4, corner_length=15, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>default()</code> <code>thickness</code> <code>int</code> <p>Thickness of the corner lines.</p> <code>4</code> <code>corner_length</code> <code>int</code> <p>Length of each corner line.</p> <code>15</code> <code>color_lookup</code> <code>str</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.default(),\n    thickness: int = 4,\n    corner_length: int = 15,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the corner lines.\n        corner_length (int): Length of each corner line.\n        color_lookup (str): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.corner_length: int = corner_length\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.BoxCornerAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with box corners based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where box corners will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; corner_annotator = sv.BoxCornerAnnotator()\n&gt;&gt;&gt; annotated_frame = corner_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with box corners based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where box corners will be drawn.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; corner_annotator = sv.BoxCornerAnnotator()\n        &gt;&gt;&gt; annotated_frame = corner_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n    ![box-corner-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/box-corner-annotator-example-purple.png)\n    \"\"\"\n    for detection_idx in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        corners = [(x1, y1), (x2, y1), (x1, y2), (x2, y2)]\n\n        for x, y in corners:\n            x_end = x + self.corner_length if x == x1 else x - self.corner_length\n            cv2.line(\n                scene, (x, y), (x_end, y), color.as_bgr(), thickness=self.thickness\n            )\n\n            y_end = y + self.corner_length if y == y1 else y - self.corner_length\n            cv2.line(\n                scene, (x, y), (x, y_end), color.as_bgr(), thickness=self.thickness\n            )\n    return scene\n</code></pre>"},{"location":"annotators/#colorannotator","title":"ColorAnnotator","text":"<p>             Bases: <code>BaseAnnotator</code></p> <p>A class for drawing box masks on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class ColorAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing box masks on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        opacity: float = 0.5,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n            color_lookup (str): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.color_lookup: ColorLookup = color_lookup\n        self.opacity = opacity\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with box masks based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where bounding boxes will be drawn.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; color_annotator = sv.ColorAnnotator()\n            &gt;&gt;&gt; annotated_frame = color_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n        ![box-mask-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/box-mask-annotator-example-purple.png)\n        \"\"\"\n        mask_image = scene.copy()\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            cv2.rectangle(\n                img=scene,\n                pt1=(x1, y1),\n                pt2=(x2, y2),\n                color=color.as_bgr(),\n                thickness=-1,\n            )\n        scene = cv2.addWeighted(\n            scene, self.opacity, mask_image, 1 - self.opacity, gamma=0\n        )\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.ColorAnnotator.__init__","title":"<code>__init__(color=ColorPalette.default(), opacity=0.5, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>default()</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask. Must be between <code>0</code> and <code>1</code>.</p> <code>0.5</code> <code>color_lookup</code> <code>str</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.default(),\n    opacity: float = 0.5,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n        color_lookup (str): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.color_lookup: ColorLookup = color_lookup\n    self.opacity = opacity\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.ColorAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with box masks based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where bounding boxes will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; color_annotator = sv.ColorAnnotator()\n&gt;&gt;&gt; annotated_frame = color_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with box masks based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where bounding boxes will be drawn.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; color_annotator = sv.ColorAnnotator()\n        &gt;&gt;&gt; annotated_frame = color_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n    ![box-mask-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/box-mask-annotator-example-purple.png)\n    \"\"\"\n    mask_image = scene.copy()\n    for detection_idx in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        cv2.rectangle(\n            img=scene,\n            pt1=(x1, y1),\n            pt2=(x2, y2),\n            color=color.as_bgr(),\n            thickness=-1,\n        )\n    scene = cv2.addWeighted(\n        scene, self.opacity, mask_image, 1 - self.opacity, gamma=0\n    )\n    return scene\n</code></pre>"},{"location":"annotators/#circleannotator","title":"CircleAnnotator","text":"<p>             Bases: <code>BaseAnnotator</code></p> <p>A class for drawing circle on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class CircleAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing circle on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        thickness: int = 2,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the circle line.\n            color_lookup (str): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.color_lookup: ColorLookup = color_lookup\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with circles based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where box corners will be drawn.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; circle_annotator = sv.CircleAnnotator()\n            &gt;&gt;&gt; annotated_frame = circle_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n\n        ![circle-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/circle-annotator-example-purple.png)\n        \"\"\"\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            center = ((x1 + x2) // 2, (y1 + y2) // 2)\n            distance = sqrt((x1 - center[0]) ** 2 + (y1 - center[1]) ** 2)\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            cv2.circle(\n                img=scene,\n                center=center,\n                radius=int(distance),\n                color=color.as_bgr(),\n                thickness=self.thickness,\n            )\n\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.CircleAnnotator.__init__","title":"<code>__init__(color=ColorPalette.default(), thickness=2, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>default()</code> <code>thickness</code> <code>int</code> <p>Thickness of the circle line.</p> <code>2</code> <code>color_lookup</code> <code>str</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.default(),\n    thickness: int = 2,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the circle line.\n        color_lookup (str): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.CircleAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with circles based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where box corners will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; circle_annotator = sv.CircleAnnotator()\n&gt;&gt;&gt; annotated_frame = circle_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with circles based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where box corners will be drawn.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; circle_annotator = sv.CircleAnnotator()\n        &gt;&gt;&gt; annotated_frame = circle_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n\n    ![circle-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/circle-annotator-example-purple.png)\n    \"\"\"\n    for detection_idx in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n        center = ((x1 + x2) // 2, (y1 + y2) // 2)\n        distance = sqrt((x1 - center[0]) ** 2 + (y1 - center[1]) ** 2)\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        cv2.circle(\n            img=scene,\n            center=center,\n            radius=int(distance),\n            color=color.as_bgr(),\n            thickness=self.thickness,\n        )\n\n    return scene\n</code></pre>"},{"location":"annotators/#dotannotator","title":"DotAnnotator","text":"<p>             Bases: <code>BaseAnnotator</code></p> <p>A class for drawing dots on an image at specific coordinates based on provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class DotAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing dots on an image at specific coordinates based on provided\n    detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        radius: int = 4,\n        position: Position = Position.CENTER,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            radius (int): Radius of the drawn dots.\n            position (Position): The anchor position for placing the dot.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.radius: int = radius\n        self.position: Position = position\n        self.color_lookup: ColorLookup = color_lookup\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with dots based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where dots will be drawn.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; dot_annotator = sv.DotAnnotator()\n            &gt;&gt;&gt; annotated_frame = dot_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n        ![dot-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/dot-annotator-example-purple.png)\n        \"\"\"\n        xy = detections.get_anchors_coordinates(anchor=self.position)\n        for detection_idx in range(len(detections)):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            center = (int(xy[detection_idx, 0]), int(xy[detection_idx, 1]))\n            cv2.circle(scene, center, self.radius, color.as_bgr(), -1)\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.DotAnnotator.__init__","title":"<code>__init__(color=ColorPalette.default(), radius=4, position=Position.CENTER, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>default()</code> <code>radius</code> <code>int</code> <p>Radius of the drawn dots.</p> <code>4</code> <code>position</code> <code>Position</code> <p>The anchor position for placing the dot.</p> <code>CENTER</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.default(),\n    radius: int = 4,\n    position: Position = Position.CENTER,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        radius (int): Radius of the drawn dots.\n        position (Position): The anchor position for placing the dot.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.radius: int = radius\n    self.position: Position = position\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.DotAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with dots based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where dots will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; dot_annotator = sv.DotAnnotator()\n&gt;&gt;&gt; annotated_frame = dot_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with dots based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where dots will be drawn.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; dot_annotator = sv.DotAnnotator()\n        &gt;&gt;&gt; annotated_frame = dot_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n    ![dot-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/dot-annotator-example-purple.png)\n    \"\"\"\n    xy = detections.get_anchors_coordinates(anchor=self.position)\n    for detection_idx in range(len(detections)):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        center = (int(xy[detection_idx, 0]), int(xy[detection_idx, 1]))\n        cv2.circle(scene, center, self.radius, color.as_bgr(), -1)\n    return scene\n</code></pre>"},{"location":"annotators/#triangleannotator","title":"TriangleAnnotator","text":"<p>             Bases: <code>BaseAnnotator</code></p> <p>A class for drawing triangle markers on an image at specific coordinates based on provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class TriangleAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing triangle markers on an image at specific coordinates based on\n    provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        base: int = 10,\n        height: int = 10,\n        position: Position = Position.TOP_CENTER,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            base (int): The base width of the triangle.\n            height (int): The height of the triangle.\n            position (Position): The anchor position for placing the triangle.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.base: int = base\n        self.height: int = height\n        self.position: Position = position\n        self.color_lookup: ColorLookup = color_lookup\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with triangles based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where triangles will be drawn.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            np.ndarray: The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; triangle_annotator = sv.TriangleAnnotator()\n            &gt;&gt;&gt; annotated_frame = triangle_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n        ![triangle-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/triangle-annotator-example.png)\n        \"\"\"\n        xy = detections.get_anchors_coordinates(anchor=self.position)\n        for detection_idx in range(len(detections)):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            tip_x, tip_y = int(xy[detection_idx, 0]), int(xy[detection_idx, 1])\n            vertices = np.array(\n                [\n                    [tip_x - self.base // 2, tip_y - self.height],\n                    [tip_x + self.base // 2, tip_y - self.height],\n                    [tip_x, tip_y],\n                ],\n                np.int32,\n            )\n\n            cv2.fillPoly(scene, [vertices], color.as_bgr())\n\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.TriangleAnnotator.__init__","title":"<code>__init__(color=ColorPalette.default(), base=10, height=10, position=Position.TOP_CENTER, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>default()</code> <code>base</code> <code>int</code> <p>The base width of the triangle.</p> <code>10</code> <code>height</code> <code>int</code> <p>The height of the triangle.</p> <code>10</code> <code>position</code> <code>Position</code> <p>The anchor position for placing the triangle.</p> <code>TOP_CENTER</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.default(),\n    base: int = 10,\n    height: int = 10,\n    position: Position = Position.TOP_CENTER,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        base (int): The base width of the triangle.\n        height (int): The height of the triangle.\n        position (Position): The anchor position for placing the triangle.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.base: int = base\n    self.height: int = height\n    self.position: Position = position\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.TriangleAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with triangles based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where triangles will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; triangle_annotator = sv.TriangleAnnotator()\n&gt;&gt;&gt; annotated_frame = triangle_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with triangles based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where triangles will be drawn.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        np.ndarray: The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; triangle_annotator = sv.TriangleAnnotator()\n        &gt;&gt;&gt; annotated_frame = triangle_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n    ![triangle-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/triangle-annotator-example.png)\n    \"\"\"\n    xy = detections.get_anchors_coordinates(anchor=self.position)\n    for detection_idx in range(len(detections)):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        tip_x, tip_y = int(xy[detection_idx, 0]), int(xy[detection_idx, 1])\n        vertices = np.array(\n            [\n                [tip_x - self.base // 2, tip_y - self.height],\n                [tip_x + self.base // 2, tip_y - self.height],\n                [tip_x, tip_y],\n            ],\n            np.int32,\n        )\n\n        cv2.fillPoly(scene, [vertices], color.as_bgr())\n\n    return scene\n</code></pre>"},{"location":"annotators/#ellipseannotator","title":"EllipseAnnotator","text":"<p>             Bases: <code>BaseAnnotator</code></p> <p>A class for drawing ellipses on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class EllipseAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing ellipses on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        thickness: int = 2,\n        start_angle: int = -45,\n        end_angle: int = 235,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the ellipse lines.\n            start_angle (int): Starting angle of the ellipse.\n            end_angle (int): Ending angle of the ellipse.\n            color_lookup (str): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.start_angle: int = start_angle\n        self.end_angle: int = end_angle\n        self.color_lookup: ColorLookup = color_lookup\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with ellipses based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where ellipses will be drawn.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; ellipse_annotator = sv.EllipseAnnotator()\n            &gt;&gt;&gt; annotated_frame = ellipse_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n        ![ellipse-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/ellipse-annotator-example-purple.png)\n        \"\"\"\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            center = (int((x1 + x2) / 2), y2)\n            width = x2 - x1\n            cv2.ellipse(\n                scene,\n                center=center,\n                axes=(int(width), int(0.35 * width)),\n                angle=0.0,\n                startAngle=self.start_angle,\n                endAngle=self.end_angle,\n                color=color.as_bgr(),\n                thickness=self.thickness,\n                lineType=cv2.LINE_4,\n            )\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.EllipseAnnotator.__init__","title":"<code>__init__(color=ColorPalette.default(), thickness=2, start_angle=-45, end_angle=235, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>default()</code> <code>thickness</code> <code>int</code> <p>Thickness of the ellipse lines.</p> <code>2</code> <code>start_angle</code> <code>int</code> <p>Starting angle of the ellipse.</p> <code>-45</code> <code>end_angle</code> <code>int</code> <p>Ending angle of the ellipse.</p> <code>235</code> <code>color_lookup</code> <code>str</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.default(),\n    thickness: int = 2,\n    start_angle: int = -45,\n    end_angle: int = 235,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the ellipse lines.\n        start_angle (int): Starting angle of the ellipse.\n        end_angle (int): Ending angle of the ellipse.\n        color_lookup (str): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.start_angle: int = start_angle\n    self.end_angle: int = end_angle\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.EllipseAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with ellipses based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where ellipses will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; ellipse_annotator = sv.EllipseAnnotator()\n&gt;&gt;&gt; annotated_frame = ellipse_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with ellipses based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where ellipses will be drawn.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; ellipse_annotator = sv.EllipseAnnotator()\n        &gt;&gt;&gt; annotated_frame = ellipse_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n    ![ellipse-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/ellipse-annotator-example-purple.png)\n    \"\"\"\n    for detection_idx in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        center = (int((x1 + x2) / 2), y2)\n        width = x2 - x1\n        cv2.ellipse(\n            scene,\n            center=center,\n            axes=(int(width), int(0.35 * width)),\n            angle=0.0,\n            startAngle=self.start_angle,\n            endAngle=self.end_angle,\n            color=color.as_bgr(),\n            thickness=self.thickness,\n            lineType=cv2.LINE_4,\n        )\n    return scene\n</code></pre>"},{"location":"annotators/#haloannotator","title":"HaloAnnotator","text":"<p>             Bases: <code>BaseAnnotator</code></p> <p>A class for drawing Halos on an image using provided detections.</p> <p>Warning</p> <p>This annotator utilizes the <code>sv.Detections.mask</code>.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class HaloAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing Halos on an image using provided detections.\n\n    !!! warning\n\n        This annotator utilizes the `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        opacity: float = 0.8,\n        kernel_size: int = 40,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n            kernel_size (int): The size of the average pooling kernel used for creating\n                the halo.\n            color_lookup (str): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.opacity = opacity\n        self.color_lookup: ColorLookup = color_lookup\n        self.kernel_size: int = kernel_size\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with halos based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where masks will be drawn.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; halo_annotator = sv.HaloAnnotator()\n            &gt;&gt;&gt; annotated_frame = halo_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n        ![halo-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/halo-annotator-example-purple.png)\n        \"\"\"\n        if detections.mask is None:\n            return scene\n        colored_mask = np.zeros_like(scene, dtype=np.uint8)\n        fmask = np.array([False] * scene.shape[0] * scene.shape[1]).reshape(\n            scene.shape[0], scene.shape[1]\n        )\n\n        for detection_idx in np.flip(np.argsort(detections.area)):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            mask = detections.mask[detection_idx]\n            fmask = np.logical_or(fmask, mask)\n            color_bgr = color.as_bgr()\n            colored_mask[mask] = color_bgr\n\n        colored_mask = cv2.blur(colored_mask, (self.kernel_size, self.kernel_size))\n        colored_mask[fmask] = [0, 0, 0]\n        gray = cv2.cvtColor(colored_mask, cv2.COLOR_BGR2GRAY)\n        alpha = self.opacity * gray / gray.max()\n        alpha_mask = alpha[:, :, np.newaxis]\n        scene = np.uint8(scene * (1 - alpha_mask) + colored_mask * self.opacity)\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.HaloAnnotator.__init__","title":"<code>__init__(color=ColorPalette.default(), opacity=0.8, kernel_size=40, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>default()</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask. Must be between <code>0</code> and <code>1</code>.</p> <code>0.8</code> <code>kernel_size</code> <code>int</code> <p>The size of the average pooling kernel used for creating the halo.</p> <code>40</code> <code>color_lookup</code> <code>str</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.default(),\n    opacity: float = 0.8,\n    kernel_size: int = 40,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n        kernel_size (int): The size of the average pooling kernel used for creating\n            the halo.\n        color_lookup (str): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.opacity = opacity\n    self.color_lookup: ColorLookup = color_lookup\n    self.kernel_size: int = kernel_size\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.HaloAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with halos based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where masks will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; halo_annotator = sv.HaloAnnotator()\n&gt;&gt;&gt; annotated_frame = halo_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with halos based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where masks will be drawn.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; halo_annotator = sv.HaloAnnotator()\n        &gt;&gt;&gt; annotated_frame = halo_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n    ![halo-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/halo-annotator-example-purple.png)\n    \"\"\"\n    if detections.mask is None:\n        return scene\n    colored_mask = np.zeros_like(scene, dtype=np.uint8)\n    fmask = np.array([False] * scene.shape[0] * scene.shape[1]).reshape(\n        scene.shape[0], scene.shape[1]\n    )\n\n    for detection_idx in np.flip(np.argsort(detections.area)):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        mask = detections.mask[detection_idx]\n        fmask = np.logical_or(fmask, mask)\n        color_bgr = color.as_bgr()\n        colored_mask[mask] = color_bgr\n\n    colored_mask = cv2.blur(colored_mask, (self.kernel_size, self.kernel_size))\n    colored_mask[fmask] = [0, 0, 0]\n    gray = cv2.cvtColor(colored_mask, cv2.COLOR_BGR2GRAY)\n    alpha = self.opacity * gray / gray.max()\n    alpha_mask = alpha[:, :, np.newaxis]\n    scene = np.uint8(scene * (1 - alpha_mask) + colored_mask * self.opacity)\n    return scene\n</code></pre>"},{"location":"annotators/#heatmapannotator","title":"HeatMapAnnotator","text":"<p>A class for drawing heatmaps on an image based on provided detections. Heat accumulates over time and is drawn as a semi-transparent overlay of blurred circles.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class HeatMapAnnotator:\n    \"\"\"\n    A class for drawing heatmaps on an image based on provided detections.\n    Heat accumulates over time and is drawn as a semi-transparent overlay\n    of blurred circles.\n    \"\"\"\n\n    def __init__(\n        self,\n        position: Position = Position.BOTTOM_CENTER,\n        opacity: float = 0.2,\n        radius: int = 40,\n        kernel_size: int = 25,\n        top_hue: int = 0,\n        low_hue: int = 125,\n    ):\n        \"\"\"\n        Args:\n            position (Position): The position of the heatmap. Defaults to\n                `BOTTOM_CENTER`.\n            opacity (float): Opacity of the overlay mask, between 0 and 1.\n            radius (int): Radius of the heat circle.\n            kernel_size (int): Kernel size for blurring the heatmap.\n            top_hue (int): Hue at the top of the heatmap. Defaults to 0 (red).\n            low_hue (int): Hue at the bottom of the heatmap. Defaults to 125 (blue).\n        \"\"\"\n        self.position = position\n        self.opacity = opacity\n        self.radius = radius\n        self.kernel_size = kernel_size\n        self.heat_mask = None\n        self.top_hue = top_hue\n        self.low_hue = low_hue\n\n    def annotate(self, scene: np.ndarray, detections: Detections) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the scene with a heatmap based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where the heatmap will be drawn.\n            detections (Detections): Object detections to annotate.\n\n        Returns:\n            Annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n            &gt;&gt;&gt; from ultralytics import YOLO\n\n            &gt;&gt;&gt; model = YOLO('yolov8x.pt')\n\n            &gt;&gt;&gt; heat_map_annotator = sv.HeatMapAnnotator()\n\n            &gt;&gt;&gt; video_info = sv.VideoInfo.from_video_path(video_path='...')\n            &gt;&gt;&gt; frames_generator = get_video_frames_generator(source_path='...')\n\n            &gt;&gt;&gt; with sv.VideoSink(target_path='...', video_info=video_info) as sink:\n            ...    for frame in frames_generator:\n            ...        result = model(frame)[0]\n            ...        detections = sv.Detections.from_ultralytics(result)\n            ...        annotated_frame = heat_map_annotator.annotate(\n            ...            scene=frame.copy(),\n            ...            detections=detections)\n            ...        sink.write_frame(frame=annotated_frame)\n            ```\n\n        ![heatmap-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/heat-map-annotator-example-purple.png)\n        \"\"\"\n\n        if self.heat_mask is None:\n            self.heat_mask = np.zeros(scene.shape[:2])\n        mask = np.zeros(scene.shape[:2])\n        for xy in detections.get_anchors_coordinates(self.position):\n            cv2.circle(mask, (int(xy[0]), int(xy[1])), self.radius, 1, -1)\n        self.heat_mask = mask + self.heat_mask\n        temp = self.heat_mask.copy()\n        temp = self.low_hue - temp / temp.max() * (self.low_hue - self.top_hue)\n        temp = temp.astype(np.uint8)\n        if self.kernel_size is not None:\n            temp = cv2.blur(temp, (self.kernel_size, self.kernel_size))\n        hsv = np.zeros(scene.shape)\n        hsv[..., 0] = temp\n        hsv[..., 1] = 255\n        hsv[..., 2] = 255\n        temp = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n        mask = cv2.cvtColor(self.heat_mask.astype(np.uint8), cv2.COLOR_GRAY2BGR) &gt; 0\n        scene[mask] = cv2.addWeighted(temp, self.opacity, scene, 1 - self.opacity, 0)[\n            mask\n        ]\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.HeatMapAnnotator.__init__","title":"<code>__init__(position=Position.BOTTOM_CENTER, opacity=0.2, radius=40, kernel_size=25, top_hue=0, low_hue=125)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>position</code> <code>Position</code> <p>The position of the heatmap. Defaults to <code>BOTTOM_CENTER</code>.</p> <code>BOTTOM_CENTER</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask, between 0 and 1.</p> <code>0.2</code> <code>radius</code> <code>int</code> <p>Radius of the heat circle.</p> <code>40</code> <code>kernel_size</code> <code>int</code> <p>Kernel size for blurring the heatmap.</p> <code>25</code> <code>top_hue</code> <code>int</code> <p>Hue at the top of the heatmap. Defaults to 0 (red).</p> <code>0</code> <code>low_hue</code> <code>int</code> <p>Hue at the bottom of the heatmap. Defaults to 125 (blue).</p> <code>125</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    position: Position = Position.BOTTOM_CENTER,\n    opacity: float = 0.2,\n    radius: int = 40,\n    kernel_size: int = 25,\n    top_hue: int = 0,\n    low_hue: int = 125,\n):\n    \"\"\"\n    Args:\n        position (Position): The position of the heatmap. Defaults to\n            `BOTTOM_CENTER`.\n        opacity (float): Opacity of the overlay mask, between 0 and 1.\n        radius (int): Radius of the heat circle.\n        kernel_size (int): Kernel size for blurring the heatmap.\n        top_hue (int): Hue at the top of the heatmap. Defaults to 0 (red).\n        low_hue (int): Hue at the bottom of the heatmap. Defaults to 125 (blue).\n    \"\"\"\n    self.position = position\n    self.opacity = opacity\n    self.radius = radius\n    self.kernel_size = kernel_size\n    self.heat_mask = None\n    self.top_hue = top_hue\n    self.low_hue = low_hue\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.HeatMapAnnotator.annotate","title":"<code>annotate(scene, detections)</code>","text":"<p>Annotates the scene with a heatmap based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where the heatmap will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; model = YOLO('yolov8x.pt')\n\n&gt;&gt;&gt; heat_map_annotator = sv.HeatMapAnnotator()\n\n&gt;&gt;&gt; video_info = sv.VideoInfo.from_video_path(video_path='...')\n&gt;&gt;&gt; frames_generator = get_video_frames_generator(source_path='...')\n\n&gt;&gt;&gt; with sv.VideoSink(target_path='...', video_info=video_info) as sink:\n...    for frame in frames_generator:\n...        result = model(frame)[0]\n...        detections = sv.Detections.from_ultralytics(result)\n...        annotated_frame = heat_map_annotator.annotate(\n...            scene=frame.copy(),\n...            detections=detections)\n...        sink.write_frame(frame=annotated_frame)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(self, scene: np.ndarray, detections: Detections) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the scene with a heatmap based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where the heatmap will be drawn.\n        detections (Detections): Object detections to annotate.\n\n    Returns:\n        Annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n        &gt;&gt;&gt; from ultralytics import YOLO\n\n        &gt;&gt;&gt; model = YOLO('yolov8x.pt')\n\n        &gt;&gt;&gt; heat_map_annotator = sv.HeatMapAnnotator()\n\n        &gt;&gt;&gt; video_info = sv.VideoInfo.from_video_path(video_path='...')\n        &gt;&gt;&gt; frames_generator = get_video_frames_generator(source_path='...')\n\n        &gt;&gt;&gt; with sv.VideoSink(target_path='...', video_info=video_info) as sink:\n        ...    for frame in frames_generator:\n        ...        result = model(frame)[0]\n        ...        detections = sv.Detections.from_ultralytics(result)\n        ...        annotated_frame = heat_map_annotator.annotate(\n        ...            scene=frame.copy(),\n        ...            detections=detections)\n        ...        sink.write_frame(frame=annotated_frame)\n        ```\n\n    ![heatmap-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/heat-map-annotator-example-purple.png)\n    \"\"\"\n\n    if self.heat_mask is None:\n        self.heat_mask = np.zeros(scene.shape[:2])\n    mask = np.zeros(scene.shape[:2])\n    for xy in detections.get_anchors_coordinates(self.position):\n        cv2.circle(mask, (int(xy[0]), int(xy[1])), self.radius, 1, -1)\n    self.heat_mask = mask + self.heat_mask\n    temp = self.heat_mask.copy()\n    temp = self.low_hue - temp / temp.max() * (self.low_hue - self.top_hue)\n    temp = temp.astype(np.uint8)\n    if self.kernel_size is not None:\n        temp = cv2.blur(temp, (self.kernel_size, self.kernel_size))\n    hsv = np.zeros(scene.shape)\n    hsv[..., 0] = temp\n    hsv[..., 1] = 255\n    hsv[..., 2] = 255\n    temp = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n    mask = cv2.cvtColor(self.heat_mask.astype(np.uint8), cv2.COLOR_GRAY2BGR) &gt; 0\n    scene[mask] = cv2.addWeighted(temp, self.opacity, scene, 1 - self.opacity, 0)[\n        mask\n    ]\n    return scene\n</code></pre>"},{"location":"annotators/#maskannotator","title":"MaskAnnotator","text":"<p>             Bases: <code>BaseAnnotator</code></p> <p>A class for drawing masks on an image using provided detections.</p> <p>Warning</p> <p>This annotator utilizes the <code>sv.Detections.mask</code>.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class MaskAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing masks on an image using provided detections.\n\n    !!! warning\n\n        This annotator utilizes the `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        opacity: float = 0.5,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n            color_lookup (str): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.opacity = opacity\n        self.color_lookup: ColorLookup = color_lookup\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with masks based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where masks will be drawn.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; mask_annotator = sv.MaskAnnotator()\n            &gt;&gt;&gt; annotated_frame = mask_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n        ![mask-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/mask-annotator-example-purple.png)\n        \"\"\"\n        if detections.mask is None:\n            return scene\n\n        colored_mask = np.array(scene, copy=True, dtype=np.uint8)\n\n        for detection_idx in np.flip(np.argsort(detections.area)):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            mask = detections.mask[detection_idx]\n            colored_mask[mask] = color.as_bgr()\n\n        scene = cv2.addWeighted(colored_mask, self.opacity, scene, 1 - self.opacity, 0)\n        return scene.astype(np.uint8)\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.MaskAnnotator.__init__","title":"<code>__init__(color=ColorPalette.default(), opacity=0.5, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>default()</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask. Must be between <code>0</code> and <code>1</code>.</p> <code>0.5</code> <code>color_lookup</code> <code>str</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.default(),\n    opacity: float = 0.5,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n        color_lookup (str): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.opacity = opacity\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.MaskAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with masks based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where masks will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; mask_annotator = sv.MaskAnnotator()\n&gt;&gt;&gt; annotated_frame = mask_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with masks based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where masks will be drawn.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; mask_annotator = sv.MaskAnnotator()\n        &gt;&gt;&gt; annotated_frame = mask_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n    ![mask-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/mask-annotator-example-purple.png)\n    \"\"\"\n    if detections.mask is None:\n        return scene\n\n    colored_mask = np.array(scene, copy=True, dtype=np.uint8)\n\n    for detection_idx in np.flip(np.argsort(detections.area)):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        mask = detections.mask[detection_idx]\n        colored_mask[mask] = color.as_bgr()\n\n    scene = cv2.addWeighted(colored_mask, self.opacity, scene, 1 - self.opacity, 0)\n    return scene.astype(np.uint8)\n</code></pre>"},{"location":"annotators/#polygonannotator","title":"PolygonAnnotator","text":"<p>             Bases: <code>BaseAnnotator</code></p> <p>A class for drawing polygons on an image using provided detections.</p> <p>Warning</p> <p>This annotator utilizes the <code>sv.Detections.mask</code>.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class PolygonAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing polygons on an image using provided detections.\n\n    !!! warning\n\n        This annotator utilizes the `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        thickness: int = 2,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the polygon lines.\n            color_lookup (str): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.color_lookup: ColorLookup = color_lookup\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with polygons based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where polygons will be drawn.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; polygon_annotator = sv.PolygonAnnotator()\n            &gt;&gt;&gt; annotated_frame = polygon_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n        ![polygon-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/polygon-annotator-example-purple.png)\n        \"\"\"\n        if detections.mask is None:\n            return scene\n\n        for detection_idx in range(len(detections)):\n            mask = detections.mask[detection_idx]\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            for polygon in mask_to_polygons(mask=mask):\n                scene = draw_polygon(\n                    scene=scene,\n                    polygon=polygon,\n                    color=color,\n                    thickness=self.thickness,\n                )\n\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.PolygonAnnotator.__init__","title":"<code>__init__(color=ColorPalette.default(), thickness=2, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>default()</code> <code>thickness</code> <code>int</code> <p>Thickness of the polygon lines.</p> <code>2</code> <code>color_lookup</code> <code>str</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.default(),\n    thickness: int = 2,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the polygon lines.\n        color_lookup (str): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.PolygonAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with polygons based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where polygons will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; polygon_annotator = sv.PolygonAnnotator()\n&gt;&gt;&gt; annotated_frame = polygon_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with polygons based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where polygons will be drawn.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; polygon_annotator = sv.PolygonAnnotator()\n        &gt;&gt;&gt; annotated_frame = polygon_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n    ![polygon-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/polygon-annotator-example-purple.png)\n    \"\"\"\n    if detections.mask is None:\n        return scene\n\n    for detection_idx in range(len(detections)):\n        mask = detections.mask[detection_idx]\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        for polygon in mask_to_polygons(mask=mask):\n            scene = draw_polygon(\n                scene=scene,\n                polygon=polygon,\n                color=color,\n                thickness=self.thickness,\n            )\n\n    return scene\n</code></pre>"},{"location":"annotators/#labelannotator","title":"LabelAnnotator","text":"<p>A class for annotating labels on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class LabelAnnotator:\n    \"\"\"\n    A class for annotating labels on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        text_color: Color = Color.black(),\n        text_scale: float = 0.5,\n        text_thickness: int = 1,\n        text_padding: int = 10,\n        text_position: Position = Position.TOP_LEFT,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating the text background.\n            text_color (Color): The color to use for the text.\n            text_scale (float): Font scale for the text.\n            text_thickness (int): Thickness of the text characters.\n            text_padding (int): Padding around the text within its background box.\n            text_position (Position): Position of the text relative to the detection.\n                Possible values are defined in the `Position` enum.\n            color_lookup (str): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.text_color: Color = text_color\n        self.text_scale: float = text_scale\n        self.text_thickness: int = text_thickness\n        self.text_padding: int = text_padding\n        self.text_anchor: Position = text_position\n        self.color_lookup: ColorLookup = color_lookup\n\n    @staticmethod\n    def resolve_text_background_xyxy(\n        center_coordinates: Tuple[int, int],\n        text_wh: Tuple[int, int],\n        position: Position,\n    ) -&gt; Tuple[int, int, int, int]:\n        center_x, center_y = center_coordinates\n        text_w, text_h = text_wh\n\n        if position == Position.TOP_LEFT:\n            return center_x, center_y - text_h, center_x + text_w, center_y\n        elif position == Position.TOP_RIGHT:\n            return center_x - text_w, center_y - text_h, center_x, center_y\n        elif position == Position.TOP_CENTER:\n            return (\n                center_x - text_w // 2,\n                center_y - text_h,\n                center_x + text_w // 2,\n                center_y,\n            )\n        elif position == Position.CENTER or position == Position.CENTER_OF_MASS:\n            return (\n                center_x - text_w // 2,\n                center_y - text_h // 2,\n                center_x + text_w // 2,\n                center_y + text_h // 2,\n            )\n        elif position == Position.BOTTOM_LEFT:\n            return center_x, center_y, center_x + text_w, center_y + text_h\n        elif position == Position.BOTTOM_RIGHT:\n            return center_x - text_w, center_y, center_x, center_y + text_h\n        elif position == Position.BOTTOM_CENTER:\n            return (\n                center_x - text_w // 2,\n                center_y,\n                center_x + text_w // 2,\n                center_y + text_h,\n            )\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        labels: List[str] = None,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene with labels based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where labels will be drawn.\n            detections (Detections): Object detections to annotate.\n            labels (List[str]): Optional. Custom labels for each detection.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; label_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n            &gt;&gt;&gt; annotated_frame = label_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n        ![label-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/label-annotator-example-purple.png)\n        \"\"\"\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        anchors_coordinates = detections.get_anchors_coordinates(\n            anchor=self.text_anchor\n        ).astype(int)\n        for detection_idx, center_coordinates in enumerate(anchors_coordinates):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            text = (\n                f\"{detections.class_id[detection_idx]}\"\n                if (labels is None or len(detections) != len(labels))\n                else labels[detection_idx]\n            )\n            text_w, text_h = cv2.getTextSize(\n                text=text,\n                fontFace=font,\n                fontScale=self.text_scale,\n                thickness=self.text_thickness,\n            )[0]\n            text_w_padded = text_w + 2 * self.text_padding\n            text_h_padded = text_h + 2 * self.text_padding\n            text_background_xyxy = self.resolve_text_background_xyxy(\n                center_coordinates=tuple(center_coordinates),\n                text_wh=(text_w_padded, text_h_padded),\n                position=self.text_anchor,\n            )\n\n            text_x = text_background_xyxy[0] + self.text_padding\n            text_y = text_background_xyxy[1] + self.text_padding + text_h\n\n            cv2.rectangle(\n                img=scene,\n                pt1=(text_background_xyxy[0], text_background_xyxy[1]),\n                pt2=(text_background_xyxy[2], text_background_xyxy[3]),\n                color=color.as_bgr(),\n                thickness=cv2.FILLED,\n            )\n            cv2.putText(\n                img=scene,\n                text=text,\n                org=(text_x, text_y),\n                fontFace=font,\n                fontScale=self.text_scale,\n                color=self.text_color.as_rgb(),\n                thickness=self.text_thickness,\n                lineType=cv2.LINE_AA,\n            )\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.LabelAnnotator.__init__","title":"<code>__init__(color=ColorPalette.default(), text_color=Color.black(), text_scale=0.5, text_thickness=1, text_padding=10, text_position=Position.TOP_LEFT, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating the text background.</p> <code>default()</code> <code>text_color</code> <code>Color</code> <p>The color to use for the text.</p> <code>black()</code> <code>text_scale</code> <code>float</code> <p>Font scale for the text.</p> <code>0.5</code> <code>text_thickness</code> <code>int</code> <p>Thickness of the text characters.</p> <code>1</code> <code>text_padding</code> <code>int</code> <p>Padding around the text within its background box.</p> <code>10</code> <code>text_position</code> <code>Position</code> <p>Position of the text relative to the detection. Possible values are defined in the <code>Position</code> enum.</p> <code>TOP_LEFT</code> <code>color_lookup</code> <code>str</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.default(),\n    text_color: Color = Color.black(),\n    text_scale: float = 0.5,\n    text_thickness: int = 1,\n    text_padding: int = 10,\n    text_position: Position = Position.TOP_LEFT,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating the text background.\n        text_color (Color): The color to use for the text.\n        text_scale (float): Font scale for the text.\n        text_thickness (int): Thickness of the text characters.\n        text_padding (int): Padding around the text within its background box.\n        text_position (Position): Position of the text relative to the detection.\n            Possible values are defined in the `Position` enum.\n        color_lookup (str): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.text_color: Color = text_color\n    self.text_scale: float = text_scale\n    self.text_thickness: int = text_thickness\n    self.text_padding: int = text_padding\n    self.text_anchor: Position = text_position\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.LabelAnnotator.annotate","title":"<code>annotate(scene, detections, labels=None, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with labels based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where labels will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>labels</code> <code>List[str]</code> <p>Optional. Custom labels for each detection.</p> <code>None</code> <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; label_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n&gt;&gt;&gt; annotated_frame = label_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    labels: List[str] = None,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene with labels based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where labels will be drawn.\n        detections (Detections): Object detections to annotate.\n        labels (List[str]): Optional. Custom labels for each detection.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; label_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n        &gt;&gt;&gt; annotated_frame = label_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n    ![label-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/label-annotator-example-purple.png)\n    \"\"\"\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    anchors_coordinates = detections.get_anchors_coordinates(\n        anchor=self.text_anchor\n    ).astype(int)\n    for detection_idx, center_coordinates in enumerate(anchors_coordinates):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        text = (\n            f\"{detections.class_id[detection_idx]}\"\n            if (labels is None or len(detections) != len(labels))\n            else labels[detection_idx]\n        )\n        text_w, text_h = cv2.getTextSize(\n            text=text,\n            fontFace=font,\n            fontScale=self.text_scale,\n            thickness=self.text_thickness,\n        )[0]\n        text_w_padded = text_w + 2 * self.text_padding\n        text_h_padded = text_h + 2 * self.text_padding\n        text_background_xyxy = self.resolve_text_background_xyxy(\n            center_coordinates=tuple(center_coordinates),\n            text_wh=(text_w_padded, text_h_padded),\n            position=self.text_anchor,\n        )\n\n        text_x = text_background_xyxy[0] + self.text_padding\n        text_y = text_background_xyxy[1] + self.text_padding + text_h\n\n        cv2.rectangle(\n            img=scene,\n            pt1=(text_background_xyxy[0], text_background_xyxy[1]),\n            pt2=(text_background_xyxy[2], text_background_xyxy[3]),\n            color=color.as_bgr(),\n            thickness=cv2.FILLED,\n        )\n        cv2.putText(\n            img=scene,\n            text=text,\n            org=(text_x, text_y),\n            fontFace=font,\n            fontScale=self.text_scale,\n            color=self.text_color.as_rgb(),\n            thickness=self.text_thickness,\n            lineType=cv2.LINE_AA,\n        )\n    return scene\n</code></pre>"},{"location":"annotators/#blurannotator","title":"BlurAnnotator","text":"<p>             Bases: <code>BaseAnnotator</code></p> <p>A class for blurring regions in an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class BlurAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for blurring regions in an image using provided detections.\n    \"\"\"\n\n    def __init__(self, kernel_size: int = 15):\n        \"\"\"\n        Args:\n            kernel_size (int): The size of the average pooling kernel used for blurring.\n        \"\"\"\n        self.kernel_size: int = kernel_size\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene by blurring regions based on the provided detections.\n\n        Args:\n            scene (np.ndarray): The image where blurring will be applied.\n            detections (Detections): Object detections to annotate.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; blur_annotator = sv.BlurAnnotator()\n            &gt;&gt;&gt; annotated_frame = circle_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n        ![blur-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/blur-annotator-example-purple.png)\n        \"\"\"\n        image_height, image_width = scene.shape[:2]\n        clipped_xyxy = clip_boxes(\n            xyxy=detections.xyxy, resolution_wh=(image_width, image_height)\n        ).astype(int)\n\n        for x1, y1, x2, y2 in clipped_xyxy:\n            roi = scene[y1:y2, x1:x2]\n            roi = cv2.blur(roi, (self.kernel_size, self.kernel_size))\n            scene[y1:y2, x1:x2] = roi\n\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.BlurAnnotator.__init__","title":"<code>__init__(kernel_size=15)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>int</code> <p>The size of the average pooling kernel used for blurring.</p> <code>15</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(self, kernel_size: int = 15):\n    \"\"\"\n    Args:\n        kernel_size (int): The size of the average pooling kernel used for blurring.\n    \"\"\"\n    self.kernel_size: int = kernel_size\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.BlurAnnotator.annotate","title":"<code>annotate(scene, detections)</code>","text":"<p>Annotates the given scene by blurring regions based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where blurring will be applied.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; blur_annotator = sv.BlurAnnotator()\n&gt;&gt;&gt; annotated_frame = circle_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene by blurring regions based on the provided detections.\n\n    Args:\n        scene (np.ndarray): The image where blurring will be applied.\n        detections (Detections): Object detections to annotate.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; blur_annotator = sv.BlurAnnotator()\n        &gt;&gt;&gt; annotated_frame = circle_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n    ![blur-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/blur-annotator-example-purple.png)\n    \"\"\"\n    image_height, image_width = scene.shape[:2]\n    clipped_xyxy = clip_boxes(\n        xyxy=detections.xyxy, resolution_wh=(image_width, image_height)\n    ).astype(int)\n\n    for x1, y1, x2, y2 in clipped_xyxy:\n        roi = scene[y1:y2, x1:x2]\n        roi = cv2.blur(roi, (self.kernel_size, self.kernel_size))\n        scene[y1:y2, x1:x2] = roi\n\n    return scene\n</code></pre>"},{"location":"annotators/#pixelateannotator","title":"PixelateAnnotator","text":"<p>             Bases: <code>BaseAnnotator</code></p> <p>A class for pixelating regions in an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class PixelateAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for pixelating regions in an image using provided detections.\n    \"\"\"\n\n    def __init__(self, pixel_size: int = 20):\n        \"\"\"\n        Args:\n            pixel_size (int): The size of the pixelation.\n        \"\"\"\n        self.pixel_size: int = pixel_size\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the given scene by pixelating regions based on the provided\n            detections.\n\n        Args:\n            scene (np.ndarray): The image where pixelating will be applied.\n            detections (Detections): Object detections to annotate.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = ...\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; pixelate_annotator = sv.PixelateAnnotator()\n            &gt;&gt;&gt; annotated_frame = pixelate_annotator.annotate(\n            ...     scene=image.copy(),\n            ...     detections=detections\n            ... )\n            ```\n\n        ![pixelate-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/pixelate-annotator-example-10.png)\n        \"\"\"\n        image_height, image_width = scene.shape[:2]\n        clipped_xyxy = clip_boxes(\n            xyxy=detections.xyxy, resolution_wh=(image_width, image_height)\n        ).astype(int)\n\n        for x1, y1, x2, y2 in clipped_xyxy:\n            roi = scene[y1:y2, x1:x2]\n            scaled_up_roi = cv2.resize(\n                src=roi, dsize=None, fx=1 / self.pixel_size, fy=1 / self.pixel_size\n            )\n            scaled_down_roi = cv2.resize(\n                src=scaled_up_roi,\n                dsize=(roi.shape[1], roi.shape[0]),\n                interpolation=cv2.INTER_NEAREST,\n            )\n\n            scene[y1:y2, x1:x2] = scaled_down_roi\n\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.PixelateAnnotator.__init__","title":"<code>__init__(pixel_size=20)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pixel_size</code> <code>int</code> <p>The size of the pixelation.</p> <code>20</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(self, pixel_size: int = 20):\n    \"\"\"\n    Args:\n        pixel_size (int): The size of the pixelation.\n    \"\"\"\n    self.pixel_size: int = pixel_size\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.PixelateAnnotator.annotate","title":"<code>annotate(scene, detections)</code>","text":"<p>Annotates the given scene by pixelating regions based on the provided     detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image where pixelating will be applied.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; pixelate_annotator = sv.PixelateAnnotator()\n&gt;&gt;&gt; annotated_frame = pixelate_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the given scene by pixelating regions based on the provided\n        detections.\n\n    Args:\n        scene (np.ndarray): The image where pixelating will be applied.\n        detections (Detections): Object detections to annotate.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = ...\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; pixelate_annotator = sv.PixelateAnnotator()\n        &gt;&gt;&gt; annotated_frame = pixelate_annotator.annotate(\n        ...     scene=image.copy(),\n        ...     detections=detections\n        ... )\n        ```\n\n    ![pixelate-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/pixelate-annotator-example-10.png)\n    \"\"\"\n    image_height, image_width = scene.shape[:2]\n    clipped_xyxy = clip_boxes(\n        xyxy=detections.xyxy, resolution_wh=(image_width, image_height)\n    ).astype(int)\n\n    for x1, y1, x2, y2 in clipped_xyxy:\n        roi = scene[y1:y2, x1:x2]\n        scaled_up_roi = cv2.resize(\n            src=roi, dsize=None, fx=1 / self.pixel_size, fy=1 / self.pixel_size\n        )\n        scaled_down_roi = cv2.resize(\n            src=scaled_up_roi,\n            dsize=(roi.shape[1], roi.shape[0]),\n            interpolation=cv2.INTER_NEAREST,\n        )\n\n        scene[y1:y2, x1:x2] = scaled_down_roi\n\n    return scene\n</code></pre>"},{"location":"annotators/#traceannotator","title":"TraceAnnotator","text":"<p>A class for drawing trace paths on an image based on detection coordinates.</p> <p>Warning</p> <p>This annotator utilizes the <code>sv.Detections.tracker_id</code>. Read here to learn how to plug tracking into your inference pipeline.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class TraceAnnotator:\n    \"\"\"\n    A class for drawing trace paths on an image based on detection coordinates.\n\n    !!! warning\n\n        This annotator utilizes the `sv.Detections.tracker_id`. Read\n        [here](https://supervision.roboflow.com/trackers/) to learn how to plug\n        tracking into your inference pipeline.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.default(),\n        position: Position = Position.CENTER,\n        trace_length: int = 30,\n        thickness: int = 2,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color to draw the trace, can be\n                a single color or a color palette.\n            position (Position): The position of the trace.\n                Defaults to `CENTER`.\n            trace_length (int): The maximum length of the trace in terms of historical\n                points. Defaults to `30`.\n            thickness (int): The thickness of the trace lines. Defaults to `2`.\n            color_lookup (str): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.trace = Trace(max_size=trace_length, anchor=position)\n        self.thickness = thickness\n        self.color_lookup: ColorLookup = color_lookup\n\n    def annotate(\n        self,\n        scene: np.ndarray,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Draws trace paths on the frame based on the detection coordinates provided.\n\n        Args:\n            scene (np.ndarray): The image on which the traces will be drawn.\n            detections (Detections): The detections which include coordinates for\n                which the traces will be drawn.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n            &gt;&gt;&gt; from ultralytics import YOLO\n\n            &gt;&gt;&gt; model = YOLO('yolov8x.pt')\n\n            &gt;&gt;&gt; trace_annotator = sv.TraceAnnotator()\n\n            &gt;&gt;&gt; video_info = sv.VideoInfo.from_video_path(video_path='...')\n            &gt;&gt;&gt; frames_generator = sv.get_video_frames_generator(source_path='...')\n            &gt;&gt;&gt; tracker = sv.ByteTrack()\n\n            &gt;&gt;&gt; with sv.VideoSink(target_path='...', video_info=video_info) as sink:\n            ...    for frame in frames_generator:\n            ...        result = model(frame)[0]\n            ...        detections = sv.Detections.from_ultralytics(result)\n            ...        detections = tracker.update_with_detections(detections)\n            ...        annotated_frame = trace_annotator.annotate(\n            ...            scene=frame.copy(),\n            ...            detections=detections)\n            ...        sink.write_frame(frame=annotated_frame)\n            ```\n\n        ![trace-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/trace-annotator-example-purple.png)\n        \"\"\"\n        self.trace.put(detections)\n\n        for detection_idx in range(len(detections)):\n            tracker_id = int(detections.tracker_id[detection_idx])\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            xy = self.trace.get(tracker_id=tracker_id)\n            if len(xy) &gt; 1:\n                scene = cv2.polylines(\n                    scene,\n                    [xy.astype(np.int32)],\n                    False,\n                    color=color.as_bgr(),\n                    thickness=self.thickness,\n                )\n        return scene\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.TraceAnnotator.__init__","title":"<code>__init__(color=ColorPalette.default(), position=Position.CENTER, trace_length=30, thickness=2, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color to draw the trace, can be a single color or a color palette.</p> <code>default()</code> <code>position</code> <code>Position</code> <p>The position of the trace. Defaults to <code>CENTER</code>.</p> <code>CENTER</code> <code>trace_length</code> <code>int</code> <p>The maximum length of the trace in terms of historical points. Defaults to <code>30</code>.</p> <code>30</code> <code>thickness</code> <code>int</code> <p>The thickness of the trace lines. Defaults to <code>2</code>.</p> <code>2</code> <code>color_lookup</code> <code>str</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.default(),\n    position: Position = Position.CENTER,\n    trace_length: int = 30,\n    thickness: int = 2,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color to draw the trace, can be\n            a single color or a color palette.\n        position (Position): The position of the trace.\n            Defaults to `CENTER`.\n        trace_length (int): The maximum length of the trace in terms of historical\n            points. Defaults to `30`.\n        thickness (int): The thickness of the trace lines. Defaults to `2`.\n        color_lookup (str): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.trace = Trace(max_size=trace_length, anchor=position)\n    self.thickness = thickness\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"annotators/#supervision.annotators.core.TraceAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Draws trace paths on the frame based on the detection coordinates provided.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image on which the traces will be drawn.</p> required <code>detections</code> <code>Detections</code> <p>The detections which include coordinates for which the traces will be drawn.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; model = YOLO('yolov8x.pt')\n\n&gt;&gt;&gt; trace_annotator = sv.TraceAnnotator()\n\n&gt;&gt;&gt; video_info = sv.VideoInfo.from_video_path(video_path='...')\n&gt;&gt;&gt; frames_generator = sv.get_video_frames_generator(source_path='...')\n&gt;&gt;&gt; tracker = sv.ByteTrack()\n\n&gt;&gt;&gt; with sv.VideoSink(target_path='...', video_info=video_info) as sink:\n...    for frame in frames_generator:\n...        result = model(frame)[0]\n...        detections = sv.Detections.from_ultralytics(result)\n...        detections = tracker.update_with_detections(detections)\n...        annotated_frame = trace_annotator.annotate(\n...            scene=frame.copy(),\n...            detections=detections)\n...        sink.write_frame(frame=annotated_frame)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>def annotate(\n    self,\n    scene: np.ndarray,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Draws trace paths on the frame based on the detection coordinates provided.\n\n    Args:\n        scene (np.ndarray): The image on which the traces will be drawn.\n        detections (Detections): The detections which include coordinates for\n            which the traces will be drawn.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n        &gt;&gt;&gt; from ultralytics import YOLO\n\n        &gt;&gt;&gt; model = YOLO('yolov8x.pt')\n\n        &gt;&gt;&gt; trace_annotator = sv.TraceAnnotator()\n\n        &gt;&gt;&gt; video_info = sv.VideoInfo.from_video_path(video_path='...')\n        &gt;&gt;&gt; frames_generator = sv.get_video_frames_generator(source_path='...')\n        &gt;&gt;&gt; tracker = sv.ByteTrack()\n\n        &gt;&gt;&gt; with sv.VideoSink(target_path='...', video_info=video_info) as sink:\n        ...    for frame in frames_generator:\n        ...        result = model(frame)[0]\n        ...        detections = sv.Detections.from_ultralytics(result)\n        ...        detections = tracker.update_with_detections(detections)\n        ...        annotated_frame = trace_annotator.annotate(\n        ...            scene=frame.copy(),\n        ...            detections=detections)\n        ...        sink.write_frame(frame=annotated_frame)\n        ```\n\n    ![trace-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/trace-annotator-example-purple.png)\n    \"\"\"\n    self.trace.put(detections)\n\n    for detection_idx in range(len(detections)):\n        tracker_id = int(detections.tracker_id[detection_idx])\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        xy = self.trace.get(tracker_id=tracker_id)\n        if len(xy) &gt; 1:\n            scene = cv2.polylines(\n                scene,\n                [xy.astype(np.int32)],\n                False,\n                color=color.as_bgr(),\n                thickness=self.thickness,\n            )\n    return scene\n</code></pre>"},{"location":"annotators/#colorlookup","title":"ColorLookup","text":"<p>             Bases: <code>Enum</code></p> <p>Enumeration class to define strategies for mapping colors to annotations.</p> This enum supports three different lookup strategies <ul> <li><code>INDEX</code>: Colors are determined by the index of the detection within the scene.</li> <li><code>CLASS</code>: Colors are determined by the class label of the detected object.</li> <li><code>TRACK</code>: Colors are determined by the tracking identifier of the object.</li> </ul> Source code in <code>supervision/annotators/utils.py</code> <pre><code>class ColorLookup(Enum):\n    \"\"\"\n    Enumeration class to define strategies for mapping colors to annotations.\n\n    This enum supports three different lookup strategies:\n        - `INDEX`: Colors are determined by the index of the detection within the scene.\n        - `CLASS`: Colors are determined by the class label of the detected object.\n        - `TRACK`: Colors are determined by the tracking identifier of the object.\n    \"\"\"\n\n    INDEX = \"index\"\n    CLASS = \"class\"\n    TRACK = \"track\"\n\n    @classmethod\n    def list(cls):\n        return list(map(lambda c: c.value, cls))\n</code></pre>"},{"location":"assets/","title":"Assets","text":"<p>Supervision offers an assets download utility that allows you to download video files that you can use in your demos.</p>"},{"location":"assets/#install-extra","title":"install extra","text":"<p>To install the Supervision assets utility, you can use <code>pip</code>. This utility is available as an extra within the Supervision package.</p> <p>pip install</p> <pre><code>pip install supervision[assets]\n</code></pre>"},{"location":"assets/#download_assets","title":"download_assets","text":"<p>Download a specified asset if it doesn't already exist or is corrupted.</p> <p>Parameters:</p> Name Type Description Default <code>asset_name</code> <code>Union[VideoAssets, str]</code> <p>The name or type of the asset to be downloaded.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The filename of the downloaded asset.</p> Example <pre><code>&gt;&gt;&gt; from supervision.assets import download_assets, VideoAssets\n\n&gt;&gt;&gt; download_assets(VideoAssets.VEHICLES)\n\"vehicles.mp4\"\n</code></pre> Source code in <code>supervision/assets/downloader.py</code> <pre><code>def download_assets(asset_name: Union[VideoAssets, str]) -&gt; str:\n    \"\"\"\n    Download a specified asset if it doesn't already exist or is corrupted.\n\n    Parameters:\n        asset_name (Union[VideoAssets, str]): The name or type of the asset to be\n            downloaded.\n\n    Returns:\n        str: The filename of the downloaded asset.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; from supervision.assets import download_assets, VideoAssets\n\n        &gt;&gt;&gt; download_assets(VideoAssets.VEHICLES)\n        \"vehicles.mp4\"\n        ```\n    \"\"\"\n\n    filename = asset_name.value if isinstance(asset_name, VideoAssets) else asset_name\n\n    if not Path(filename).exists() and filename in VIDEO_ASSETS:\n        print(f\"Downloading {filename} assets \\n\")\n        response = get(VIDEO_ASSETS[filename][0], stream=True, allow_redirects=True)\n        response.raise_for_status()\n\n        file_size = int(response.headers.get(\"Content-Length\", 0))\n        folder_path = Path(filename).expanduser().resolve()\n        folder_path.parent.mkdir(parents=True, exist_ok=True)\n\n        with tqdm.wrapattr(\n            response.raw, \"read\", total=file_size, desc=\"\", colour=\"#a351fb\"\n        ) as raw_resp:\n            with folder_path.open(\"wb\") as file:\n                copyfileobj(raw_resp, file)\n\n    elif Path(filename).exists():\n        if not is_md5_hash_matching(filename, VIDEO_ASSETS[filename][1]):\n            print(\"File corrupted. Re-downloading... \\n\")\n            os.remove(filename)\n            return download_assets(filename)\n\n        print(f\"{filename} asset download complete. \\n\")\n\n    else:\n        valid_assets = \", \".join(asset.value for asset in VideoAssets)\n        raise ValueError(\n            f\"Invalid asset. It should be one of the following: {valid_assets}.\"\n        )\n\n    return filename\n</code></pre>"},{"location":"assets/#videoassets","title":"VideoAssets","text":"<p>             Bases: <code>Enum</code></p> <p>Each member of this enum represents a video asset. The value associated with each member is the filename of the video.</p> Enum Member Video Filename Video URL <code>VEHICLES</code> <code>vehicles.mp4</code> Link <code>MILK_BOTTLING_PLANT</code> <code>milk-bottling-plant.mp4</code> Link <code>VEHICLES_2</code> <code>vehicles-2.mp4</code> Link <code>GROCERY_STORE</code> <code>grocery-store.mp4</code> Link <code>SUBWAY</code> <code>subway.mp4</code> Link <code>MARKET_SQUARE</code> <code>market-square.mp4</code> Link <code>PEOPLE_WALKING</code> <code>people-walking.mp4</code> Link Source code in <code>supervision/assets/list.py</code> <pre><code>class VideoAssets(Enum):\n    \"\"\"\n    Each member of this enum represents a video asset. The value associated with each\n    member is the filename of the video.\n\n    | Enum Member            | Video Filename             | Video URL                                                                             |\n    |------------------------|----------------------------|---------------------------------------------------------------------------------------|\n    | `VEHICLES`             | `vehicles.mp4`             | [Link](https://media.roboflow.com/supervision/video-examples/vehicles.mp4)            |\n    | `MILK_BOTTLING_PLANT`  | `milk-bottling-plant.mp4`  | [Link](https://media.roboflow.com/supervision/video-examples/milk-bottling-plant.mp4) |\n    | `VEHICLES_2`           | `vehicles-2.mp4`           | [Link](https://media.roboflow.com/supervision/video-examples/vehicles-2.mp4)          |\n    | `GROCERY_STORE`        | `grocery-store.mp4`        | [Link](https://media.roboflow.com/supervision/video-examples/grocery-store.mp4)       |\n    | `SUBWAY`               | `subway.mp4`               | [Link](https://media.roboflow.com/supervision/video-examples/subway.mp4)              |\n    | `MARKET_SQUARE`        | `market-square.mp4`        | [Link](https://media.roboflow.com/supervision/video-examples/market-square.mp4)       |\n    | `PEOPLE_WALKING`       | `people-walking.mp4`       | [Link](https://media.roboflow.com/supervision/video-examples/people-walking.mp4)      |\n    \"\"\"  # noqa: E501 // docs\n\n    VEHICLES = \"vehicles.mp4\"\n    MILK_BOTTLING_PLANT = \"milk-bottling-plant.mp4\"\n    VEHICLES_2 = \"vehicles-2.mp4\"\n    GROCERY_STORE = \"grocery-store.mp4\"\n    SUBWAY = \"subway.mp4\"\n    MARKET_SQUARE = \"market-square.mp4\"\n    PEOPLE_WALKING = \"people-walking.mp4\"\n\n    @classmethod\n    def list(cls):\n        return list(map(lambda c: c.value, cls))\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#0170-december-06-2023","title":"0.17.0 December 06, 2023","text":"<ul> <li> <p>Added #633: <code>sv.PixelateAnnotator</code> allowing to pixelate objects on images and videos.</p> </li> <li> <p>Added #652: <code>sv.TriangleAnnotator</code> allowing to annotate images and videos with triangle markers.</p> </li> <li> <p>Added #602: <code>sv.PolygonAnnotator</code> allowing to annotate images and videos with segmentation mask outline.</p> </li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; polygon_annotator = sv.PolygonAnnotator()\n&gt;&gt;&gt; annotated_frame = polygon_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <ul> <li>Added #476: <code>sv.assets</code> allowing download of video files that you can use in your demos.</li> </ul> <pre><code>&gt;&gt;&gt; from supervision.assets import download_assets, VideoAssets\n&gt;&gt;&gt; download_assets(VideoAssets.VEHICLES)\n\"vehicles.mp4\"\n</code></pre> <ul> <li> <p>Added #605: <code>Position.CENTER_OF_MASS</code> allowing to place labels in center of mass of segmentation masks.</p> </li> <li> <p>Added #651: <code>sv.scale_boxes</code> allowing to scale <code>sv.Detections.xyxy</code> values.</p> </li> <li> <p>Added #637: <code>sv.calculate_dynamic_text_scale</code> and <code>sv.calculate_dynamic_line_thickness</code> allowing text scale and line thickness to match image resolution.</p> </li> <li> <p>Added #620: <code>sv.Color.as_hex</code> allowing to extract color value in HEX format.</p> </li> <li> <p>Added #572: <code>sv.Classifications.from_timm</code> allowing to load classification result from timm models.</p> </li> <li> <p>Added #478: <code>sv.Classifications.from_clip</code> allowing to load classification result from clip model.</p> </li> <li> <p>Added #571: <code>sv.Detections.from_azure_analyze_image</code> allowing to load detection results from Azure Image Analysis.</p> </li> <li> <p>Changed #646: <code>sv.BoxMaskAnnotator</code> renaming it to <code>sv.ColorAnnotator</code>.</p> </li> <li> <p>Changed #606: <code>sv.MaskAnnotator</code> to make it 5x faster.</p> </li> <li> <p>Fixed #584: <code>sv.DetectionDataset.from_yolo</code> to ignore empty lines in annotation files.</p> </li> <li> <p>Fixed #555: <code>sv.BlurAnnotator</code> to trim negative coordinates before bluring detections.</p> </li> <li> <p>Fixed #511: <code>sv.TraceAnnotator</code> to respect trace position.</p> </li> </ul>"},{"location":"changelog/#0160-october-19-2023","title":"0.16.0 October 19, 2023","text":"<ul> <li> <p>Added #422: <code>sv.BoxMaskAnnotator</code> allowing to annotate images and videos with mox masks.</p> </li> <li> <p>Added #433: <code>sv.HaloAnnotator</code> allowing to annotate images and videos with halo effect.</p> </li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; halo_annotator = sv.HaloAnnotator()\n&gt;&gt;&gt; annotated_frame = halo_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <ul> <li> <p>Added #466: <code>sv.HeatMapAnnotator</code> allowing to annotate videos with heat maps.</p> </li> <li> <p>Added #492: <code>sv.DotAnnotator</code> allowing to annotate images and videos with dots.</p> </li> <li> <p>Added #449: <code>sv.draw_image</code> allowing to draw an image onto a given scene with specified opacity and dimensions.</p> </li> <li> <p>Added #280: <code>sv.FPSMonitor</code> for monitoring frames per second (FPS) to benchmark latency.</p> </li> <li> <p>Added #454: \ud83e\udd17 Hugging Face Annotators space.</p> </li> <li> <p>Changed #482: <code>sv.LineZone.tigger</code> now return <code>Tuple[np.ndarray, np.ndarray]</code>. The first array indicates which detections have crossed the line from outside to inside. The second array indicates which detections have crossed the line from inside to outside.</p> </li> <li> <p>Changed #465: Annotator argument name from <code>color_map: str</code> to <code>color_lookup: ColorLookup</code> enum to increase type safety.</p> </li> <li> <p>Changed #426: <code>sv.MaskAnnotator</code> allowing 2x faster annotation.</p> </li> <li> <p>Fixed #477: Poetry env definition allowing proper local installation.</p> </li> <li> <p>Fixed #430:  <code>sv.ByteTrack</code> to return <code>np.array([], dtype=int)</code> when <code>svDetections</code> is empty.</p> </li> </ul> <p>Warning</p> <p><code>sv.Detections.from_yolov8</code> and <code>sv.Classifications.from_yolov8</code> as those are now replaced by <code>sv.Detections.from_ultralytics</code> and <code>sv.Classifications.from_ultralytics</code>.</p>"},{"location":"changelog/#0150-october-5-2023","title":"0.15.0 October 5, 2023","text":"<ul> <li> <p>Added #170: <code>sv.BoundingBoxAnnotator</code> allowing to annotate images and videos with bounding boxes.</p> </li> <li> <p>Added #170: <code>sv.BoxCornerAnnotator</code> allowing to annotate images and videos with just bounding box corners.</p> </li> <li> <p>Added #170: <code>sv.MaskAnnotator</code> allowing to annotate images and videos with segmentation masks.</p> </li> <li> <p>Added #170: <code>sv.EllipseAnnotator</code> allowing to annotate images and videos with ellipses (sports game style).</p> </li> <li> <p>Added #386: <code>sv.CircleAnnotator</code> allowing to annotate images and videos with circles.</p> </li> <li> <p>Added #354: <code>sv.TraceAnnotator</code> allowing to draw path of moving objects on videos.</p> </li> <li> <p>Added #405: <code>sv.BlurAnnotator</code> allowing to blur objects on images and videos.</p> </li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; bounding_box_annotator = sv.BoundingBoxAnnotator()\n&gt;&gt;&gt; annotated_frame = bounding_box_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <ul> <li> <p>Added #354: Supervision usage example. You can now learn how to perform traffic flow analysis with Supervision.</p> </li> <li> <p>Changed #399: <code>sv.Detections.from_roboflow</code> now does not require <code>class_list</code> to be specified. The <code>class_id</code> value can be extracted directly from the inference response.</p> </li> <li> <p>Changed #381: <code>sv.VideoSink</code> now allows to customize the output codec.</p> </li> <li> <p>Changed #361: <code>sv.InferenceSlicer</code> can now operate in multithreading mode.</p> </li> <li> <p>Fixed #348: <code>sv.Detections.from_deepsparse</code> to allow processing empty deepsparse result object.</p> </li> </ul>"},{"location":"changelog/#0140-august-31-2023","title":"0.14.0 August 31, 2023","text":"<ul> <li>Added #282: support for SAHI inference technique with <code>sv.InferenceSlicer</code>.</li> </ul> <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n&gt;&gt;&gt; model = YOLO(...)\n\n&gt;&gt;&gt; def callback(image_slice: np.ndarray) -&gt; sv.Detections:\n...     result = model(image_slice)[0]\n...     return sv.Detections.from_ultralytics(result)\n\n&gt;&gt;&gt; slicer = sv.InferenceSlicer(callback = callback)\n\n&gt;&gt;&gt; detections = slicer(image)\n</code></pre> <ul> <li> <p>Added #297: <code>Detections.from_deepsparse</code> to enable seamless integration with DeepSparse framework.</p> </li> <li> <p>Added #281: <code>sv.Classifications.from_ultralytics</code> to enable seamless integration with Ultralytics framework. This will enable you to use supervision with all models that Ultralytics supports.</p> </li> </ul> <p>Warning</p> <p>sv.Detections.from_yolov8 and sv.Classifications.from_yolov8 are now deprecated and will be removed with supervision-0.16.0 release.</p> <ul> <li> <p>Added #341: First supervision usage example script showing how to detect and track objects on video using YOLOv8 + Supervision.</p> </li> <li> <p>Changed #296: <code>sv.ClassificationDataset</code> and <code>sv.DetectionDataset</code> now use image path (not image name) as dataset keys.</p> </li> <li> <p>Fixed #300: <code>Detections.from_roboflow</code> to filter out polygons with less than 3 points.</p> </li> </ul>"},{"location":"changelog/#0130-august-8-2023","title":"0.13.0 August 8, 2023","text":"<ul> <li>Added #236: support for mean average precision (mAP) for object detection models with <code>sv.MeanAveragePrecision</code>.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; dataset = sv.DetectionDataset.from_yolo(...)\n\n&gt;&gt;&gt; model = YOLO(...)\n&gt;&gt;&gt; def callback(image: np.ndarray) -&gt; sv.Detections:\n...     result = model(image)[0]\n...     return sv.Detections.from_yolov8(result)\n\n&gt;&gt;&gt; mean_average_precision = sv.MeanAveragePrecision.benchmark(\n...     dataset = dataset,\n...     callback = callback\n... )\n\n&gt;&gt;&gt; mean_average_precision.map50_95\n0.433\n</code></pre> <ul> <li> <p>Added #256: support for ByteTrack for object tracking with <code>sv.ByteTrack</code>.</p> </li> <li> <p>Added #222: <code>sv.Detections.from_ultralytics</code> to enable seamless integration with Ultralytics framework. This will enable you to use <code>supervision</code> with all models that Ultralytics supports.</p> </li> </ul> <p>Warning</p> <p><code>sv.Detections.from_yolov8</code> is now deprecated and will be removed with <code>supervision-0.15.0</code> release.</p> <ul> <li> <p>Added #191: <code>sv.Detections.from_paddledet</code> to enable seamless integration with PaddleDetection framework.</p> </li> <li> <p>Added #245: support for loading PASCAL VOC segmentation datasets with <code>sv.DetectionDataset.</code>.</p> </li> </ul>"},{"location":"changelog/#0120-july-24-2023","title":"0.12.0 July 24, 2023","text":"<p>Warning</p> <p>With the <code>supervision-0.12.0</code> release, we are terminating official support for Python 3.7.</p> <ul> <li>Added #177: initial support for object detection model benchmarking with <code>sv.ConfusionMatrix</code>.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; dataset = sv.DetectionDataset.from_yolo(...)\n\n&gt;&gt;&gt; model = YOLO(...)\n&gt;&gt;&gt; def callback(image: np.ndarray) -&gt; sv.Detections:\n...     result = model(image)[0]\n...     return sv.Detections.from_yolov8(result)\n\n&gt;&gt;&gt; confusion_matrix = sv.ConfusionMatrix.benchmark(\n...     dataset = dataset,\n...     callback = callback\n... )\n\n&gt;&gt;&gt; confusion_matrix.matrix\narray([\n    [0., 0., 0., 0.],\n    [0., 1., 0., 1.],\n    [0., 1., 1., 0.],\n    [1., 1., 0., 0.]\n])\n</code></pre> <ul> <li> <p>Added #173: <code>Detections.from_mmdetection</code> to enable seamless integration with MMDetection framework.</p> </li> <li> <p>Added #130: ability to install package in <code>headless</code> or <code>desktop</code> mode.</p> </li> <li> <p>Changed #180: packing method from <code>setup.py</code> to <code>pyproject.toml</code>.</p> </li> <li> <p>Fixed #188: <code>sv.DetectionDataset.from_cooc</code> can't be loaded when there are images without annotations.</p> </li> <li> <p>Fixed #226: <code>sv.DetectionDataset.from_yolo</code> can't load background instances.</p> </li> </ul>"},{"location":"changelog/#0111-june-29-2023","title":"0.11.1 June 29, 2023","text":"<ul> <li>Fix #165: <code>as_folder_structure</code> fails to save <code>sv.ClassificationDataset</code> when it is result of inference.</li> </ul>"},{"location":"changelog/#0110-june-28-2023","title":"0.11.0 June 28, 2023","text":"<ul> <li>Added #150: ability to load and save <code>sv.DetectionDataset</code> in COCO format using <code>as_coco</code> and <code>from_coco</code> methods.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; ds = sv.DetectionDataset.from_coco(\n...     images_directory_path='...',\n...     annotations_path='...'\n... )\n\n&gt;&gt;&gt; ds.as_coco(\n...     images_directory_path='...',\n...     annotations_path='...'\n... )\n</code></pre> <ul> <li>Added #158: ability to marge multiple <code>sv.DetectionDataset</code> together using <code>merge</code> method.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; ds_1 = sv.DetectionDataset(...)\n&gt;&gt;&gt; len(ds_1)\n100\n&gt;&gt;&gt; ds_1.classes\n['dog', 'person']\n\n&gt;&gt;&gt; ds_2 = sv.DetectionDataset(...)\n&gt;&gt;&gt; len(ds_2)\n200\n&gt;&gt;&gt; ds_2.classes\n['cat']\n\n&gt;&gt;&gt; ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\n&gt;&gt;&gt; len(ds_merged)\n300\n&gt;&gt;&gt; ds_merged.classes\n['cat', 'dog', 'person']\n</code></pre> <ul> <li> <p>Added #162: additional <code>start</code> and <code>end</code> arguments to <code>sv.get_video_frames_generator</code> allowing to generate frames only for a selected part of the video.</p> </li> <li> <p>Fix #157: incorrect loading of YOLO dataset class names from <code>data.yaml</code>.</p> </li> </ul>"},{"location":"changelog/#0100-june-14-2023","title":"0.10.0 June 14, 2023","text":"<ul> <li>Added #125: ability to load and save <code>sv.ClassificationDataset</code> in a folder structure format.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; cs = sv.ClassificationDataset.from_folder_structure(\n...     root_directory_path='...'\n... )\n\n&gt;&gt;&gt; cs.as_folder_structure(\n...     root_directory_path='...'\n... )\n</code></pre> <ul> <li> <p>Added #125: support for <code>sv.ClassificationDataset.split</code> allowing to divide <code>sv.ClassificationDataset</code> into two parts.</p> </li> <li> <p>Added #110: ability to extract masks from Roboflow API results using <code>sv.Detections.from_roboflow</code>.</p> </li> <li> <p>Added commit hash: Supervision Quickstart notebook where you can learn more about Detection, Dataset and Video APIs.</p> </li> <li> <p>Changed #135: <code>sv.get_video_frames_generator</code> documentation to better describe actual behavior.</p> </li> </ul>"},{"location":"changelog/#090-june-7-2023","title":"0.9.0 June 7, 2023","text":"<ul> <li>Added #118: ability to select <code>sv.Detections</code> by index, list of indexes or slice. Here is an example illustrating the new selection methods.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; detections = sv.Detections(...)\n&gt;&gt;&gt; len(detections[0])\n1\n&gt;&gt;&gt; len(detections[[0, 1]])\n2\n&gt;&gt;&gt; len(detections[0:2])\n2\n</code></pre> <ul> <li> <p>Added #101: ability to extract masks from YOLOv8 result using <code>sv.Detections.from_yolov8</code>. Here is an example illustrating how to extract boolean masks from the result of the YOLOv8 model inference.</p> </li> <li> <p>Added #122: ability to crop image using <code>sv.crop</code>. Here is an example showing how to get a separate crop for each detection in <code>sv.Detections</code>.</p> </li> <li> <p>Added #120: ability to conveniently save multiple images into directory using <code>sv.ImageSink</code>. Here is an example showing how to save every tenth video frame as a separate image.</p> </li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; with sv.ImageSink(target_dir_path='target/directory/path') as sink:\n...     for image in sv.get_video_frames_generator(source_path='source_video.mp4', stride=10):\n...         sink.save_image(image=image)\n</code></pre> <ul> <li>Fixed #106: inconvenient handling of <code>sv.PolygonZone</code> coordinates. Now <code>sv.PolygonZone</code> accepts coordinates in the form of <code>[[x1, y1], [x2, y2], ...]</code> that can be both integers and floats.</li> </ul>"},{"location":"changelog/#080-may-17-2023","title":"0.8.0 May 17, 2023","text":"<ul> <li>Added #100: support for dataset inheritance. The current <code>Dataset</code> got renamed to <code>DetectionDataset</code>. Now <code>DetectionDataset</code> inherits from <code>BaseDataset</code>. This change was made to enforce the future consistency of APIs of different types of computer vision datasets.</li> <li>Added #100: ability to save datasets in YOLO format using <code>DetectionDataset.as_yolo</code>.</li> </ul> <pre><code>&gt;&gt;&gt; import roboflow\n&gt;&gt;&gt; from roboflow import Roboflow\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; roboflow.login()\n\n&gt;&gt;&gt; rf = Roboflow()\n\n&gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n&gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"yolov5\")\n\n&gt;&gt;&gt; ds = sv.DetectionDataset.from_yolo(\n...     images_directory_path=f\"{dataset.location}/train/images\",\n...     annotations_directory_path=f\"{dataset.location}/train/labels\",\n...     data_yaml_path=f\"{dataset.location}/data.yaml\"\n... )\n\n&gt;&gt;&gt; ds.classes\n['dog', 'person']\n</code></pre> <ul> <li>Added #102: support for <code>DetectionDataset.split</code> allowing to divide <code>DetectionDataset</code> into two parts.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; ds = sv.DetectionDataset(...)\n&gt;&gt;&gt; train_ds, test_ds = ds.split(split_ratio=0.7, random_state=42, shuffle=True)\n\n&gt;&gt;&gt; len(train_ds), len(test_ds)\n(700, 300)\n</code></pre> <ul> <li>Changed #100: default value of <code>approximation_percentage</code> parameter from <code>0.75</code> to <code>0.0</code> in <code>DetectionDataset.as_yolo</code> and <code>DetectionDataset.as_pascal_voc</code>.</li> </ul>"},{"location":"changelog/#070-may-11-2023","title":"0.7.0 May 11, 2023","text":"<ul> <li>Added #91: <code>Detections.from_yolo_nas</code> to enable seamless integration with YOLO-NAS model.</li> <li>Added #86: ability to load datasets in YOLO format using <code>Dataset.from_yolo</code>.</li> <li>Added #84: <code>Detections.merge</code> to merge multiple <code>Detections</code> objects together.</li> <li>Fixed #81: <code>LineZoneAnnotator.annotate</code> does not return annotated frame.</li> <li>Changed #44: <code>LineZoneAnnotator.annotate</code> to allow for custom text for the in and out tags.</li> </ul>"},{"location":"changelog/#060-april-19-2023","title":"0.6.0 April 19, 2023","text":"<ul> <li>Added #71: initial <code>Dataset</code> support and ability to save <code>Detections</code> in Pascal VOC XML format.</li> <li>Added #71: new <code>mask_to_polygons</code>, <code>filter_polygons_by_area</code>, <code>polygon_to_xyxy</code> and <code>approximate_polygon</code> utilities.</li> <li>Added #72: ability to load Pascal VOC XML object detections dataset as <code>Dataset</code>.</li> <li>Changed #70: order of <code>Detections</code> attributes to make it consistent with order of objects in <code>__iter__</code> tuple.</li> <li>Changed #71: <code>generate_2d_mask</code> to <code>polygon_to_mask</code>.</li> </ul>"},{"location":"changelog/#052-april-13-2023","title":"0.5.2 April 13, 2023","text":"<ul> <li>Fixed #63: <code>LineZone.trigger</code> function expects 4 values instead of 5.</li> </ul>"},{"location":"changelog/#051-april-12-2023","title":"0.5.1 April 12, 2023","text":"<ul> <li>Fixed <code>Detections.__getitem__</code> method did not return mask for selected item.</li> <li>Fixed <code>Detections.area</code> crashed for mask detections.</li> </ul>"},{"location":"changelog/#050-april-10-2023","title":"0.5.0 April 10, 2023","text":"<ul> <li>Added #58: <code>Detections.mask</code> to enable segmentation support.</li> <li>Added #58: <code>MaskAnnotator</code> to allow easy <code>Detections.mask</code> annotation.</li> <li>Added #58: <code>Detections.from_sam</code> to enable native Segment Anything Model (SAM) support.</li> <li>Changed #58: <code>Detections.area</code> behaviour to work not only with boxes but also with masks.</li> </ul>"},{"location":"changelog/#040-april-5-2023","title":"0.4.0 April 5, 2023","text":"<ul> <li>Added #46: <code>Detections.empty</code> to allow easy creation of empty <code>Detections</code> objects.</li> <li>Added #56: <code>Detections.from_roboflow</code> to allow easy creation of <code>Detections</code> objects from Roboflow API inference results.</li> <li>Added #56: <code>plot_images_grid</code> to allow easy plotting of multiple images on single plot.</li> <li>Added #56: initial support for Pascal VOC XML format with <code>detections_to_voc_xml</code> method.</li> <li>Changed #56: <code>show_frame_in_notebook</code> refactored and renamed to <code>plot_image</code>.</li> </ul>"},{"location":"changelog/#032-march-23-2023","title":"0.3.2 March 23, 2023","text":"<ul> <li>Changed #50: Allow <code>Detections.class_id</code> to be <code>None</code>.</li> </ul>"},{"location":"changelog/#031-march-6-2023","title":"0.3.1 March 6, 2023","text":"<ul> <li>Fixed #41: <code>PolygonZone</code> throws an exception when the object touches the bottom edge of the image.</li> <li>Fixed #42: <code>Detections.wth_nms</code> method throws an exception when <code>Detections</code> is empty.</li> <li>Changed #36: <code>Detections.wth_nms</code> support class agnostic and non-class agnostic case.</li> </ul>"},{"location":"changelog/#030-march-6-2023","title":"0.3.0 March 6, 2023","text":"<ul> <li>Changed: Allow <code>Detections.confidence</code> to be <code>None</code>.</li> <li>Added: <code>Detections.from_transformers</code> and <code>Detections.from_detectron2</code> to enable seamless integration with Transformers and Detectron2 models.</li> <li>Added: <code>Detections.area</code> to dynamically calculate bounding box area.</li> <li>Added: <code>Detections.wth_nms</code> to filter out double detections with NMS. Initial - only class agnostic - implementation.</li> </ul>"},{"location":"changelog/#020-february-2-2023","title":"0.2.0 February 2, 2023","text":"<ul> <li>Added: Advanced <code>Detections</code> filtering with pandas-like API.</li> <li>Added: <code>Detections.from_yolov5</code> and <code>Detections.from_yolov8</code> to enable seamless integration with YOLOv5 and YOLOv8 models.</li> </ul>"},{"location":"changelog/#010-january-19-2023","title":"0.1.0 January 19, 2023","text":"<p>Say hello to Supervision \ud83d\udc4b</p>"},{"location":"datasets/","title":"Datasets","text":"<p>Warning</p> <p>Dataset API is still fluid and may change. If you use Dataset API in your project until further notice, freeze the <code>supervision</code> version in your <code>requirements.txt</code> or <code>setup.py</code>.</p>"},{"location":"datasets/#detectiondataset","title":"DetectionDataset","text":"<p>             Bases: <code>BaseDataset</code></p> <p>Dataclass containing information about object detection dataset.</p> <p>Attributes:</p> Name Type Description <code>classes</code> <code>List[str]</code> <p>List containing dataset class names.</p> <code>images</code> <code>Dict[str, ndarray]</code> <p>Dictionary mapping image name to image.</p> <code>annotations</code> <code>Dict[str, Detections]</code> <p>Dictionary mapping image name to annotations.</p> Source code in <code>supervision/dataset/core.py</code> <pre><code>@dataclass\nclass DetectionDataset(BaseDataset):\n    \"\"\"\n    Dataclass containing information about object detection dataset.\n\n    Attributes:\n        classes (List[str]): List containing dataset class names.\n        images (Dict[str, np.ndarray]): Dictionary mapping image name to image.\n        annotations (Dict[str, Detections]): Dictionary mapping\n            image name to annotations.\n    \"\"\"\n\n    classes: List[str]\n    images: Dict[str, np.ndarray]\n    annotations: Dict[str, Detections]\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return the number of images in the dataset.\n\n        Returns:\n            int: The number of images.\n        \"\"\"\n        return len(self.images)\n\n    def __iter__(self) -&gt; Iterator[Tuple[str, np.ndarray, Detections]]:\n        \"\"\"\n        Iterate over the images and annotations in the dataset.\n\n        Yields:\n            Iterator[Tuple[str, np.ndarray, Detections]]:\n                An iterator that yields tuples containing the image name,\n                the image data, and its corresponding annotation.\n        \"\"\"\n        for image_name, image in self.images.items():\n            yield image_name, image, self.annotations.get(image_name, None)\n\n    def __eq__(self, other):\n        if not isinstance(other, DetectionDataset):\n            return False\n\n        if set(self.classes) != set(other.classes):\n            return False\n\n        for key in self.images:\n            if not np.array_equal(self.images[key], other.images[key]):\n                return False\n            if not self.annotations[key] == other.annotations[key]:\n                return False\n\n        return True\n\n    def split(\n        self, split_ratio=0.8, random_state=None, shuffle: bool = True\n    ) -&gt; Tuple[DetectionDataset, DetectionDataset]:\n        \"\"\"\n        Splits the dataset into two parts (training and testing)\n            using the provided split_ratio.\n\n        Args:\n            split_ratio (float, optional): The ratio of the training\n                set to the entire dataset.\n            random_state (int, optional): The seed for the random number generator.\n                This is used for reproducibility.\n            shuffle (bool, optional): Whether to shuffle the data before splitting.\n\n        Returns:\n            Tuple[DetectionDataset, DetectionDataset]: A tuple containing\n                the training and testing datasets.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; ds = sv.DetectionDataset(...)\n            &gt;&gt;&gt; train_ds, test_ds = ds.split(split_ratio=0.7,\n            ...                              random_state=42, shuffle=True)\n            &gt;&gt;&gt; len(train_ds), len(test_ds)\n            (700, 300)\n            ```\n        \"\"\"\n\n        image_names = list(self.images.keys())\n        train_names, test_names = train_test_split(\n            data=image_names,\n            train_ratio=split_ratio,\n            random_state=random_state,\n            shuffle=shuffle,\n        )\n\n        train_dataset = DetectionDataset(\n            classes=self.classes,\n            images={name: self.images[name] for name in train_names},\n            annotations={name: self.annotations[name] for name in train_names},\n        )\n        test_dataset = DetectionDataset(\n            classes=self.classes,\n            images={name: self.images[name] for name in test_names},\n            annotations={name: self.annotations[name] for name in test_names},\n        )\n        return train_dataset, test_dataset\n\n    def as_pascal_voc(\n        self,\n        images_directory_path: Optional[str] = None,\n        annotations_directory_path: Optional[str] = None,\n        min_image_area_percentage: float = 0.0,\n        max_image_area_percentage: float = 1.0,\n        approximation_percentage: float = 0.0,\n    ) -&gt; None:\n        \"\"\"\n        Exports the dataset to PASCAL VOC format. This method saves the images\n        and their corresponding annotations in PASCAL VOC format.\n\n        Args:\n            images_directory_path (Optional[str]): The path to the directory\n                where the images should be saved.\n                If not provided, images will not be saved.\n            annotations_directory_path (Optional[str]): The path to\n                the directory where the annotations in PASCAL VOC format should be\n                saved. If not provided, annotations will not be saved.\n            min_image_area_percentage (float): The minimum percentage of\n                detection area relative to\n                the image area for a detection to be included.\n                Argument is used only for segmentation datasets.\n            max_image_area_percentage (float): The maximum percentage\n                of detection area relative to\n                the image area for a detection to be included.\n                Argument is used only for segmentation datasets.\n            approximation_percentage (float): The percentage of\n                polygon points to be removed from the input polygon,\n                in the range [0, 1). Argument is used only for segmentation datasets.\n        \"\"\"\n        if images_directory_path:\n            save_dataset_images(\n                images_directory_path=images_directory_path, images=self.images\n            )\n        if annotations_directory_path:\n            Path(annotations_directory_path).mkdir(parents=True, exist_ok=True)\n\n        for image_path, image in self.images.items():\n            detections = self.annotations[image_path]\n\n            if annotations_directory_path:\n                annotation_name = Path(image_path).stem\n                annotations_path = os.path.join(\n                    annotations_directory_path, f\"{annotation_name}.xml\"\n                )\n                image_name = Path(image_path).name\n                pascal_voc_xml = detections_to_pascal_voc(\n                    detections=detections,\n                    classes=self.classes,\n                    filename=image_name,\n                    image_shape=image.shape,\n                    min_image_area_percentage=min_image_area_percentage,\n                    max_image_area_percentage=max_image_area_percentage,\n                    approximation_percentage=approximation_percentage,\n                )\n\n                with open(annotations_path, \"w\") as f:\n                    f.write(pascal_voc_xml)\n\n    @classmethod\n    def from_pascal_voc(\n        cls,\n        images_directory_path: str,\n        annotations_directory_path: str,\n        force_masks: bool = False,\n    ) -&gt; DetectionDataset:\n        \"\"\"\n        Creates a Dataset instance from PASCAL VOC formatted data.\n\n        Args:\n            images_directory_path (str): Path to the directory containing the images.\n            annotations_directory_path (str): Path to the directory\n                containing the PASCAL VOC XML annotations.\n            force_masks (bool, optional): If True, forces masks to\n                be loaded for all annotations, regardless of whether they are present.\n\n        Returns:\n            DetectionDataset: A DetectionDataset instance containing\n                the loaded images and annotations.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import roboflow\n            &gt;&gt;&gt; from roboflow import Roboflow\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; roboflow.login()\n\n            &gt;&gt;&gt; rf = Roboflow()\n\n            &gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n            &gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"voc\")\n\n            &gt;&gt;&gt; ds = sv.DetectionDataset.from_pascal_voc(\n            ...     images_directory_path=f\"{dataset.location}/train/images\",\n            ...     annotations_directory_path=f\"{dataset.location}/train/labels\"\n            ... )\n\n            &gt;&gt;&gt; ds.classes\n            ['dog', 'person']\n            ```\n        \"\"\"\n\n        classes, images, annotations = load_pascal_voc_annotations(\n            images_directory_path=images_directory_path,\n            annotations_directory_path=annotations_directory_path,\n            force_masks=force_masks,\n        )\n\n        return DetectionDataset(classes=classes, images=images, annotations=annotations)\n\n    @classmethod\n    def from_yolo(\n        cls,\n        images_directory_path: str,\n        annotations_directory_path: str,\n        data_yaml_path: str,\n        force_masks: bool = False,\n    ) -&gt; DetectionDataset:\n        \"\"\"\n        Creates a Dataset instance from YOLO formatted data.\n\n        Args:\n            images_directory_path (str): The path to the\n                directory containing the images.\n            annotations_directory_path (str): The path to the directory\n                containing the YOLO annotation files.\n            data_yaml_path (str): The path to the data\n                YAML file containing class information.\n            force_masks (bool, optional): If True, forces\n                masks to be loaded for all annotations,\n                regardless of whether they are present.\n\n        Returns:\n            DetectionDataset: A DetectionDataset instance\n                containing the loaded images and annotations.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import roboflow\n            &gt;&gt;&gt; from roboflow import Roboflow\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; roboflow.login()\n\n            &gt;&gt;&gt; rf = Roboflow()\n\n            &gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n            &gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"yolov5\")\n\n            &gt;&gt;&gt; ds = sv.DetectionDataset.from_yolo(\n            ...     images_directory_path=f\"{dataset.location}/train/images\",\n            ...     annotations_directory_path=f\"{dataset.location}/train/labels\",\n            ...     data_yaml_path=f\"{dataset.location}/data.yaml\"\n            ... )\n\n            &gt;&gt;&gt; ds.classes\n            ['dog', 'person']\n            ```\n        \"\"\"\n        classes, images, annotations = load_yolo_annotations(\n            images_directory_path=images_directory_path,\n            annotations_directory_path=annotations_directory_path,\n            data_yaml_path=data_yaml_path,\n            force_masks=force_masks,\n        )\n        return DetectionDataset(classes=classes, images=images, annotations=annotations)\n\n    def as_yolo(\n        self,\n        images_directory_path: Optional[str] = None,\n        annotations_directory_path: Optional[str] = None,\n        data_yaml_path: Optional[str] = None,\n        min_image_area_percentage: float = 0.0,\n        max_image_area_percentage: float = 1.0,\n        approximation_percentage: float = 0.0,\n    ) -&gt; None:\n        \"\"\"\n        Exports the dataset to YOLO format. This method saves the\n        images and their corresponding annotations in YOLO format.\n\n        Args:\n            images_directory_path (Optional[str]): The path to the\n                directory where the images should be saved.\n                If not provided, images will not be saved.\n            annotations_directory_path (Optional[str]): The path to the\n                directory where the annotations in\n                YOLO format should be saved. If not provided,\n                annotations will not be saved.\n            data_yaml_path (Optional[str]): The path where the data.yaml\n                file should be saved.\n                If not provided, the file will not be saved.\n            min_image_area_percentage (float): The minimum percentage of\n                detection area relative to\n                the image area for a detection to be included.\n                Argument is used only for segmentation datasets.\n            max_image_area_percentage (float): The maximum percentage\n                of detection area relative to\n                the image area for a detection to be included.\n                Argument is used only for segmentation datasets.\n            approximation_percentage (float): The percentage of polygon points to\n                be removed from the input polygon, in the range [0, 1).\n                This is useful for simplifying the annotations.\n                Argument is used only for segmentation datasets.\n        \"\"\"\n        if images_directory_path is not None:\n            save_dataset_images(\n                images_directory_path=images_directory_path, images=self.images\n            )\n        if annotations_directory_path is not None:\n            save_yolo_annotations(\n                annotations_directory_path=annotations_directory_path,\n                images=self.images,\n                annotations=self.annotations,\n                min_image_area_percentage=min_image_area_percentage,\n                max_image_area_percentage=max_image_area_percentage,\n                approximation_percentage=approximation_percentage,\n            )\n        if data_yaml_path is not None:\n            save_data_yaml(data_yaml_path=data_yaml_path, classes=self.classes)\n\n    @classmethod\n    def from_coco(\n        cls,\n        images_directory_path: str,\n        annotations_path: str,\n        force_masks: bool = False,\n    ) -&gt; DetectionDataset:\n        \"\"\"\n        Creates a Dataset instance from COCO formatted data.\n\n        Args:\n            images_directory_path (str): The path to the\n                directory containing the images.\n            annotations_path (str): The path to the json annotation files.\n            force_masks (bool, optional): If True,\n                forces masks to be loaded for all annotations,\n                regardless of whether they are present.\n\n        Returns:\n            DetectionDataset: A DetectionDataset instance containing\n                the loaded images and annotations.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import roboflow\n            &gt;&gt;&gt; from roboflow import Roboflow\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; roboflow.login()\n\n            &gt;&gt;&gt; rf = Roboflow()\n\n            &gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n            &gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"coco\")\n\n            &gt;&gt;&gt; ds = sv.DetectionDataset.from_coco(\n            ...     images_directory_path=f\"{dataset.location}/train\",\n            ...     annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n            ... )\n\n            &gt;&gt;&gt; ds.classes\n            ['dog', 'person']\n            ```\n        \"\"\"\n        classes, images, annotations = load_coco_annotations(\n            images_directory_path=images_directory_path,\n            annotations_path=annotations_path,\n            force_masks=force_masks,\n        )\n        return DetectionDataset(classes=classes, images=images, annotations=annotations)\n\n    def as_coco(\n        self,\n        images_directory_path: Optional[str] = None,\n        annotations_path: Optional[str] = None,\n        min_image_area_percentage: float = 0.0,\n        max_image_area_percentage: float = 1.0,\n        approximation_percentage: float = 0.0,\n    ) -&gt; None:\n        \"\"\"\n        Exports the dataset to COCO format. This method saves the\n        images and their corresponding annotations in COCO format.\n\n        Args:\n            images_directory_path (Optional[str]): The path to the directory\n                where the images should be saved.\n                If not provided, images will not be saved.\n            annotations_path (Optional[str]): The path to COCO annotation file.\n            min_image_area_percentage (float): The minimum percentage of\n                detection area relative to\n                the image area for a detection to be included.\n                Argument is used only for segmentation datasets.\n            max_image_area_percentage (float): The maximum percentage of\n                detection area relative to\n                the image area for a detection to be included.\n                Argument is used only for segmentation datasets.\n            approximation_percentage (float): The percentage of polygon points\n                to be removed from the input polygon,\n                in the range [0, 1). This is useful for simplifying the annotations.\n                Argument is used only for segmentation datasets.\n        \"\"\"\n        if images_directory_path is not None:\n            save_dataset_images(\n                images_directory_path=images_directory_path, images=self.images\n            )\n        if annotations_path is not None:\n            save_coco_annotations(\n                annotation_path=annotations_path,\n                images=self.images,\n                annotations=self.annotations,\n                classes=self.classes,\n                min_image_area_percentage=min_image_area_percentage,\n                max_image_area_percentage=max_image_area_percentage,\n                approximation_percentage=approximation_percentage,\n            )\n\n    @classmethod\n    def merge(cls, dataset_list: List[DetectionDataset]) -&gt; DetectionDataset:\n        \"\"\"\n        Merge a list of `DetectionDataset` objects into a single\n            `DetectionDataset` object.\n\n        This method takes a list of `DetectionDataset` objects and combines\n        their respective fields (`classes`, `images`,\n        `annotations`) into a single `DetectionDataset` object.\n\n        Args:\n            dataset_list (List[DetectionDataset]): A list of `DetectionDataset`\n                objects to merge.\n\n        Returns:\n            (DetectionDataset): A single `DetectionDataset` object containing\n            the merged data from the input list.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; ds_1 = sv.DetectionDataset(...)\n            &gt;&gt;&gt; len(ds_1)\n            100\n            &gt;&gt;&gt; ds_1.classes\n            ['dog', 'person']\n\n            &gt;&gt;&gt; ds_2 = sv.DetectionDataset(...)\n            &gt;&gt;&gt; len(ds_2)\n            200\n            &gt;&gt;&gt; ds_2.classes\n            ['cat']\n\n            &gt;&gt;&gt; ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\n            &gt;&gt;&gt; len(ds_merged)\n            300\n            &gt;&gt;&gt; ds_merged.classes\n            ['cat', 'dog', 'person']\n            ```\n        \"\"\"\n        merged_images, merged_annotations = {}, {}\n        class_lists = [dataset.classes for dataset in dataset_list]\n        merged_classes = merge_class_lists(class_lists=class_lists)\n\n        for dataset in dataset_list:\n            class_index_mapping = build_class_index_mapping(\n                source_classes=dataset.classes, target_classes=merged_classes\n            )\n            for image_name, image, detections in dataset:\n                if image_name in merged_annotations:\n                    raise ValueError(\n                        f\"Image name {image_name} is not unique across datasets.\"\n                    )\n\n                merged_images[image_name] = image\n                merged_annotations[image_name] = map_detections_class_id(\n                    source_to_target_mapping=class_index_mapping,\n                    detections=detections,\n                )\n\n        return cls(\n            classes=merged_classes, images=merged_images, annotations=merged_annotations\n        )\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.DetectionDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the images and annotations in the dataset.</p> <p>Yields:</p> Type Description <code>str</code> <p>Iterator[Tuple[str, np.ndarray, Detections]]: An iterator that yields tuples containing the image name, the image data, and its corresponding annotation.</p> Source code in <code>supervision/dataset/core.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Tuple[str, np.ndarray, Detections]]:\n    \"\"\"\n    Iterate over the images and annotations in the dataset.\n\n    Yields:\n        Iterator[Tuple[str, np.ndarray, Detections]]:\n            An iterator that yields tuples containing the image name,\n            the image data, and its corresponding annotation.\n    \"\"\"\n    for image_name, image in self.images.items():\n        yield image_name, image, self.annotations.get(image_name, None)\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.DetectionDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of images in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of images.</p> Source code in <code>supervision/dataset/core.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the number of images in the dataset.\n\n    Returns:\n        int: The number of images.\n    \"\"\"\n    return len(self.images)\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.DetectionDataset.as_coco","title":"<code>as_coco(images_directory_path=None, annotations_path=None, min_image_area_percentage=0.0, max_image_area_percentage=1.0, approximation_percentage=0.0)</code>","text":"<p>Exports the dataset to COCO format. This method saves the images and their corresponding annotations in COCO format.</p> <p>Parameters:</p> Name Type Description Default <code>images_directory_path</code> <code>Optional[str]</code> <p>The path to the directory where the images should be saved. If not provided, images will not be saved.</p> <code>None</code> <code>annotations_path</code> <code>Optional[str]</code> <p>The path to COCO annotation file.</p> <code>None</code> <code>min_image_area_percentage</code> <code>float</code> <p>The minimum percentage of detection area relative to the image area for a detection to be included. Argument is used only for segmentation datasets.</p> <code>0.0</code> <code>max_image_area_percentage</code> <code>float</code> <p>The maximum percentage of detection area relative to the image area for a detection to be included. Argument is used only for segmentation datasets.</p> <code>1.0</code> <code>approximation_percentage</code> <code>float</code> <p>The percentage of polygon points to be removed from the input polygon, in the range [0, 1). This is useful for simplifying the annotations. Argument is used only for segmentation datasets.</p> <code>0.0</code> Source code in <code>supervision/dataset/core.py</code> <pre><code>def as_coco(\n    self,\n    images_directory_path: Optional[str] = None,\n    annotations_path: Optional[str] = None,\n    min_image_area_percentage: float = 0.0,\n    max_image_area_percentage: float = 1.0,\n    approximation_percentage: float = 0.0,\n) -&gt; None:\n    \"\"\"\n    Exports the dataset to COCO format. This method saves the\n    images and their corresponding annotations in COCO format.\n\n    Args:\n        images_directory_path (Optional[str]): The path to the directory\n            where the images should be saved.\n            If not provided, images will not be saved.\n        annotations_path (Optional[str]): The path to COCO annotation file.\n        min_image_area_percentage (float): The minimum percentage of\n            detection area relative to\n            the image area for a detection to be included.\n            Argument is used only for segmentation datasets.\n        max_image_area_percentage (float): The maximum percentage of\n            detection area relative to\n            the image area for a detection to be included.\n            Argument is used only for segmentation datasets.\n        approximation_percentage (float): The percentage of polygon points\n            to be removed from the input polygon,\n            in the range [0, 1). This is useful for simplifying the annotations.\n            Argument is used only for segmentation datasets.\n    \"\"\"\n    if images_directory_path is not None:\n        save_dataset_images(\n            images_directory_path=images_directory_path, images=self.images\n        )\n    if annotations_path is not None:\n        save_coco_annotations(\n            annotation_path=annotations_path,\n            images=self.images,\n            annotations=self.annotations,\n            classes=self.classes,\n            min_image_area_percentage=min_image_area_percentage,\n            max_image_area_percentage=max_image_area_percentage,\n            approximation_percentage=approximation_percentage,\n        )\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.DetectionDataset.as_pascal_voc","title":"<code>as_pascal_voc(images_directory_path=None, annotations_directory_path=None, min_image_area_percentage=0.0, max_image_area_percentage=1.0, approximation_percentage=0.0)</code>","text":"<p>Exports the dataset to PASCAL VOC format. This method saves the images and their corresponding annotations in PASCAL VOC format.</p> <p>Parameters:</p> Name Type Description Default <code>images_directory_path</code> <code>Optional[str]</code> <p>The path to the directory where the images should be saved. If not provided, images will not be saved.</p> <code>None</code> <code>annotations_directory_path</code> <code>Optional[str]</code> <p>The path to the directory where the annotations in PASCAL VOC format should be saved. If not provided, annotations will not be saved.</p> <code>None</code> <code>min_image_area_percentage</code> <code>float</code> <p>The minimum percentage of detection area relative to the image area for a detection to be included. Argument is used only for segmentation datasets.</p> <code>0.0</code> <code>max_image_area_percentage</code> <code>float</code> <p>The maximum percentage of detection area relative to the image area for a detection to be included. Argument is used only for segmentation datasets.</p> <code>1.0</code> <code>approximation_percentage</code> <code>float</code> <p>The percentage of polygon points to be removed from the input polygon, in the range [0, 1). Argument is used only for segmentation datasets.</p> <code>0.0</code> Source code in <code>supervision/dataset/core.py</code> <pre><code>def as_pascal_voc(\n    self,\n    images_directory_path: Optional[str] = None,\n    annotations_directory_path: Optional[str] = None,\n    min_image_area_percentage: float = 0.0,\n    max_image_area_percentage: float = 1.0,\n    approximation_percentage: float = 0.0,\n) -&gt; None:\n    \"\"\"\n    Exports the dataset to PASCAL VOC format. This method saves the images\n    and their corresponding annotations in PASCAL VOC format.\n\n    Args:\n        images_directory_path (Optional[str]): The path to the directory\n            where the images should be saved.\n            If not provided, images will not be saved.\n        annotations_directory_path (Optional[str]): The path to\n            the directory where the annotations in PASCAL VOC format should be\n            saved. If not provided, annotations will not be saved.\n        min_image_area_percentage (float): The minimum percentage of\n            detection area relative to\n            the image area for a detection to be included.\n            Argument is used only for segmentation datasets.\n        max_image_area_percentage (float): The maximum percentage\n            of detection area relative to\n            the image area for a detection to be included.\n            Argument is used only for segmentation datasets.\n        approximation_percentage (float): The percentage of\n            polygon points to be removed from the input polygon,\n            in the range [0, 1). Argument is used only for segmentation datasets.\n    \"\"\"\n    if images_directory_path:\n        save_dataset_images(\n            images_directory_path=images_directory_path, images=self.images\n        )\n    if annotations_directory_path:\n        Path(annotations_directory_path).mkdir(parents=True, exist_ok=True)\n\n    for image_path, image in self.images.items():\n        detections = self.annotations[image_path]\n\n        if annotations_directory_path:\n            annotation_name = Path(image_path).stem\n            annotations_path = os.path.join(\n                annotations_directory_path, f\"{annotation_name}.xml\"\n            )\n            image_name = Path(image_path).name\n            pascal_voc_xml = detections_to_pascal_voc(\n                detections=detections,\n                classes=self.classes,\n                filename=image_name,\n                image_shape=image.shape,\n                min_image_area_percentage=min_image_area_percentage,\n                max_image_area_percentage=max_image_area_percentage,\n                approximation_percentage=approximation_percentage,\n            )\n\n            with open(annotations_path, \"w\") as f:\n                f.write(pascal_voc_xml)\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.DetectionDataset.as_yolo","title":"<code>as_yolo(images_directory_path=None, annotations_directory_path=None, data_yaml_path=None, min_image_area_percentage=0.0, max_image_area_percentage=1.0, approximation_percentage=0.0)</code>","text":"<p>Exports the dataset to YOLO format. This method saves the images and their corresponding annotations in YOLO format.</p> <p>Parameters:</p> Name Type Description Default <code>images_directory_path</code> <code>Optional[str]</code> <p>The path to the directory where the images should be saved. If not provided, images will not be saved.</p> <code>None</code> <code>annotations_directory_path</code> <code>Optional[str]</code> <p>The path to the directory where the annotations in YOLO format should be saved. If not provided, annotations will not be saved.</p> <code>None</code> <code>data_yaml_path</code> <code>Optional[str]</code> <p>The path where the data.yaml file should be saved. If not provided, the file will not be saved.</p> <code>None</code> <code>min_image_area_percentage</code> <code>float</code> <p>The minimum percentage of detection area relative to the image area for a detection to be included. Argument is used only for segmentation datasets.</p> <code>0.0</code> <code>max_image_area_percentage</code> <code>float</code> <p>The maximum percentage of detection area relative to the image area for a detection to be included. Argument is used only for segmentation datasets.</p> <code>1.0</code> <code>approximation_percentage</code> <code>float</code> <p>The percentage of polygon points to be removed from the input polygon, in the range [0, 1). This is useful for simplifying the annotations. Argument is used only for segmentation datasets.</p> <code>0.0</code> Source code in <code>supervision/dataset/core.py</code> <pre><code>def as_yolo(\n    self,\n    images_directory_path: Optional[str] = None,\n    annotations_directory_path: Optional[str] = None,\n    data_yaml_path: Optional[str] = None,\n    min_image_area_percentage: float = 0.0,\n    max_image_area_percentage: float = 1.0,\n    approximation_percentage: float = 0.0,\n) -&gt; None:\n    \"\"\"\n    Exports the dataset to YOLO format. This method saves the\n    images and their corresponding annotations in YOLO format.\n\n    Args:\n        images_directory_path (Optional[str]): The path to the\n            directory where the images should be saved.\n            If not provided, images will not be saved.\n        annotations_directory_path (Optional[str]): The path to the\n            directory where the annotations in\n            YOLO format should be saved. If not provided,\n            annotations will not be saved.\n        data_yaml_path (Optional[str]): The path where the data.yaml\n            file should be saved.\n            If not provided, the file will not be saved.\n        min_image_area_percentage (float): The minimum percentage of\n            detection area relative to\n            the image area for a detection to be included.\n            Argument is used only for segmentation datasets.\n        max_image_area_percentage (float): The maximum percentage\n            of detection area relative to\n            the image area for a detection to be included.\n            Argument is used only for segmentation datasets.\n        approximation_percentage (float): The percentage of polygon points to\n            be removed from the input polygon, in the range [0, 1).\n            This is useful for simplifying the annotations.\n            Argument is used only for segmentation datasets.\n    \"\"\"\n    if images_directory_path is not None:\n        save_dataset_images(\n            images_directory_path=images_directory_path, images=self.images\n        )\n    if annotations_directory_path is not None:\n        save_yolo_annotations(\n            annotations_directory_path=annotations_directory_path,\n            images=self.images,\n            annotations=self.annotations,\n            min_image_area_percentage=min_image_area_percentage,\n            max_image_area_percentage=max_image_area_percentage,\n            approximation_percentage=approximation_percentage,\n        )\n    if data_yaml_path is not None:\n        save_data_yaml(data_yaml_path=data_yaml_path, classes=self.classes)\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.DetectionDataset.from_coco","title":"<code>from_coco(images_directory_path, annotations_path, force_masks=False)</code>  <code>classmethod</code>","text":"<p>Creates a Dataset instance from COCO formatted data.</p> <p>Parameters:</p> Name Type Description Default <code>images_directory_path</code> <code>str</code> <p>The path to the directory containing the images.</p> required <code>annotations_path</code> <code>str</code> <p>The path to the json annotation files.</p> required <code>force_masks</code> <code>bool</code> <p>If True, forces masks to be loaded for all annotations, regardless of whether they are present.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DetectionDataset</code> <code>DetectionDataset</code> <p>A DetectionDataset instance containing the loaded images and annotations.</p> Example <pre><code>&gt;&gt;&gt; import roboflow\n&gt;&gt;&gt; from roboflow import Roboflow\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; roboflow.login()\n\n&gt;&gt;&gt; rf = Roboflow()\n\n&gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n&gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"coco\")\n\n&gt;&gt;&gt; ds = sv.DetectionDataset.from_coco(\n...     images_directory_path=f\"{dataset.location}/train\",\n...     annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n... )\n\n&gt;&gt;&gt; ds.classes\n['dog', 'person']\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>@classmethod\ndef from_coco(\n    cls,\n    images_directory_path: str,\n    annotations_path: str,\n    force_masks: bool = False,\n) -&gt; DetectionDataset:\n    \"\"\"\n    Creates a Dataset instance from COCO formatted data.\n\n    Args:\n        images_directory_path (str): The path to the\n            directory containing the images.\n        annotations_path (str): The path to the json annotation files.\n        force_masks (bool, optional): If True,\n            forces masks to be loaded for all annotations,\n            regardless of whether they are present.\n\n    Returns:\n        DetectionDataset: A DetectionDataset instance containing\n            the loaded images and annotations.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import roboflow\n        &gt;&gt;&gt; from roboflow import Roboflow\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; roboflow.login()\n\n        &gt;&gt;&gt; rf = Roboflow()\n\n        &gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n        &gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"coco\")\n\n        &gt;&gt;&gt; ds = sv.DetectionDataset.from_coco(\n        ...     images_directory_path=f\"{dataset.location}/train\",\n        ...     annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n        ... )\n\n        &gt;&gt;&gt; ds.classes\n        ['dog', 'person']\n        ```\n    \"\"\"\n    classes, images, annotations = load_coco_annotations(\n        images_directory_path=images_directory_path,\n        annotations_path=annotations_path,\n        force_masks=force_masks,\n    )\n    return DetectionDataset(classes=classes, images=images, annotations=annotations)\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.DetectionDataset.from_pascal_voc","title":"<code>from_pascal_voc(images_directory_path, annotations_directory_path, force_masks=False)</code>  <code>classmethod</code>","text":"<p>Creates a Dataset instance from PASCAL VOC formatted data.</p> <p>Parameters:</p> Name Type Description Default <code>images_directory_path</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>annotations_directory_path</code> <code>str</code> <p>Path to the directory containing the PASCAL VOC XML annotations.</p> required <code>force_masks</code> <code>bool</code> <p>If True, forces masks to be loaded for all annotations, regardless of whether they are present.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DetectionDataset</code> <code>DetectionDataset</code> <p>A DetectionDataset instance containing the loaded images and annotations.</p> Example <pre><code>&gt;&gt;&gt; import roboflow\n&gt;&gt;&gt; from roboflow import Roboflow\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; roboflow.login()\n\n&gt;&gt;&gt; rf = Roboflow()\n\n&gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n&gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"voc\")\n\n&gt;&gt;&gt; ds = sv.DetectionDataset.from_pascal_voc(\n...     images_directory_path=f\"{dataset.location}/train/images\",\n...     annotations_directory_path=f\"{dataset.location}/train/labels\"\n... )\n\n&gt;&gt;&gt; ds.classes\n['dog', 'person']\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>@classmethod\ndef from_pascal_voc(\n    cls,\n    images_directory_path: str,\n    annotations_directory_path: str,\n    force_masks: bool = False,\n) -&gt; DetectionDataset:\n    \"\"\"\n    Creates a Dataset instance from PASCAL VOC formatted data.\n\n    Args:\n        images_directory_path (str): Path to the directory containing the images.\n        annotations_directory_path (str): Path to the directory\n            containing the PASCAL VOC XML annotations.\n        force_masks (bool, optional): If True, forces masks to\n            be loaded for all annotations, regardless of whether they are present.\n\n    Returns:\n        DetectionDataset: A DetectionDataset instance containing\n            the loaded images and annotations.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import roboflow\n        &gt;&gt;&gt; from roboflow import Roboflow\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; roboflow.login()\n\n        &gt;&gt;&gt; rf = Roboflow()\n\n        &gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n        &gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"voc\")\n\n        &gt;&gt;&gt; ds = sv.DetectionDataset.from_pascal_voc(\n        ...     images_directory_path=f\"{dataset.location}/train/images\",\n        ...     annotations_directory_path=f\"{dataset.location}/train/labels\"\n        ... )\n\n        &gt;&gt;&gt; ds.classes\n        ['dog', 'person']\n        ```\n    \"\"\"\n\n    classes, images, annotations = load_pascal_voc_annotations(\n        images_directory_path=images_directory_path,\n        annotations_directory_path=annotations_directory_path,\n        force_masks=force_masks,\n    )\n\n    return DetectionDataset(classes=classes, images=images, annotations=annotations)\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.DetectionDataset.from_yolo","title":"<code>from_yolo(images_directory_path, annotations_directory_path, data_yaml_path, force_masks=False)</code>  <code>classmethod</code>","text":"<p>Creates a Dataset instance from YOLO formatted data.</p> <p>Parameters:</p> Name Type Description Default <code>images_directory_path</code> <code>str</code> <p>The path to the directory containing the images.</p> required <code>annotations_directory_path</code> <code>str</code> <p>The path to the directory containing the YOLO annotation files.</p> required <code>data_yaml_path</code> <code>str</code> <p>The path to the data YAML file containing class information.</p> required <code>force_masks</code> <code>bool</code> <p>If True, forces masks to be loaded for all annotations, regardless of whether they are present.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DetectionDataset</code> <code>DetectionDataset</code> <p>A DetectionDataset instance containing the loaded images and annotations.</p> Example <pre><code>&gt;&gt;&gt; import roboflow\n&gt;&gt;&gt; from roboflow import Roboflow\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; roboflow.login()\n\n&gt;&gt;&gt; rf = Roboflow()\n\n&gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n&gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"yolov5\")\n\n&gt;&gt;&gt; ds = sv.DetectionDataset.from_yolo(\n...     images_directory_path=f\"{dataset.location}/train/images\",\n...     annotations_directory_path=f\"{dataset.location}/train/labels\",\n...     data_yaml_path=f\"{dataset.location}/data.yaml\"\n... )\n\n&gt;&gt;&gt; ds.classes\n['dog', 'person']\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>@classmethod\ndef from_yolo(\n    cls,\n    images_directory_path: str,\n    annotations_directory_path: str,\n    data_yaml_path: str,\n    force_masks: bool = False,\n) -&gt; DetectionDataset:\n    \"\"\"\n    Creates a Dataset instance from YOLO formatted data.\n\n    Args:\n        images_directory_path (str): The path to the\n            directory containing the images.\n        annotations_directory_path (str): The path to the directory\n            containing the YOLO annotation files.\n        data_yaml_path (str): The path to the data\n            YAML file containing class information.\n        force_masks (bool, optional): If True, forces\n            masks to be loaded for all annotations,\n            regardless of whether they are present.\n\n    Returns:\n        DetectionDataset: A DetectionDataset instance\n            containing the loaded images and annotations.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import roboflow\n        &gt;&gt;&gt; from roboflow import Roboflow\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; roboflow.login()\n\n        &gt;&gt;&gt; rf = Roboflow()\n\n        &gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n        &gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"yolov5\")\n\n        &gt;&gt;&gt; ds = sv.DetectionDataset.from_yolo(\n        ...     images_directory_path=f\"{dataset.location}/train/images\",\n        ...     annotations_directory_path=f\"{dataset.location}/train/labels\",\n        ...     data_yaml_path=f\"{dataset.location}/data.yaml\"\n        ... )\n\n        &gt;&gt;&gt; ds.classes\n        ['dog', 'person']\n        ```\n    \"\"\"\n    classes, images, annotations = load_yolo_annotations(\n        images_directory_path=images_directory_path,\n        annotations_directory_path=annotations_directory_path,\n        data_yaml_path=data_yaml_path,\n        force_masks=force_masks,\n    )\n    return DetectionDataset(classes=classes, images=images, annotations=annotations)\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.DetectionDataset.merge","title":"<code>merge(dataset_list)</code>  <code>classmethod</code>","text":"<p>Merge a list of <code>DetectionDataset</code> objects into a single     <code>DetectionDataset</code> object.</p> <p>This method takes a list of <code>DetectionDataset</code> objects and combines their respective fields (<code>classes</code>, <code>images</code>, <code>annotations</code>) into a single <code>DetectionDataset</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_list</code> <code>List[DetectionDataset]</code> <p>A list of <code>DetectionDataset</code> objects to merge.</p> required <p>Returns:</p> Type Description <code>DetectionDataset</code> <p>A single <code>DetectionDataset</code> object containing</p> <code>DetectionDataset</code> <p>the merged data from the input list.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; ds_1 = sv.DetectionDataset(...)\n&gt;&gt;&gt; len(ds_1)\n100\n&gt;&gt;&gt; ds_1.classes\n['dog', 'person']\n\n&gt;&gt;&gt; ds_2 = sv.DetectionDataset(...)\n&gt;&gt;&gt; len(ds_2)\n200\n&gt;&gt;&gt; ds_2.classes\n['cat']\n\n&gt;&gt;&gt; ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\n&gt;&gt;&gt; len(ds_merged)\n300\n&gt;&gt;&gt; ds_merged.classes\n['cat', 'dog', 'person']\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>@classmethod\ndef merge(cls, dataset_list: List[DetectionDataset]) -&gt; DetectionDataset:\n    \"\"\"\n    Merge a list of `DetectionDataset` objects into a single\n        `DetectionDataset` object.\n\n    This method takes a list of `DetectionDataset` objects and combines\n    their respective fields (`classes`, `images`,\n    `annotations`) into a single `DetectionDataset` object.\n\n    Args:\n        dataset_list (List[DetectionDataset]): A list of `DetectionDataset`\n            objects to merge.\n\n    Returns:\n        (DetectionDataset): A single `DetectionDataset` object containing\n        the merged data from the input list.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; ds_1 = sv.DetectionDataset(...)\n        &gt;&gt;&gt; len(ds_1)\n        100\n        &gt;&gt;&gt; ds_1.classes\n        ['dog', 'person']\n\n        &gt;&gt;&gt; ds_2 = sv.DetectionDataset(...)\n        &gt;&gt;&gt; len(ds_2)\n        200\n        &gt;&gt;&gt; ds_2.classes\n        ['cat']\n\n        &gt;&gt;&gt; ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\n        &gt;&gt;&gt; len(ds_merged)\n        300\n        &gt;&gt;&gt; ds_merged.classes\n        ['cat', 'dog', 'person']\n        ```\n    \"\"\"\n    merged_images, merged_annotations = {}, {}\n    class_lists = [dataset.classes for dataset in dataset_list]\n    merged_classes = merge_class_lists(class_lists=class_lists)\n\n    for dataset in dataset_list:\n        class_index_mapping = build_class_index_mapping(\n            source_classes=dataset.classes, target_classes=merged_classes\n        )\n        for image_name, image, detections in dataset:\n            if image_name in merged_annotations:\n                raise ValueError(\n                    f\"Image name {image_name} is not unique across datasets.\"\n                )\n\n            merged_images[image_name] = image\n            merged_annotations[image_name] = map_detections_class_id(\n                source_to_target_mapping=class_index_mapping,\n                detections=detections,\n            )\n\n    return cls(\n        classes=merged_classes, images=merged_images, annotations=merged_annotations\n    )\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.DetectionDataset.split","title":"<code>split(split_ratio=0.8, random_state=None, shuffle=True)</code>","text":"<p>Splits the dataset into two parts (training and testing)     using the provided split_ratio.</p> <p>Parameters:</p> Name Type Description Default <code>split_ratio</code> <code>float</code> <p>The ratio of the training set to the entire dataset.</p> <code>0.8</code> <code>random_state</code> <code>int</code> <p>The seed for the random number generator. This is used for reproducibility.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[DetectionDataset, DetectionDataset]</code> <p>Tuple[DetectionDataset, DetectionDataset]: A tuple containing the training and testing datasets.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; ds = sv.DetectionDataset(...)\n&gt;&gt;&gt; train_ds, test_ds = ds.split(split_ratio=0.7,\n...                              random_state=42, shuffle=True)\n&gt;&gt;&gt; len(train_ds), len(test_ds)\n(700, 300)\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>def split(\n    self, split_ratio=0.8, random_state=None, shuffle: bool = True\n) -&gt; Tuple[DetectionDataset, DetectionDataset]:\n    \"\"\"\n    Splits the dataset into two parts (training and testing)\n        using the provided split_ratio.\n\n    Args:\n        split_ratio (float, optional): The ratio of the training\n            set to the entire dataset.\n        random_state (int, optional): The seed for the random number generator.\n            This is used for reproducibility.\n        shuffle (bool, optional): Whether to shuffle the data before splitting.\n\n    Returns:\n        Tuple[DetectionDataset, DetectionDataset]: A tuple containing\n            the training and testing datasets.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; ds = sv.DetectionDataset(...)\n        &gt;&gt;&gt; train_ds, test_ds = ds.split(split_ratio=0.7,\n        ...                              random_state=42, shuffle=True)\n        &gt;&gt;&gt; len(train_ds), len(test_ds)\n        (700, 300)\n        ```\n    \"\"\"\n\n    image_names = list(self.images.keys())\n    train_names, test_names = train_test_split(\n        data=image_names,\n        train_ratio=split_ratio,\n        random_state=random_state,\n        shuffle=shuffle,\n    )\n\n    train_dataset = DetectionDataset(\n        classes=self.classes,\n        images={name: self.images[name] for name in train_names},\n        annotations={name: self.annotations[name] for name in train_names},\n    )\n    test_dataset = DetectionDataset(\n        classes=self.classes,\n        images={name: self.images[name] for name in test_names},\n        annotations={name: self.annotations[name] for name in test_names},\n    )\n    return train_dataset, test_dataset\n</code></pre>"},{"location":"datasets/#classificationdataset","title":"ClassificationDataset","text":"<p>             Bases: <code>BaseDataset</code></p> <p>Dataclass containing information about a classification dataset.</p> <p>Attributes:</p> Name Type Description <code>classes</code> <code>List[str]</code> <p>List containing dataset class names.</p> <code>images</code> <code>Dict[str, ndarray]</code> <p>Dictionary mapping image name to image.</p> <code>annotations</code> <code>Dict[str, Detections]</code> <p>Dictionary mapping image name to annotations.</p> Source code in <code>supervision/dataset/core.py</code> <pre><code>@dataclass\nclass ClassificationDataset(BaseDataset):\n    \"\"\"\n    Dataclass containing information about a classification dataset.\n\n    Attributes:\n        classes (List[str]): List containing dataset class names.\n        images (Dict[str, np.ndarray]): Dictionary mapping image name to image.\n        annotations (Dict[str, Detections]): Dictionary mapping\n            image name to annotations.\n    \"\"\"\n\n    classes: List[str]\n    images: Dict[str, np.ndarray]\n    annotations: Dict[str, Classifications]\n\n    def __len__(self) -&gt; int:\n        return len(self.images)\n\n    def split(\n        self, split_ratio=0.8, random_state=None, shuffle: bool = True\n    ) -&gt; Tuple[ClassificationDataset, ClassificationDataset]:\n        \"\"\"\n        Splits the dataset into two parts (training and testing)\n            using the provided split_ratio.\n\n        Args:\n            split_ratio (float, optional): The ratio of the training\n                set to the entire dataset.\n            random_state (int, optional): The seed for the\n                random number generator. This is used for reproducibility.\n            shuffle (bool, optional): Whether to shuffle the data before splitting.\n\n        Returns:\n            Tuple[ClassificationDataset, ClassificationDataset]: A tuple containing\n            the training and testing datasets.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; cd = sv.ClassificationDataset(...)\n            &gt;&gt;&gt; train_cd,test_cd = cd.split(split_ratio=0.7,\n            ...                             random_state=42,shuffle=True)\n            &gt;&gt;&gt; len(train_cd), len(test_cd)\n            (700, 300)\n            ```\n        \"\"\"\n        image_names = list(self.images.keys())\n        train_names, test_names = train_test_split(\n            data=image_names,\n            train_ratio=split_ratio,\n            random_state=random_state,\n            shuffle=shuffle,\n        )\n\n        train_dataset = ClassificationDataset(\n            classes=self.classes,\n            images={name: self.images[name] for name in train_names},\n            annotations={name: self.annotations[name] for name in train_names},\n        )\n        test_dataset = ClassificationDataset(\n            classes=self.classes,\n            images={name: self.images[name] for name in test_names},\n            annotations={name: self.annotations[name] for name in test_names},\n        )\n        return train_dataset, test_dataset\n\n    def as_folder_structure(self, root_directory_path: str) -&gt; None:\n        \"\"\"\n        Saves the dataset as a multi-class folder structure.\n\n        Args:\n            root_directory_path (str): The path to the directory\n                where the dataset will be saved.\n        \"\"\"\n        os.makedirs(root_directory_path, exist_ok=True)\n\n        for class_name in self.classes:\n            os.makedirs(os.path.join(root_directory_path, class_name), exist_ok=True)\n\n        for image_path in self.images:\n            classification = self.annotations[image_path]\n            image = self.images[image_path]\n            image_name = Path(image_path).name\n            class_id = (\n                classification.class_id[0]\n                if classification.confidence is None\n                else classification.get_top_k(1)[0][0]\n            )\n            class_name = self.classes[class_id]\n            image_path = os.path.join(root_directory_path, class_name, image_name)\n            cv2.imwrite(image_path, image)\n\n    @classmethod\n    def from_folder_structure(cls, root_directory_path: str) -&gt; ClassificationDataset:\n        \"\"\"\n        Load data from a multiclass folder structure into a ClassificationDataset.\n\n        Args:\n            root_directory_path (str): The path to the dataset directory.\n\n        Returns:\n            ClassificationDataset: The dataset.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import roboflow\n            &gt;&gt;&gt; from roboflow import Roboflow\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; roboflow.login()\n\n            &gt;&gt;&gt; rf = Roboflow()\n\n            &gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n            &gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"folder\")\n\n            &gt;&gt;&gt; cd = sv.ClassificationDataset.from_folder_structure(\n            ...     root_directory_path=f\"{dataset.location}/train\"\n            ... )\n            ```\n        \"\"\"\n        classes = os.listdir(root_directory_path)\n        classes = sorted(set(classes))\n\n        images = {}\n        annotations = {}\n\n        for class_name in classes:\n            class_id = classes.index(class_name)\n\n            for image in os.listdir(os.path.join(root_directory_path, class_name)):\n                image_path = str(os.path.join(root_directory_path, class_name, image))\n                images[image_path] = cv2.imread(image_path)\n                annotations[image_path] = Classifications(\n                    class_id=np.array([class_id]),\n                )\n\n        return cls(\n            classes=classes,\n            images=images,\n            annotations=annotations,\n        )\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.ClassificationDataset.as_folder_structure","title":"<code>as_folder_structure(root_directory_path)</code>","text":"<p>Saves the dataset as a multi-class folder structure.</p> <p>Parameters:</p> Name Type Description Default <code>root_directory_path</code> <code>str</code> <p>The path to the directory where the dataset will be saved.</p> required Source code in <code>supervision/dataset/core.py</code> <pre><code>def as_folder_structure(self, root_directory_path: str) -&gt; None:\n    \"\"\"\n    Saves the dataset as a multi-class folder structure.\n\n    Args:\n        root_directory_path (str): The path to the directory\n            where the dataset will be saved.\n    \"\"\"\n    os.makedirs(root_directory_path, exist_ok=True)\n\n    for class_name in self.classes:\n        os.makedirs(os.path.join(root_directory_path, class_name), exist_ok=True)\n\n    for image_path in self.images:\n        classification = self.annotations[image_path]\n        image = self.images[image_path]\n        image_name = Path(image_path).name\n        class_id = (\n            classification.class_id[0]\n            if classification.confidence is None\n            else classification.get_top_k(1)[0][0]\n        )\n        class_name = self.classes[class_id]\n        image_path = os.path.join(root_directory_path, class_name, image_name)\n        cv2.imwrite(image_path, image)\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.ClassificationDataset.from_folder_structure","title":"<code>from_folder_structure(root_directory_path)</code>  <code>classmethod</code>","text":"<p>Load data from a multiclass folder structure into a ClassificationDataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_directory_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <p>Returns:</p> Name Type Description <code>ClassificationDataset</code> <code>ClassificationDataset</code> <p>The dataset.</p> Example <pre><code>&gt;&gt;&gt; import roboflow\n&gt;&gt;&gt; from roboflow import Roboflow\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; roboflow.login()\n\n&gt;&gt;&gt; rf = Roboflow()\n\n&gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n&gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"folder\")\n\n&gt;&gt;&gt; cd = sv.ClassificationDataset.from_folder_structure(\n...     root_directory_path=f\"{dataset.location}/train\"\n... )\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>@classmethod\ndef from_folder_structure(cls, root_directory_path: str) -&gt; ClassificationDataset:\n    \"\"\"\n    Load data from a multiclass folder structure into a ClassificationDataset.\n\n    Args:\n        root_directory_path (str): The path to the dataset directory.\n\n    Returns:\n        ClassificationDataset: The dataset.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import roboflow\n        &gt;&gt;&gt; from roboflow import Roboflow\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; roboflow.login()\n\n        &gt;&gt;&gt; rf = Roboflow()\n\n        &gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n        &gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"folder\")\n\n        &gt;&gt;&gt; cd = sv.ClassificationDataset.from_folder_structure(\n        ...     root_directory_path=f\"{dataset.location}/train\"\n        ... )\n        ```\n    \"\"\"\n    classes = os.listdir(root_directory_path)\n    classes = sorted(set(classes))\n\n    images = {}\n    annotations = {}\n\n    for class_name in classes:\n        class_id = classes.index(class_name)\n\n        for image in os.listdir(os.path.join(root_directory_path, class_name)):\n            image_path = str(os.path.join(root_directory_path, class_name, image))\n            images[image_path] = cv2.imread(image_path)\n            annotations[image_path] = Classifications(\n                class_id=np.array([class_id]),\n            )\n\n    return cls(\n        classes=classes,\n        images=images,\n        annotations=annotations,\n    )\n</code></pre>"},{"location":"datasets/#supervision.dataset.core.ClassificationDataset.split","title":"<code>split(split_ratio=0.8, random_state=None, shuffle=True)</code>","text":"<p>Splits the dataset into two parts (training and testing)     using the provided split_ratio.</p> <p>Parameters:</p> Name Type Description Default <code>split_ratio</code> <code>float</code> <p>The ratio of the training set to the entire dataset.</p> <code>0.8</code> <code>random_state</code> <code>int</code> <p>The seed for the random number generator. This is used for reproducibility.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting.</p> <code>True</code> <p>Returns:</p> Type Description <code>ClassificationDataset</code> <p>Tuple[ClassificationDataset, ClassificationDataset]: A tuple containing</p> <code>ClassificationDataset</code> <p>the training and testing datasets.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; cd = sv.ClassificationDataset(...)\n&gt;&gt;&gt; train_cd,test_cd = cd.split(split_ratio=0.7,\n...                             random_state=42,shuffle=True)\n&gt;&gt;&gt; len(train_cd), len(test_cd)\n(700, 300)\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>def split(\n    self, split_ratio=0.8, random_state=None, shuffle: bool = True\n) -&gt; Tuple[ClassificationDataset, ClassificationDataset]:\n    \"\"\"\n    Splits the dataset into two parts (training and testing)\n        using the provided split_ratio.\n\n    Args:\n        split_ratio (float, optional): The ratio of the training\n            set to the entire dataset.\n        random_state (int, optional): The seed for the\n            random number generator. This is used for reproducibility.\n        shuffle (bool, optional): Whether to shuffle the data before splitting.\n\n    Returns:\n        Tuple[ClassificationDataset, ClassificationDataset]: A tuple containing\n        the training and testing datasets.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; cd = sv.ClassificationDataset(...)\n        &gt;&gt;&gt; train_cd,test_cd = cd.split(split_ratio=0.7,\n        ...                             random_state=42,shuffle=True)\n        &gt;&gt;&gt; len(train_cd), len(test_cd)\n        (700, 300)\n        ```\n    \"\"\"\n    image_names = list(self.images.keys())\n    train_names, test_names = train_test_split(\n        data=image_names,\n        train_ratio=split_ratio,\n        random_state=random_state,\n        shuffle=shuffle,\n    )\n\n    train_dataset = ClassificationDataset(\n        classes=self.classes,\n        images={name: self.images[name] for name in train_names},\n        annotations={name: self.annotations[name] for name in train_names},\n    )\n    test_dataset = ClassificationDataset(\n        classes=self.classes,\n        images={name: self.images[name] for name in test_names},\n        annotations={name: self.annotations[name] for name in test_names},\n    )\n    return train_dataset, test_dataset\n</code></pre>"},{"location":"trackers/","title":"Trackers","text":""},{"location":"trackers/#bytetrack","title":"ByteTrack","text":"<p>Initialize the ByteTrack object.</p> <p>Parameters:</p> Name Type Description Default <code>track_thresh</code> <code>float</code> <p>Detection confidence threshold for track activation.</p> <code>0.25</code> <code>track_buffer</code> <code>int</code> <p>Number of frames to buffer when a track is lost.</p> <code>30</code> <code>match_thresh</code> <code>float</code> <p>Threshold for matching tracks with detections.</p> <code>0.8</code> <code>frame_rate</code> <code>int</code> <p>The frame rate of the video.</p> <code>30</code> Source code in <code>supervision/tracker/byte_tracker/core.py</code> <pre><code>class ByteTrack:\n    \"\"\"\n    Initialize the ByteTrack object.\n\n    Parameters:\n        track_thresh (float, optional): Detection confidence threshold\n            for track activation.\n        track_buffer (int, optional): Number of frames to buffer when a track is lost.\n        match_thresh (float, optional): Threshold for matching tracks with detections.\n        frame_rate (int, optional): The frame rate of the video.\n    \"\"\"\n\n    def __init__(\n        self,\n        track_thresh: float = 0.25,\n        track_buffer: int = 30,\n        match_thresh: float = 0.8,\n        frame_rate: int = 30,\n    ):\n        self.track_thresh = track_thresh\n        self.match_thresh = match_thresh\n\n        self.frame_id = 0\n        self.det_thresh = self.track_thresh + 0.1\n        self.max_time_lost = int(frame_rate / 30.0 * track_buffer)\n        self.kalman_filter = KalmanFilter()\n\n        self.tracked_tracks: List[STrack] = []\n        self.lost_tracks: List[STrack] = []\n        self.removed_tracks: List[STrack] = []\n\n    def update_with_detections(self, detections: Detections) -&gt; Detections:\n        \"\"\"\n        Updates the tracker with the provided detections and\n            returns the updated detection results.\n\n        Parameters:\n            detections: The new detections to update with.\n        Returns:\n            Detection: The updated detection results that now include tracking IDs.\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n            &gt;&gt;&gt; from ultralytics import YOLO\n\n            &gt;&gt;&gt; model = YOLO(...)\n            &gt;&gt;&gt; byte_tracker = sv.ByteTrack()\n            &gt;&gt;&gt; annotator = sv.BoxAnnotator()\n\n            &gt;&gt;&gt; def callback(frame: np.ndarray, index: int) -&gt; np.ndarray:\n            ...     results = model(frame)[0]\n            ...     detections = sv.Detections.from_ultralytics(results)\n            ...     detections = byte_tracker.update_with_detections(detections)\n            ...     labels = [\n            ...         f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\n            ...         for _, _, confidence, class_id, tracker_id\n            ...         in detections\n            ...     ]\n            ...     return annotator.annotate(scene=frame.copy(),\n            ...                               detections=detections, labels=labels)\n\n            &gt;&gt;&gt; sv.process_video(\n            ...     source_path='...',\n            ...     target_path='...',\n            ...     callback=callback\n            ... )\n            ```\n        \"\"\"\n\n        tracks = self.update_with_tensors(\n            tensors=detections2boxes(detections=detections)\n        )\n        detections = Detections.empty()\n        if len(tracks) &gt; 0:\n            detections.xyxy = np.array(\n                [track.tlbr for track in tracks], dtype=np.float32\n            )\n            detections.class_id = np.array(\n                [int(t.class_ids) for t in tracks], dtype=int\n            )\n            detections.tracker_id = np.array(\n                [int(t.track_id) for t in tracks], dtype=int\n            )\n            detections.confidence = np.array(\n                [t.score for t in tracks], dtype=np.float32\n            )\n        else:\n            detections.tracker_id = np.array([], dtype=int)\n\n        return detections\n\n    def update_with_tensors(self, tensors: np.ndarray) -&gt; List[STrack]:\n        \"\"\"\n        Updates the tracker with the provided tensors and returns the updated tracks.\n\n        Parameters:\n            tensors: The new tensors to update with.\n\n        Returns:\n            List[STrack]: Updated tracks.\n        \"\"\"\n        self.frame_id += 1\n        activated_starcks = []\n        refind_stracks = []\n        lost_stracks = []\n        removed_stracks = []\n\n        class_ids = tensors[:, 5]\n        scores = tensors[:, 4]\n        bboxes = tensors[:, :4]\n\n        remain_inds = scores &gt; self.track_thresh\n        inds_low = scores &gt; 0.1\n        inds_high = scores &lt; self.track_thresh\n\n        inds_second = np.logical_and(inds_low, inds_high)\n        dets_second = bboxes[inds_second]\n        dets = bboxes[remain_inds]\n        scores_keep = scores[remain_inds]\n        scores_second = scores[inds_second]\n\n        class_ids_keep = class_ids[remain_inds]\n        class_ids_second = class_ids[inds_second]\n\n        if len(dets) &gt; 0:\n            \"\"\"Detections\"\"\"\n            detections = [\n                STrack(STrack.tlbr_to_tlwh(tlbr), s, c)\n                for (tlbr, s, c) in zip(dets, scores_keep, class_ids_keep)\n            ]\n        else:\n            detections = []\n\n        \"\"\" Add newly detected tracklets to tracked_stracks\"\"\"\n        unconfirmed = []\n        tracked_stracks = []  # type: list[STrack]\n        for track in self.tracked_tracks:\n            if not track.is_activated:\n                unconfirmed.append(track)\n            else:\n                tracked_stracks.append(track)\n\n        \"\"\" Step 2: First association, with high score detection boxes\"\"\"\n        strack_pool = joint_tracks(tracked_stracks, self.lost_tracks)\n        # Predict the current location with KF\n        STrack.multi_predict(strack_pool)\n        dists = matching.iou_distance(strack_pool, detections)\n\n        dists = matching.fuse_score(dists, detections)\n        matches, u_track, u_detection = matching.linear_assignment(\n            dists, thresh=self.match_thresh\n        )\n\n        for itracked, idet in matches:\n            track = strack_pool[itracked]\n            det = detections[idet]\n            if track.state == TrackState.Tracked:\n                track.update(detections[idet], self.frame_id)\n                activated_starcks.append(track)\n            else:\n                track.re_activate(det, self.frame_id, new_id=False)\n                refind_stracks.append(track)\n\n        \"\"\" Step 3: Second association, with low score detection boxes\"\"\"\n        # association the untrack to the low score detections\n        if len(dets_second) &gt; 0:\n            \"\"\"Detections\"\"\"\n            detections_second = [\n                STrack(STrack.tlbr_to_tlwh(tlbr), s, c)\n                for (tlbr, s, c) in zip(dets_second, scores_second, class_ids_second)\n            ]\n        else:\n            detections_second = []\n        r_tracked_stracks = [\n            strack_pool[i]\n            for i in u_track\n            if strack_pool[i].state == TrackState.Tracked\n        ]\n        dists = matching.iou_distance(r_tracked_stracks, detections_second)\n        matches, u_track, u_detection_second = matching.linear_assignment(\n            dists, thresh=0.5\n        )\n        for itracked, idet in matches:\n            track = r_tracked_stracks[itracked]\n            det = detections_second[idet]\n            if track.state == TrackState.Tracked:\n                track.update(det, self.frame_id)\n                activated_starcks.append(track)\n            else:\n                track.re_activate(det, self.frame_id, new_id=False)\n                refind_stracks.append(track)\n\n        for it in u_track:\n            track = r_tracked_stracks[it]\n            if not track.state == TrackState.Lost:\n                track.mark_lost()\n                lost_stracks.append(track)\n\n        \"\"\"Deal with unconfirmed tracks, usually tracks with only one beginning frame\"\"\"\n        detections = [detections[i] for i in u_detection]\n        dists = matching.iou_distance(unconfirmed, detections)\n\n        dists = matching.fuse_score(dists, detections)\n        matches, u_unconfirmed, u_detection = matching.linear_assignment(\n            dists, thresh=0.7\n        )\n        for itracked, idet in matches:\n            unconfirmed[itracked].update(detections[idet], self.frame_id)\n            activated_starcks.append(unconfirmed[itracked])\n        for it in u_unconfirmed:\n            track = unconfirmed[it]\n            track.mark_removed()\n            removed_stracks.append(track)\n\n        \"\"\" Step 4: Init new stracks\"\"\"\n        for inew in u_detection:\n            track = detections[inew]\n            if track.score &lt; self.det_thresh:\n                continue\n            track.activate(self.kalman_filter, self.frame_id)\n            activated_starcks.append(track)\n        \"\"\" Step 5: Update state\"\"\"\n        for track in self.lost_tracks:\n            if self.frame_id - track.end_frame &gt; self.max_time_lost:\n                track.mark_removed()\n                removed_stracks.append(track)\n\n        self.tracked_tracks = [\n            t for t in self.tracked_tracks if t.state == TrackState.Tracked\n        ]\n        self.tracked_tracks = joint_tracks(self.tracked_tracks, activated_starcks)\n        self.tracked_tracks = joint_tracks(self.tracked_tracks, refind_stracks)\n        self.lost_tracks = sub_tracks(self.lost_tracks, self.tracked_tracks)\n        self.lost_tracks.extend(lost_stracks)\n        self.lost_tracks = sub_tracks(self.lost_tracks, self.removed_tracks)\n        self.removed_tracks.extend(removed_stracks)\n        self.tracked_tracks, self.lost_tracks = remove_duplicate_tracks(\n            self.tracked_tracks, self.lost_tracks\n        )\n        output_stracks = [track for track in self.tracked_tracks if track.is_activated]\n\n        return output_stracks\n</code></pre>"},{"location":"trackers/#supervision.tracker.byte_tracker.core.ByteTrack.update_with_detections","title":"<code>update_with_detections(detections)</code>","text":"<p>Updates the tracker with the provided detections and     returns the updated detection results.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The new detections to update with.</p> required <p>Returns:     Detection: The updated detection results that now include tracking IDs. Example:     <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; model = YOLO(...)\n&gt;&gt;&gt; byte_tracker = sv.ByteTrack()\n&gt;&gt;&gt; annotator = sv.BoxAnnotator()\n\n&gt;&gt;&gt; def callback(frame: np.ndarray, index: int) -&gt; np.ndarray:\n...     results = model(frame)[0]\n...     detections = sv.Detections.from_ultralytics(results)\n...     detections = byte_tracker.update_with_detections(detections)\n...     labels = [\n...         f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\n...         for _, _, confidence, class_id, tracker_id\n...         in detections\n...     ]\n...     return annotator.annotate(scene=frame.copy(),\n...                               detections=detections, labels=labels)\n\n&gt;&gt;&gt; sv.process_video(\n...     source_path='...',\n...     target_path='...',\n...     callback=callback\n... )\n</code></pre></p> Source code in <code>supervision/tracker/byte_tracker/core.py</code> <pre><code>def update_with_detections(self, detections: Detections) -&gt; Detections:\n    \"\"\"\n    Updates the tracker with the provided detections and\n        returns the updated detection results.\n\n    Parameters:\n        detections: The new detections to update with.\n    Returns:\n        Detection: The updated detection results that now include tracking IDs.\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n        &gt;&gt;&gt; from ultralytics import YOLO\n\n        &gt;&gt;&gt; model = YOLO(...)\n        &gt;&gt;&gt; byte_tracker = sv.ByteTrack()\n        &gt;&gt;&gt; annotator = sv.BoxAnnotator()\n\n        &gt;&gt;&gt; def callback(frame: np.ndarray, index: int) -&gt; np.ndarray:\n        ...     results = model(frame)[0]\n        ...     detections = sv.Detections.from_ultralytics(results)\n        ...     detections = byte_tracker.update_with_detections(detections)\n        ...     labels = [\n        ...         f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\n        ...         for _, _, confidence, class_id, tracker_id\n        ...         in detections\n        ...     ]\n        ...     return annotator.annotate(scene=frame.copy(),\n        ...                               detections=detections, labels=labels)\n\n        &gt;&gt;&gt; sv.process_video(\n        ...     source_path='...',\n        ...     target_path='...',\n        ...     callback=callback\n        ... )\n        ```\n    \"\"\"\n\n    tracks = self.update_with_tensors(\n        tensors=detections2boxes(detections=detections)\n    )\n    detections = Detections.empty()\n    if len(tracks) &gt; 0:\n        detections.xyxy = np.array(\n            [track.tlbr for track in tracks], dtype=np.float32\n        )\n        detections.class_id = np.array(\n            [int(t.class_ids) for t in tracks], dtype=int\n        )\n        detections.tracker_id = np.array(\n            [int(t.track_id) for t in tracks], dtype=int\n        )\n        detections.confidence = np.array(\n            [t.score for t in tracks], dtype=np.float32\n        )\n    else:\n        detections.tracker_id = np.array([], dtype=int)\n\n    return detections\n</code></pre>"},{"location":"trackers/#supervision.tracker.byte_tracker.core.ByteTrack.update_with_tensors","title":"<code>update_with_tensors(tensors)</code>","text":"<p>Updates the tracker with the provided tensors and returns the updated tracks.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>ndarray</code> <p>The new tensors to update with.</p> required <p>Returns:</p> Type Description <code>List[STrack]</code> <p>List[STrack]: Updated tracks.</p> Source code in <code>supervision/tracker/byte_tracker/core.py</code> <pre><code>def update_with_tensors(self, tensors: np.ndarray) -&gt; List[STrack]:\n    \"\"\"\n    Updates the tracker with the provided tensors and returns the updated tracks.\n\n    Parameters:\n        tensors: The new tensors to update with.\n\n    Returns:\n        List[STrack]: Updated tracks.\n    \"\"\"\n    self.frame_id += 1\n    activated_starcks = []\n    refind_stracks = []\n    lost_stracks = []\n    removed_stracks = []\n\n    class_ids = tensors[:, 5]\n    scores = tensors[:, 4]\n    bboxes = tensors[:, :4]\n\n    remain_inds = scores &gt; self.track_thresh\n    inds_low = scores &gt; 0.1\n    inds_high = scores &lt; self.track_thresh\n\n    inds_second = np.logical_and(inds_low, inds_high)\n    dets_second = bboxes[inds_second]\n    dets = bboxes[remain_inds]\n    scores_keep = scores[remain_inds]\n    scores_second = scores[inds_second]\n\n    class_ids_keep = class_ids[remain_inds]\n    class_ids_second = class_ids[inds_second]\n\n    if len(dets) &gt; 0:\n        \"\"\"Detections\"\"\"\n        detections = [\n            STrack(STrack.tlbr_to_tlwh(tlbr), s, c)\n            for (tlbr, s, c) in zip(dets, scores_keep, class_ids_keep)\n        ]\n    else:\n        detections = []\n\n    \"\"\" Add newly detected tracklets to tracked_stracks\"\"\"\n    unconfirmed = []\n    tracked_stracks = []  # type: list[STrack]\n    for track in self.tracked_tracks:\n        if not track.is_activated:\n            unconfirmed.append(track)\n        else:\n            tracked_stracks.append(track)\n\n    \"\"\" Step 2: First association, with high score detection boxes\"\"\"\n    strack_pool = joint_tracks(tracked_stracks, self.lost_tracks)\n    # Predict the current location with KF\n    STrack.multi_predict(strack_pool)\n    dists = matching.iou_distance(strack_pool, detections)\n\n    dists = matching.fuse_score(dists, detections)\n    matches, u_track, u_detection = matching.linear_assignment(\n        dists, thresh=self.match_thresh\n    )\n\n    for itracked, idet in matches:\n        track = strack_pool[itracked]\n        det = detections[idet]\n        if track.state == TrackState.Tracked:\n            track.update(detections[idet], self.frame_id)\n            activated_starcks.append(track)\n        else:\n            track.re_activate(det, self.frame_id, new_id=False)\n            refind_stracks.append(track)\n\n    \"\"\" Step 3: Second association, with low score detection boxes\"\"\"\n    # association the untrack to the low score detections\n    if len(dets_second) &gt; 0:\n        \"\"\"Detections\"\"\"\n        detections_second = [\n            STrack(STrack.tlbr_to_tlwh(tlbr), s, c)\n            for (tlbr, s, c) in zip(dets_second, scores_second, class_ids_second)\n        ]\n    else:\n        detections_second = []\n    r_tracked_stracks = [\n        strack_pool[i]\n        for i in u_track\n        if strack_pool[i].state == TrackState.Tracked\n    ]\n    dists = matching.iou_distance(r_tracked_stracks, detections_second)\n    matches, u_track, u_detection_second = matching.linear_assignment(\n        dists, thresh=0.5\n    )\n    for itracked, idet in matches:\n        track = r_tracked_stracks[itracked]\n        det = detections_second[idet]\n        if track.state == TrackState.Tracked:\n            track.update(det, self.frame_id)\n            activated_starcks.append(track)\n        else:\n            track.re_activate(det, self.frame_id, new_id=False)\n            refind_stracks.append(track)\n\n    for it in u_track:\n        track = r_tracked_stracks[it]\n        if not track.state == TrackState.Lost:\n            track.mark_lost()\n            lost_stracks.append(track)\n\n    \"\"\"Deal with unconfirmed tracks, usually tracks with only one beginning frame\"\"\"\n    detections = [detections[i] for i in u_detection]\n    dists = matching.iou_distance(unconfirmed, detections)\n\n    dists = matching.fuse_score(dists, detections)\n    matches, u_unconfirmed, u_detection = matching.linear_assignment(\n        dists, thresh=0.7\n    )\n    for itracked, idet in matches:\n        unconfirmed[itracked].update(detections[idet], self.frame_id)\n        activated_starcks.append(unconfirmed[itracked])\n    for it in u_unconfirmed:\n        track = unconfirmed[it]\n        track.mark_removed()\n        removed_stracks.append(track)\n\n    \"\"\" Step 4: Init new stracks\"\"\"\n    for inew in u_detection:\n        track = detections[inew]\n        if track.score &lt; self.det_thresh:\n            continue\n        track.activate(self.kalman_filter, self.frame_id)\n        activated_starcks.append(track)\n    \"\"\" Step 5: Update state\"\"\"\n    for track in self.lost_tracks:\n        if self.frame_id - track.end_frame &gt; self.max_time_lost:\n            track.mark_removed()\n            removed_stracks.append(track)\n\n    self.tracked_tracks = [\n        t for t in self.tracked_tracks if t.state == TrackState.Tracked\n    ]\n    self.tracked_tracks = joint_tracks(self.tracked_tracks, activated_starcks)\n    self.tracked_tracks = joint_tracks(self.tracked_tracks, refind_stracks)\n    self.lost_tracks = sub_tracks(self.lost_tracks, self.tracked_tracks)\n    self.lost_tracks.extend(lost_stracks)\n    self.lost_tracks = sub_tracks(self.lost_tracks, self.removed_tracks)\n    self.removed_tracks.extend(removed_stracks)\n    self.tracked_tracks, self.lost_tracks = remove_duplicate_tracks(\n        self.tracked_tracks, self.lost_tracks\n    )\n    output_stracks = [track for track in self.tracked_tracks if track.is_activated]\n\n    return output_stracks\n</code></pre>"},{"location":"classification/core/","title":"Core","text":""},{"location":"classification/core/#classifications","title":"Classifications","text":"Source code in <code>supervision/classification/core.py</code> <pre><code>@dataclass\nclass Classifications:\n    class_id: np.ndarray\n    confidence: Optional[np.ndarray] = None\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"\n        Validate the classification inputs.\n        \"\"\"\n        n = len(self.class_id)\n\n        _validate_class_ids(self.class_id, n)\n        _validate_confidence(self.confidence, n)\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the number of classifications.\n        \"\"\"\n        return len(self.class_id)\n\n    @classmethod\n    def from_clip(cls, clip_results) -&gt; Classifications:\n        \"\"\"\n        Creates a Classifications instance from a\n        [clip](https://github.com/openai/clip) inference result.\n\n        Args:\n            clip_results (np.ndarray): The inference result from clip model.\n\n        Returns:\n            Classifications: A new Classifications object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; from PIL import Image\n            &gt;&gt;&gt; import clip\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; model, preprocess = clip.load('ViT-B/32')\n\n            &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n            &gt;&gt;&gt; image = preprocess(image).unsqueeze(0)\n\n            &gt;&gt;&gt; text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"])\n            &gt;&gt;&gt; output, _ = model(image, text)\n            &gt;&gt;&gt; classifications = sv.Classifications.from_clip(output)\n            ```\n        \"\"\"\n\n        confidence = clip_results.softmax(dim=-1).cpu().detach().numpy()[0]\n\n        if len(confidence) == 0:\n            return cls(class_id=np.array([]), confidence=np.array([]))\n\n        class_ids = np.arange(len(confidence))\n        return cls(class_id=class_ids, confidence=confidence)\n\n    @classmethod\n    def from_ultralytics(cls, ultralytics_results) -&gt; Classifications:\n        \"\"\"\n        Creates a Classifications instance from a\n        [ultralytics](https://github.com/ultralytics/ultralytics) inference result.\n\n        Args:\n            ultralytics_results (ultralytics.engine.results.Results):\n                The inference result from ultralytics model.\n\n        Returns:\n            Classifications: A new Classifications object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import cv2\n            &gt;&gt;&gt; from ultralytics import YOLO\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n            &gt;&gt;&gt; model = YOLO('yolov8n-cls.pt')\n\n            &gt;&gt;&gt; output = model(image)[0]\n            &gt;&gt;&gt; classifications = sv.Classifications.from_ultralytics(output)\n            ```\n        \"\"\"\n        confidence = ultralytics_results.probs.data.cpu().numpy()\n        return cls(class_id=np.arange(confidence.shape[0]), confidence=confidence)\n\n    @classmethod\n    def from_timm(cls, timm_results) -&gt; Classifications:\n        \"\"\"\n        Creates a Classifications instance from a\n        [timm](https://huggingface.co/docs/hub/timm) inference result.\n\n        Args:\n            timm_results: The inference result from timm model.\n\n        Returns:\n            Classifications: A new Classifications object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; from PIL import Image\n            &gt;&gt;&gt; import timm\n            &gt;&gt;&gt; from timm.data import resolve_data_config, create_transform\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; model = timm.create_model(\n            ...     model_name='hf-hub:nateraw/resnet50-oxford-iiit-pet',\n            ...     pretrained=True\n            ... ).eval()\n\n            &gt;&gt;&gt; config = resolve_data_config({}, model=model)\n            &gt;&gt;&gt; transform = create_transform(**config)\n\n            &gt;&gt;&gt; image = Image.open(SOURCE_IMAGE_PATH).convert('RGB')\n            &gt;&gt;&gt; x = transform(image).unsqueeze(0)\n\n            &gt;&gt;&gt; output = model(x)\n\n            &gt;&gt;&gt; classifications = sv.Classifications.from_timm(output)\n            ```\n        \"\"\"\n        confidence = timm_results.cpu().detach().numpy()[0]\n\n        if len(confidence) == 0:\n            return cls(class_id=np.array([]), confidence=np.array([]))\n\n        class_id = np.arange(len(confidence))\n        return cls(class_id=class_id, confidence=confidence)\n\n    def get_top_k(self, k: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Retrieve the top k class IDs and confidences,\n            ordered in descending order by confidence.\n\n        Args:\n            k (int): The number of top class IDs and confidences to retrieve.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: A tuple containing\n                the top k class IDs and confidences.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; classifications = sv.Classifications(...)\n\n            &gt;&gt;&gt; classifications.get_top_k(1)\n\n            (array([1]), array([0.9]))\n            ```\n        \"\"\"\n        if self.confidence is None:\n            raise ValueError(\"top_k could not be calculated, confidence is None\")\n\n        order = np.argsort(self.confidence)[::-1]\n        top_k_order = order[:k]\n        top_k_class_id = self.class_id[top_k_order]\n        top_k_confidence = self.confidence[top_k_order]\n\n        return top_k_class_id, top_k_confidence\n</code></pre>"},{"location":"classification/core/#supervision.classification.core.Classifications.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of classifications.</p> Source code in <code>supervision/classification/core.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the number of classifications.\n    \"\"\"\n    return len(self.class_id)\n</code></pre>"},{"location":"classification/core/#supervision.classification.core.Classifications.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the classification inputs.</p> Source code in <code>supervision/classification/core.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"\n    Validate the classification inputs.\n    \"\"\"\n    n = len(self.class_id)\n\n    _validate_class_ids(self.class_id, n)\n    _validate_confidence(self.confidence, n)\n</code></pre>"},{"location":"classification/core/#supervision.classification.core.Classifications.from_clip","title":"<code>from_clip(clip_results)</code>  <code>classmethod</code>","text":"<p>Creates a Classifications instance from a clip inference result.</p> <p>Parameters:</p> Name Type Description Default <code>clip_results</code> <code>ndarray</code> <p>The inference result from clip model.</p> required <p>Returns:</p> Name Type Description <code>Classifications</code> <code>Classifications</code> <p>A new Classifications object.</p> Example <pre><code>&gt;&gt;&gt; from PIL import Image\n&gt;&gt;&gt; import clip\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; model, preprocess = clip.load('ViT-B/32')\n\n&gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n&gt;&gt;&gt; image = preprocess(image).unsqueeze(0)\n\n&gt;&gt;&gt; text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"])\n&gt;&gt;&gt; output, _ = model(image, text)\n&gt;&gt;&gt; classifications = sv.Classifications.from_clip(output)\n</code></pre> Source code in <code>supervision/classification/core.py</code> <pre><code>@classmethod\ndef from_clip(cls, clip_results) -&gt; Classifications:\n    \"\"\"\n    Creates a Classifications instance from a\n    [clip](https://github.com/openai/clip) inference result.\n\n    Args:\n        clip_results (np.ndarray): The inference result from clip model.\n\n    Returns:\n        Classifications: A new Classifications object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; from PIL import Image\n        &gt;&gt;&gt; import clip\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; model, preprocess = clip.load('ViT-B/32')\n\n        &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n        &gt;&gt;&gt; image = preprocess(image).unsqueeze(0)\n\n        &gt;&gt;&gt; text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"])\n        &gt;&gt;&gt; output, _ = model(image, text)\n        &gt;&gt;&gt; classifications = sv.Classifications.from_clip(output)\n        ```\n    \"\"\"\n\n    confidence = clip_results.softmax(dim=-1).cpu().detach().numpy()[0]\n\n    if len(confidence) == 0:\n        return cls(class_id=np.array([]), confidence=np.array([]))\n\n    class_ids = np.arange(len(confidence))\n    return cls(class_id=class_ids, confidence=confidence)\n</code></pre>"},{"location":"classification/core/#supervision.classification.core.Classifications.from_timm","title":"<code>from_timm(timm_results)</code>  <code>classmethod</code>","text":"<p>Creates a Classifications instance from a timm inference result.</p> <p>Parameters:</p> Name Type Description Default <code>timm_results</code> <p>The inference result from timm model.</p> required <p>Returns:</p> Name Type Description <code>Classifications</code> <code>Classifications</code> <p>A new Classifications object.</p> Example <pre><code>&gt;&gt;&gt; from PIL import Image\n&gt;&gt;&gt; import timm\n&gt;&gt;&gt; from timm.data import resolve_data_config, create_transform\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; model = timm.create_model(\n...     model_name='hf-hub:nateraw/resnet50-oxford-iiit-pet',\n...     pretrained=True\n... ).eval()\n\n&gt;&gt;&gt; config = resolve_data_config({}, model=model)\n&gt;&gt;&gt; transform = create_transform(**config)\n\n&gt;&gt;&gt; image = Image.open(SOURCE_IMAGE_PATH).convert('RGB')\n&gt;&gt;&gt; x = transform(image).unsqueeze(0)\n\n&gt;&gt;&gt; output = model(x)\n\n&gt;&gt;&gt; classifications = sv.Classifications.from_timm(output)\n</code></pre> Source code in <code>supervision/classification/core.py</code> <pre><code>@classmethod\ndef from_timm(cls, timm_results) -&gt; Classifications:\n    \"\"\"\n    Creates a Classifications instance from a\n    [timm](https://huggingface.co/docs/hub/timm) inference result.\n\n    Args:\n        timm_results: The inference result from timm model.\n\n    Returns:\n        Classifications: A new Classifications object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; from PIL import Image\n        &gt;&gt;&gt; import timm\n        &gt;&gt;&gt; from timm.data import resolve_data_config, create_transform\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; model = timm.create_model(\n        ...     model_name='hf-hub:nateraw/resnet50-oxford-iiit-pet',\n        ...     pretrained=True\n        ... ).eval()\n\n        &gt;&gt;&gt; config = resolve_data_config({}, model=model)\n        &gt;&gt;&gt; transform = create_transform(**config)\n\n        &gt;&gt;&gt; image = Image.open(SOURCE_IMAGE_PATH).convert('RGB')\n        &gt;&gt;&gt; x = transform(image).unsqueeze(0)\n\n        &gt;&gt;&gt; output = model(x)\n\n        &gt;&gt;&gt; classifications = sv.Classifications.from_timm(output)\n        ```\n    \"\"\"\n    confidence = timm_results.cpu().detach().numpy()[0]\n\n    if len(confidence) == 0:\n        return cls(class_id=np.array([]), confidence=np.array([]))\n\n    class_id = np.arange(len(confidence))\n    return cls(class_id=class_id, confidence=confidence)\n</code></pre>"},{"location":"classification/core/#supervision.classification.core.Classifications.from_ultralytics","title":"<code>from_ultralytics(ultralytics_results)</code>  <code>classmethod</code>","text":"<p>Creates a Classifications instance from a ultralytics inference result.</p> <p>Parameters:</p> Name Type Description Default <code>ultralytics_results</code> <code>Results</code> <p>The inference result from ultralytics model.</p> required <p>Returns:</p> Name Type Description <code>Classifications</code> <code>Classifications</code> <p>A new Classifications object.</p> Example <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; from ultralytics import YOLO\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n&gt;&gt;&gt; model = YOLO('yolov8n-cls.pt')\n\n&gt;&gt;&gt; output = model(image)[0]\n&gt;&gt;&gt; classifications = sv.Classifications.from_ultralytics(output)\n</code></pre> Source code in <code>supervision/classification/core.py</code> <pre><code>@classmethod\ndef from_ultralytics(cls, ultralytics_results) -&gt; Classifications:\n    \"\"\"\n    Creates a Classifications instance from a\n    [ultralytics](https://github.com/ultralytics/ultralytics) inference result.\n\n    Args:\n        ultralytics_results (ultralytics.engine.results.Results):\n            The inference result from ultralytics model.\n\n    Returns:\n        Classifications: A new Classifications object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import cv2\n        &gt;&gt;&gt; from ultralytics import YOLO\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n        &gt;&gt;&gt; model = YOLO('yolov8n-cls.pt')\n\n        &gt;&gt;&gt; output = model(image)[0]\n        &gt;&gt;&gt; classifications = sv.Classifications.from_ultralytics(output)\n        ```\n    \"\"\"\n    confidence = ultralytics_results.probs.data.cpu().numpy()\n    return cls(class_id=np.arange(confidence.shape[0]), confidence=confidence)\n</code></pre>"},{"location":"classification/core/#supervision.classification.core.Classifications.get_top_k","title":"<code>get_top_k(k)</code>","text":"<p>Retrieve the top k class IDs and confidences,     ordered in descending order by confidence.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of top class IDs and confidences to retrieve.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: A tuple containing the top k class IDs and confidences.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; classifications = sv.Classifications(...)\n\n&gt;&gt;&gt; classifications.get_top_k(1)\n\n(array([1]), array([0.9]))\n</code></pre> Source code in <code>supervision/classification/core.py</code> <pre><code>def get_top_k(self, k: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Retrieve the top k class IDs and confidences,\n        ordered in descending order by confidence.\n\n    Args:\n        k (int): The number of top class IDs and confidences to retrieve.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: A tuple containing\n            the top k class IDs and confidences.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; classifications = sv.Classifications(...)\n\n        &gt;&gt;&gt; classifications.get_top_k(1)\n\n        (array([1]), array([0.9]))\n        ```\n    \"\"\"\n    if self.confidence is None:\n        raise ValueError(\"top_k could not be calculated, confidence is None\")\n\n    order = np.argsort(self.confidence)[::-1]\n    top_k_order = order[:k]\n    top_k_class_id = self.class_id[top_k_order]\n    top_k_confidence = self.confidence[top_k_order]\n\n    return top_k_class_id, top_k_confidence\n</code></pre>"},{"location":"detection/core/","title":"Core","text":""},{"location":"detection/core/#detections","title":"Detections","text":"<p>Data class containing information about the detections in a video frame.</p> <p>Attributes:</p> Name Type Description <code>xyxy</code> <code>ndarray</code> <p>An array of shape <code>(n, 4)</code> containing the bounding boxes coordinates in format <code>[x1, y1, x2, y2]</code></p> <code>mask</code> <code>Optional[ndarray]</code> <p>(Optional[np.ndarray]): An array of shape <code>(n, H, W)</code> containing the segmentation masks.</p> <code>confidence</code> <code>Optional[ndarray]</code> <p>An array of shape <code>(n,)</code> containing the confidence scores of the detections.</p> <code>class_id</code> <code>Optional[ndarray]</code> <p>An array of shape <code>(n,)</code> containing the class ids of the detections.</p> <code>tracker_id</code> <code>Optional[ndarray]</code> <p>An array of shape <code>(n,)</code> containing the tracker ids of the detections.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>@dataclass\nclass Detections:\n    \"\"\"\n    Data class containing information about the detections in a video frame.\n\n    Attributes:\n        xyxy (np.ndarray): An array of shape `(n, 4)` containing\n            the bounding boxes coordinates in format `[x1, y1, x2, y2]`\n        mask: (Optional[np.ndarray]): An array of shape\n            `(n, H, W)` containing the segmentation masks.\n        confidence (Optional[np.ndarray]): An array of shape\n            `(n,)` containing the confidence scores of the detections.\n        class_id (Optional[np.ndarray]): An array of shape\n            `(n,)` containing the class ids of the detections.\n        tracker_id (Optional[np.ndarray]): An array of shape\n            `(n,)` containing the tracker ids of the detections.\n    \"\"\"\n\n    xyxy: np.ndarray\n    mask: Optional[np.ndarray] = None\n    confidence: Optional[np.ndarray] = None\n    class_id: Optional[np.ndarray] = None\n    tracker_id: Optional[np.ndarray] = None\n\n    def __post_init__(self):\n        n = len(self.xyxy)\n        _validate_xyxy(xyxy=self.xyxy, n=n)\n        _validate_mask(mask=self.mask, n=n)\n        _validate_class_id(class_id=self.class_id, n=n)\n        _validate_confidence(confidence=self.confidence, n=n)\n        _validate_tracker_id(tracker_id=self.tracker_id, n=n)\n\n    def __len__(self):\n        \"\"\"\n        Returns the number of detections in the Detections object.\n        \"\"\"\n        return len(self.xyxy)\n\n    def __iter__(\n        self,\n    ) -&gt; Iterator[\n        Tuple[\n            np.ndarray,\n            Optional[np.ndarray],\n            Optional[float],\n            Optional[int],\n            Optional[int],\n        ]\n    ]:\n        \"\"\"\n        Iterates over the Detections object and yield a tuple of\n        `(xyxy, mask, confidence, class_id, tracker_id)` for each detection.\n        \"\"\"\n        for i in range(len(self.xyxy)):\n            yield (\n                self.xyxy[i],\n                self.mask[i] if self.mask is not None else None,\n                self.confidence[i] if self.confidence is not None else None,\n                self.class_id[i] if self.class_id is not None else None,\n                self.tracker_id[i] if self.tracker_id is not None else None,\n            )\n\n    def __eq__(self, other: Detections):\n        return all(\n            [\n                np.array_equal(self.xyxy, other.xyxy),\n                any(\n                    [\n                        self.mask is None and other.mask is None,\n                        np.array_equal(self.mask, other.mask),\n                    ]\n                ),\n                any(\n                    [\n                        self.class_id is None and other.class_id is None,\n                        np.array_equal(self.class_id, other.class_id),\n                    ]\n                ),\n                any(\n                    [\n                        self.confidence is None and other.confidence is None,\n                        np.array_equal(self.confidence, other.confidence),\n                    ]\n                ),\n                any(\n                    [\n                        self.tracker_id is None and other.tracker_id is None,\n                        np.array_equal(self.tracker_id, other.tracker_id),\n                    ]\n                ),\n            ]\n        )\n\n    @classmethod\n    def from_yolov5(cls, yolov5_results) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from a\n        [YOLOv5](https://github.com/ultralytics/yolov5) inference result.\n\n        Args:\n            yolov5_results (yolov5.models.common.Detections):\n                The output Detections instance from YOLOv5\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import cv2\n            &gt;&gt;&gt; import torch\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n            &gt;&gt;&gt; model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n            &gt;&gt;&gt; result = model(image)\n            &gt;&gt;&gt; detections = sv.Detections.from_yolov5(result)\n            ```\n        \"\"\"\n        yolov5_detections_predictions = yolov5_results.pred[0].cpu().cpu().numpy()\n\n        return cls(\n            xyxy=yolov5_detections_predictions[:, :4],\n            confidence=yolov5_detections_predictions[:, 4],\n            class_id=yolov5_detections_predictions[:, 5].astype(int),\n        )\n\n    @classmethod\n    def from_ultralytics(cls, ultralytics_results) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from a\n            [YOLOv8](https://github.com/ultralytics/ultralytics) inference result.\n\n        Args:\n            ultralytics_results (ultralytics.yolo.engine.results.Results):\n                The output Results instance from YOLOv8\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import cv2\n            &gt;&gt;&gt; from ultralytics import YOLO, FastSAM, SAM, RTDETR\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n            &gt;&gt;&gt; model = YOLO('yolov8s.pt')\n            &gt;&gt;&gt; model = SAM('sam_b.pt')\n            &gt;&gt;&gt; model = SAM('mobile_sam.pt')\n            &gt;&gt;&gt; model = FastSAM('FastSAM-s.pt')\n            &gt;&gt;&gt; model = RTDETR('rtdetr-l.pt')\n            &gt;&gt;&gt; # model inferences\n            &gt;&gt;&gt; result = model(image)[0]\n            &gt;&gt;&gt; # if tracker is enabled\n            &gt;&gt;&gt; result = model.track(image)[0]\n            &gt;&gt;&gt; detections = sv.Detections.from_ultralytics(result)\n            ```\n        \"\"\"\n\n        return cls(\n            xyxy=ultralytics_results.boxes.xyxy.cpu().numpy(),\n            confidence=ultralytics_results.boxes.conf.cpu().numpy(),\n            class_id=ultralytics_results.boxes.cls.cpu().numpy().astype(int),\n            mask=extract_ultralytics_masks(ultralytics_results),\n            tracker_id=ultralytics_results.boxes.id.int().cpu().numpy()\n            if ultralytics_results.boxes.id is not None\n            else None,\n        )\n\n    @classmethod\n    def from_yolo_nas(cls, yolo_nas_results) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from a\n        [YOLO-NAS](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md)\n        inference result.\n\n        Args:\n            yolo_nas_results (ImageDetectionPrediction):\n                The output Results instance from YOLO-NAS\n                ImageDetectionPrediction is coming from\n                'super_gradients.training.models.prediction_results'\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import cv2\n            &gt;&gt;&gt; from super_gradients.training import models\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n            &gt;&gt;&gt; model = models.get('yolo_nas_l', pretrained_weights=\"coco\")\n            &gt;&gt;&gt; result = list(model.predict(image, conf=0.35))[0]\n            &gt;&gt;&gt; detections = sv.Detections.from_yolo_nas(result)\n            ```\n        \"\"\"\n        if np.asarray(yolo_nas_results.prediction.bboxes_xyxy).shape[0] == 0:\n            return cls.empty()\n\n        return cls(\n            xyxy=yolo_nas_results.prediction.bboxes_xyxy,\n            confidence=yolo_nas_results.prediction.confidence,\n            class_id=yolo_nas_results.prediction.labels.astype(int),\n        )\n\n    @classmethod\n    def from_tensorflow(\n        cls, tensorflow_results: dict, resolution_wh: tuple\n    ) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from a\n        [Tensorflow Hub](https://www.tensorflow.org/hub/tutorials/tf2_object_detection)\n        inference result.\n\n        Args:\n            tensorflow_results (dict):\n                The output results from Tensorflow Hub.\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import tensorflow as tf\n            &gt;&gt;&gt; import tensorflow_hub as hub\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import cv2\n\n            &gt;&gt;&gt; module_handle = \"https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1\"\n\n            &gt;&gt;&gt; model = hub.load(module_handle)\n\n            &gt;&gt;&gt; img = np.array(cv2.imread(SOURCE_IMAGE_PATH))\n\n            &gt;&gt;&gt; result = model(img)\n\n            &gt;&gt;&gt; detections = sv.Detections.from_tensorflow(result)\n            ```\n        \"\"\"  # noqa: E501 // docs\n\n        boxes = tensorflow_results[\"detection_boxes\"][0].numpy()\n        boxes[:, [0, 2]] *= resolution_wh[0]\n        boxes[:, [1, 3]] *= resolution_wh[1]\n        boxes = boxes[:, [1, 0, 3, 2]]\n        return cls(\n            xyxy=boxes,\n            confidence=tensorflow_results[\"detection_scores\"][0].numpy(),\n            class_id=tensorflow_results[\"detection_classes\"][0].numpy().astype(int),\n        )\n\n    @classmethod\n    def from_deepsparse(cls, deepsparse_results) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from a\n        [DeepSparse](https://github.com/neuralmagic/deepsparse)\n        inference result.\n\n        Args:\n            deepsparse_results (deepsparse.yolo.schemas.YOLOOutput):\n                The output Results instance from DeepSparse.\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; from deepsparse import Pipeline\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; yolo_pipeline = Pipeline.create(\n            ...     task=\"yolo\",\n            ...     model_path = \"zoo:cv/detection/yolov5-l/pytorch/\" \\\n            ...                  \"ultralytics/coco/pruned80_quant-none\"\n            &gt;&gt;&gt; pipeline_outputs = yolo_pipeline(SOURCE_IMAGE_PATH,\n            ...                         iou_thres=0.6, conf_thres=0.001)\n            &gt;&gt;&gt; detections = sv.Detections.from_deepsparse(result)\n            ```\n        \"\"\"\n        if np.asarray(deepsparse_results.boxes[0]).shape[0] == 0:\n            return cls.empty()\n\n        return cls(\n            xyxy=np.array(deepsparse_results.boxes[0]),\n            confidence=np.array(deepsparse_results.scores[0]),\n            class_id=np.array(deepsparse_results.labels[0]).astype(float).astype(int),\n        )\n\n    @classmethod\n    def from_mmdetection(cls, mmdet_results) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from\n        a [mmdetection](https://github.com/open-mmlab/mmdetection) inference result.\n        Also supported for [mmyolo](https://github.com/open-mmlab/mmyolo)\n\n        Args:\n            mmdet_results (mmdet.structures.DetDataSample):\n                The output Results instance from MMDetection.\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import cv2\n            &gt;&gt;&gt; import supervision as sv\n            &gt;&gt;&gt; from mmdet.apis import DetInferencer\n\n            &gt;&gt;&gt; inferencer = DetInferencer(model_name, checkpoint, device)\n            &gt;&gt;&gt; mmdet_result = inferencer(SOURCE_IMAGE_PATH, out_dir='./output',\n            ...                           return_datasamples=True)[\"predictions\"][0]\n            &gt;&gt;&gt; detections = sv.Detections.from_mmdetection(mmdet_result)\n            ```\n        \"\"\"\n\n        return cls(\n            xyxy=mmdet_results.pred_instances.bboxes.cpu().numpy(),\n            confidence=mmdet_results.pred_instances.scores.cpu().numpy(),\n            class_id=mmdet_results.pred_instances.labels.cpu().numpy().astype(int),\n        )\n\n    @classmethod\n    def from_transformers(cls, transformers_results: dict) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from object detection\n        [transformer](https://github.com/huggingface/transformers) inference result.\n\n        Returns:\n            Detections: A new Detections object.\n        \"\"\"\n\n        return cls(\n            xyxy=transformers_results[\"boxes\"].cpu().numpy(),\n            confidence=transformers_results[\"scores\"].cpu().numpy(),\n            class_id=transformers_results[\"labels\"].cpu().numpy().astype(int),\n        )\n\n    @classmethod\n    def from_detectron2(cls, detectron2_results) -&gt; Detections:\n        \"\"\"\n        Create a Detections object from the\n        [Detectron2](https://github.com/facebookresearch/detectron2) inference result.\n\n        Args:\n            detectron2_results: The output of a\n                Detectron2 model containing instances with prediction data.\n\n        Returns:\n            (Detections): A Detections object containing the bounding boxes,\n                class IDs, and confidences of the predictions.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import cv2\n            &gt;&gt;&gt; from detectron2.engine import DefaultPredictor\n            &gt;&gt;&gt; from detectron2.config import get_cfg\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n            &gt;&gt;&gt; cfg = get_cfg()\n            &gt;&gt;&gt; cfg.merge_from_file(\"path/to/config.yaml\")\n            &gt;&gt;&gt; cfg.MODEL.WEIGHTS = \"path/to/model_weights.pth\"\n            &gt;&gt;&gt; predictor = DefaultPredictor(cfg)\n            &gt;&gt;&gt; result = predictor(image)\n            &gt;&gt;&gt; detections = sv.Detections.from_detectron2(result)\n            ```\n        \"\"\"\n\n        return cls(\n            xyxy=detectron2_results[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n            confidence=detectron2_results[\"instances\"].scores.cpu().numpy(),\n            class_id=detectron2_results[\"instances\"]\n            .pred_classes.cpu()\n            .numpy()\n            .astype(int),\n        )\n\n    @classmethod\n    def from_roboflow(cls, roboflow_result: dict) -&gt; Detections:\n        \"\"\"\n        Create a Detections object from the [Roboflow](https://roboflow.com/)\n            API inference result.\n\n        Args:\n            roboflow_result (dict): The result from the\n                Roboflow API containing predictions.\n\n        Returns:\n            (Detections): A Detections object containing the bounding boxes, class IDs,\n                and confidences of the predictions.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; roboflow_result = {\n            ...     \"predictions\": [\n            ...         {\n            ...             \"x\": 0.5,\n            ...             \"y\": 0.5,\n            ...             \"width\": 0.2,\n            ...             \"height\": 0.3,\n            ...             \"class_id\": 0,\n            ...             \"class\": \"person\",\n            ...             \"confidence\": 0.9\n            ...         },\n            ...         # ... more predictions ...\n            ...     ]\n            ... }\n\n            &gt;&gt;&gt; detections = sv.Detections.from_roboflow(roboflow_result)\n            ```\n        \"\"\"\n        xyxy, confidence, class_id, masks, trackers = process_roboflow_result(\n            roboflow_result=roboflow_result\n        )\n\n        if np.asarray(xyxy).shape[0] == 0:\n            return cls.empty()\n\n        return cls(\n            xyxy=xyxy,\n            confidence=confidence,\n            class_id=class_id,\n            mask=masks,\n            tracker_id=trackers,\n        )\n\n    @classmethod\n    def from_sam(cls, sam_result: List[dict]) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from\n        [Segment Anything Model](https://github.com/facebookresearch/segment-anything)\n        inference result.\n\n        Args:\n            sam_result (List[dict]): The output Results instance from SAM\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n            &gt;&gt;&gt; from segment_anything import (\n            ...     sam_model_registry,\n            ...     SamAutomaticMaskGenerator\n            ... )\n\n            &gt;&gt;&gt; sam_model_reg = sam_model_registry[MODEL_TYPE]\n            &gt;&gt;&gt; sam = sam_model_reg(checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n            &gt;&gt;&gt; mask_generator = SamAutomaticMaskGenerator(sam)\n            &gt;&gt;&gt; sam_result = mask_generator.generate(IMAGE)\n            &gt;&gt;&gt; detections = sv.Detections.from_sam(sam_result=sam_result)\n            ```\n        \"\"\"\n\n        sorted_generated_masks = sorted(\n            sam_result, key=lambda x: x[\"area\"], reverse=True\n        )\n\n        xywh = np.array([mask[\"bbox\"] for mask in sorted_generated_masks])\n        mask = np.array([mask[\"segmentation\"] for mask in sorted_generated_masks])\n\n        if np.asarray(xywh).shape[0] == 0:\n            return cls.empty()\n\n        xyxy = xywh_to_xyxy(boxes_xywh=xywh)\n        return cls(xyxy=xyxy, mask=mask)\n\n    @classmethod\n    def from_azure_analyze_image(\n        cls, azure_result: dict, class_map: Optional[Dict[int, str]] = None\n    ) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from [Azure Image Analysis 4.0](\n        https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/\n        concept-object-detection-40).\n\n        Args:\n            azure_result (dict): The result from Azure Image Analysis. It should\n                contain detected objects and their bounding box coordinates.\n            class_map (Optional[Dict[int, str]]): A mapping ofclass IDs (int) to class\n                names (str). If None, a new mapping is created dynamically.\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import requests\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; image = open(input, \"rb\").read()\n\n            &gt;&gt;&gt; endpoint = \"https://.cognitiveservices.azure.com/\"\n            &gt;&gt;&gt; subscription_key = \"...\"\n\n            &gt;&gt;&gt; headers = {\n            ...    \"Content-Type\": \"application/octet-stream\",\n            ...    \"Ocp-Apim-Subscription-Key\": subscription_key\n            ... }\n\n            &gt;&gt;&gt; response = requests.post(endpoint,\n            ...     headers=self.headers,\n            ...     data=image\n            ... ).json()\n\n            &gt;&gt;&gt; detections = sv.Detections.from_azure_analyze_image(response)\n            ```\n        \"\"\"\n        if \"error\" in azure_result:\n            raise ValueError(\n                f'Azure API returned an error {azure_result[\"error\"][\"message\"]}'\n            )\n\n        xyxy, confidences, class_ids = [], [], []\n\n        is_dynamic_mapping = class_map is None\n        if is_dynamic_mapping:\n            class_map = {}\n\n        class_map = {value: key for key, value in class_map.items()}\n\n        for detection in azure_result[\"objectsResult\"][\"values\"]:\n            bbox = detection[\"boundingBox\"]\n\n            tags = detection[\"tags\"]\n\n            x0 = bbox[\"x\"]\n            y0 = bbox[\"y\"]\n            x1 = x0 + bbox[\"w\"]\n            y1 = y0 + bbox[\"h\"]\n\n            for tag in tags:\n                confidence = tag[\"confidence\"]\n                class_name = tag[\"name\"]\n                class_id = class_map.get(class_name, None)\n\n                if is_dynamic_mapping and class_id is None:\n                    class_id = len(class_map)\n                    class_map[class_name] = class_id\n\n                if class_id is not None:\n                    xyxy.append([x0, y0, x1, y1])\n                    confidences.append(confidence)\n                    class_ids.append(class_id)\n\n        if len(xyxy) == 0:\n            return Detections.empty()\n\n        return cls(\n            xyxy=np.array(xyxy),\n            class_id=np.array(class_ids),\n            confidence=np.array(confidences),\n        )\n\n    @classmethod\n    def from_paddledet(cls, paddledet_result) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from\n            [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)\n            inference result.\n\n        Args:\n            paddledet_result (List[dict]): The output Results instance from PaddleDet\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n            &gt;&gt;&gt; import paddle\n            &gt;&gt;&gt; from ppdet.engine import Trainer\n            &gt;&gt;&gt; from ppdet.core.workspace import load_config\n\n            &gt;&gt;&gt; weights = (...)\n            &gt;&gt;&gt; config = (...)\n\n            &gt;&gt;&gt; cfg = load_config(config)\n            &gt;&gt;&gt; trainer = Trainer(cfg, mode='test')\n            &gt;&gt;&gt; trainer.load_weights(weights)\n\n            &gt;&gt;&gt; paddledet_result = trainer.predict([images])[0]\n\n            &gt;&gt;&gt; detections = sv.Detections.from_paddledet(paddledet_result)\n            ```\n        \"\"\"\n\n        if np.asarray(paddledet_result[\"bbox\"][:, 2:6]).shape[0] == 0:\n            return cls.empty()\n\n        return cls(\n            xyxy=paddledet_result[\"bbox\"][:, 2:6],\n            confidence=paddledet_result[\"bbox\"][:, 1],\n            class_id=paddledet_result[\"bbox\"][:, 0].astype(int),\n        )\n\n    @classmethod\n    def empty(cls) -&gt; Detections:\n        \"\"\"\n        Create an empty Detections object with no bounding boxes,\n            confidences, or class IDs.\n\n        Returns:\n            (Detections): An empty Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; from supervision import Detections\n\n            &gt;&gt;&gt; empty_detections = Detections.empty()\n            ```\n        \"\"\"\n        return cls(\n            xyxy=np.empty((0, 4), dtype=np.float32),\n            confidence=np.array([], dtype=np.float32),\n            class_id=np.array([], dtype=int),\n        )\n\n    @classmethod\n    def merge(cls, detections_list: List[Detections]) -&gt; Detections:\n        \"\"\"\n        Merge a list of Detections objects into a single Detections object.\n\n        This method takes a list of Detections objects and combines their\n        respective fields (`xyxy`, `mask`, `confidence`, `class_id`, and `tracker_id`)\n        into a single Detections object. If all elements in a field are not\n        `None`, the corresponding field will be stacked.\n        Otherwise, the field will be set to `None`.\n\n        Args:\n            detections_list (List[Detections]): A list of Detections objects to merge.\n\n        Returns:\n            (Detections): A single Detections object containing\n                the merged data from the input list.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; from supervision import Detections\n\n            &gt;&gt;&gt; detections_1 = Detections(...)\n            &gt;&gt;&gt; detections_2 = Detections(...)\n\n            &gt;&gt;&gt; merged_detections = Detections.merge([detections_1, detections_2])\n            ```\n        \"\"\"\n        if len(detections_list) == 0:\n            return Detections.empty()\n\n        detections_tuples_list = [astuple(detection) for detection in detections_list]\n        xyxy, mask, confidence, class_id, tracker_id = [\n            list(field) for field in zip(*detections_tuples_list)\n        ]\n\n        def __all_not_none(item_list: List[Any]):\n            return all(x is not None for x in item_list)\n\n        xyxy = np.vstack(xyxy)\n        mask = np.vstack(mask) if __all_not_none(mask) else None\n        confidence = np.hstack(confidence) if __all_not_none(confidence) else None\n        class_id = np.hstack(class_id) if __all_not_none(class_id) else None\n        tracker_id = np.hstack(tracker_id) if __all_not_none(tracker_id) else None\n\n        return cls(\n            xyxy=xyxy,\n            mask=mask,\n            confidence=confidence,\n            class_id=class_id,\n            tracker_id=tracker_id,\n        )\n\n    def get_anchors_coordinates(self, anchor: Position) -&gt; np.ndarray:\n        \"\"\"\n        Calculates and returns the coordinates of a specific anchor point\n        within the bounding boxes defined by the `xyxy` attribute. The anchor\n        point can be any of the predefined positions in the `Position` enum,\n        such as `CENTER`, `CENTER_LEFT`, `BOTTOM_RIGHT`, etc.\n\n        Args:\n            anchor (Position): An enum specifying the position of the anchor point\n                within the bounding box. Supported positions are defined in the\n                `Position` enum.\n\n        Returns:\n            np.ndarray: An array of shape `(n, 2)`, where `n` is the number of bounding\n                boxes. Each row contains the `[x, y]` coordinates of the specified\n                anchor point for the corresponding bounding box.\n\n        Raises:\n            ValueError: If the provided `anchor` is not supported.\n        \"\"\"\n        if anchor == Position.CENTER:\n            return np.array(\n                [\n                    (self.xyxy[:, 0] + self.xyxy[:, 2]) / 2,\n                    (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n                ]\n            ).transpose()\n        elif anchor == Position.CENTER_OF_MASS:\n            if self.mask is None:\n                raise ValueError(\n                    \"Cannot use `Position.CENTER_OF_MASS` without a detection mask.\"\n                )\n            return calculate_masks_centroids(masks=self.mask)\n        elif anchor == Position.CENTER_LEFT:\n            return np.array(\n                [\n                    self.xyxy[:, 0],\n                    (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n                ]\n            ).transpose()\n        elif anchor == Position.CENTER_RIGHT:\n            return np.array(\n                [\n                    self.xyxy[:, 2],\n                    (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n                ]\n            ).transpose()\n        elif anchor == Position.BOTTOM_CENTER:\n            return np.array(\n                [(self.xyxy[:, 0] + self.xyxy[:, 2]) / 2, self.xyxy[:, 3]]\n            ).transpose()\n        elif anchor == Position.BOTTOM_LEFT:\n            return np.array([self.xyxy[:, 0], self.xyxy[:, 3]]).transpose()\n        elif anchor == Position.BOTTOM_RIGHT:\n            return np.array([self.xyxy[:, 2], self.xyxy[:, 3]]).transpose()\n        elif anchor == Position.TOP_CENTER:\n            return np.array(\n                [(self.xyxy[:, 0] + self.xyxy[:, 2]) / 2, self.xyxy[:, 1]]\n            ).transpose()\n        elif anchor == Position.TOP_LEFT:\n            return np.array([self.xyxy[:, 0], self.xyxy[:, 1]]).transpose()\n        elif anchor == Position.TOP_RIGHT:\n            return np.array([self.xyxy[:, 2], self.xyxy[:, 1]]).transpose()\n\n        raise ValueError(f\"{anchor} is not supported.\")\n\n    def __getitem__(\n        self, index: Union[int, slice, List[int], np.ndarray]\n    ) -&gt; Detections:\n        \"\"\"\n        Get a subset of the Detections object.\n\n        Args:\n            index (Union[int, slice, List[int], np.ndarray]):\n                The index or indices of the subset of the Detections\n\n        Returns:\n            (Detections): A subset of the Detections object.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; detections = sv.Detections(...)\n\n            &gt;&gt;&gt; first_detection = detections[0]\n\n            &gt;&gt;&gt; first_10_detections = detections[0:10]\n\n            &gt;&gt;&gt; some_detections = detections[[0, 2, 4]]\n\n            &gt;&gt;&gt; class_0_detections = detections[detections.class_id == 0]\n\n            &gt;&gt;&gt; high_confidence_detections = detections[detections.confidence &gt; 0.5]\n            ```\n        \"\"\"\n        if isinstance(index, int):\n            index = [index]\n        return Detections(\n            xyxy=self.xyxy[index],\n            mask=self.mask[index] if self.mask is not None else None,\n            confidence=self.confidence[index] if self.confidence is not None else None,\n            class_id=self.class_id[index] if self.class_id is not None else None,\n            tracker_id=self.tracker_id[index] if self.tracker_id is not None else None,\n        )\n\n    @property\n    def area(self) -&gt; np.ndarray:\n        \"\"\"\n        Calculate the area of each detection in the set of object detections.\n        If masks field is defined property returns are of each mask.\n        If only box is given property return area of each box.\n\n        Returns:\n          np.ndarray: An array of floats containing the area of each detection\n            in the format of `(area_1, area_2, ..., area_n)`,\n            where n is the number of detections.\n        \"\"\"\n        if self.mask is not None:\n            return np.array([np.sum(mask) for mask in self.mask])\n        else:\n            return self.box_area\n\n    @property\n    def box_area(self) -&gt; np.ndarray:\n        \"\"\"\n        Calculate the area of each bounding box in the set of object detections.\n\n        Returns:\n            np.ndarray: An array of floats containing the area of each bounding\n                box in the format of `(area_1, area_2, ..., area_n)`,\n                where n is the number of detections.\n        \"\"\"\n        return (self.xyxy[:, 3] - self.xyxy[:, 1]) * (self.xyxy[:, 2] - self.xyxy[:, 0])\n\n    def with_nms(\n        self, threshold: float = 0.5, class_agnostic: bool = False\n    ) -&gt; Detections:\n        \"\"\"\n        Perform non-maximum suppression on the current set of object detections.\n\n        Args:\n            threshold (float, optional): The intersection-over-union threshold\n                to use for non-maximum suppression. Defaults to 0.5.\n            class_agnostic (bool, optional): Whether to perform class-agnostic\n                non-maximum suppression. If True, the class_id of each detection\n                will be ignored. Defaults to False.\n\n        Returns:\n            Detections: A new Detections object containing the subset of detections\n                after non-maximum suppression.\n\n        Raises:\n            AssertionError: If `confidence` is None and class_agnostic is False.\n                If `class_id` is None and class_agnostic is False.\n        \"\"\"\n        if len(self) == 0:\n            return self\n\n        assert (\n            self.confidence is not None\n        ), \"Detections confidence must be given for NMS to be executed.\"\n\n        if class_agnostic:\n            predictions = np.hstack((self.xyxy, self.confidence.reshape(-1, 1)))\n            indices = non_max_suppression(\n                predictions=predictions, iou_threshold=threshold\n            )\n            return self[indices]\n\n        assert self.class_id is not None, (\n            \"Detections class_id must be given for NMS to be executed. If you intended\"\n            \" to perform class agnostic NMS set class_agnostic=True.\"\n        )\n\n        predictions = np.hstack(\n            (self.xyxy, self.confidence.reshape(-1, 1), self.class_id.reshape(-1, 1))\n        )\n        indices = non_max_suppression(predictions=predictions, iou_threshold=threshold)\n        return self[indices]\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.area","title":"<code>area: np.ndarray</code>  <code>property</code>","text":"<p>Calculate the area of each detection in the set of object detections. If masks field is defined property returns are of each mask. If only box is given property return area of each box.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of floats containing the area of each detection in the format of <code>(area_1, area_2, ..., area_n)</code>, where n is the number of detections.</p>"},{"location":"detection/core/#supervision.detection.core.Detections.box_area","title":"<code>box_area: np.ndarray</code>  <code>property</code>","text":"<p>Calculate the area of each bounding box in the set of object detections.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of floats containing the area of each bounding box in the format of <code>(area_1, area_2, ..., area_n)</code>, where n is the number of detections.</p>"},{"location":"detection/core/#supervision.detection.core.Detections.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get a subset of the Detections object.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[int, slice, List[int], ndarray]</code> <p>The index or indices of the subset of the Detections</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>A subset of the Detections object.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; first_detection = detections[0]\n\n&gt;&gt;&gt; first_10_detections = detections[0:10]\n\n&gt;&gt;&gt; some_detections = detections[[0, 2, 4]]\n\n&gt;&gt;&gt; class_0_detections = detections[detections.class_id == 0]\n\n&gt;&gt;&gt; high_confidence_detections = detections[detections.confidence &gt; 0.5]\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>def __getitem__(\n    self, index: Union[int, slice, List[int], np.ndarray]\n) -&gt; Detections:\n    \"\"\"\n    Get a subset of the Detections object.\n\n    Args:\n        index (Union[int, slice, List[int], np.ndarray]):\n            The index or indices of the subset of the Detections\n\n    Returns:\n        (Detections): A subset of the Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; detections = sv.Detections(...)\n\n        &gt;&gt;&gt; first_detection = detections[0]\n\n        &gt;&gt;&gt; first_10_detections = detections[0:10]\n\n        &gt;&gt;&gt; some_detections = detections[[0, 2, 4]]\n\n        &gt;&gt;&gt; class_0_detections = detections[detections.class_id == 0]\n\n        &gt;&gt;&gt; high_confidence_detections = detections[detections.confidence &gt; 0.5]\n        ```\n    \"\"\"\n    if isinstance(index, int):\n        index = [index]\n    return Detections(\n        xyxy=self.xyxy[index],\n        mask=self.mask[index] if self.mask is not None else None,\n        confidence=self.confidence[index] if self.confidence is not None else None,\n        class_id=self.class_id[index] if self.class_id is not None else None,\n        tracker_id=self.tracker_id[index] if self.tracker_id is not None else None,\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over the Detections object and yield a tuple of <code>(xyxy, mask, confidence, class_id, tracker_id)</code> for each detection.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Iterator[\n    Tuple[\n        np.ndarray,\n        Optional[np.ndarray],\n        Optional[float],\n        Optional[int],\n        Optional[int],\n    ]\n]:\n    \"\"\"\n    Iterates over the Detections object and yield a tuple of\n    `(xyxy, mask, confidence, class_id, tracker_id)` for each detection.\n    \"\"\"\n    for i in range(len(self.xyxy)):\n        yield (\n            self.xyxy[i],\n            self.mask[i] if self.mask is not None else None,\n            self.confidence[i] if self.confidence is not None else None,\n            self.class_id[i] if self.class_id is not None else None,\n            self.tracker_id[i] if self.tracker_id is not None else None,\n        )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of detections in the Detections object.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Returns the number of detections in the Detections object.\n    \"\"\"\n    return len(self.xyxy)\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Create an empty Detections object with no bounding boxes,     confidences, or class IDs.</p> <p>Returns:</p> Type Description <code>Detections</code> <p>An empty Detections object.</p> Example <pre><code>&gt;&gt;&gt; from supervision import Detections\n\n&gt;&gt;&gt; empty_detections = Detections.empty()\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; Detections:\n    \"\"\"\n    Create an empty Detections object with no bounding boxes,\n        confidences, or class IDs.\n\n    Returns:\n        (Detections): An empty Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; from supervision import Detections\n\n        &gt;&gt;&gt; empty_detections = Detections.empty()\n        ```\n    \"\"\"\n    return cls(\n        xyxy=np.empty((0, 4), dtype=np.float32),\n        confidence=np.array([], dtype=np.float32),\n        class_id=np.array([], dtype=int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_azure_analyze_image","title":"<code>from_azure_analyze_image(azure_result, class_map=None)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from Azure Image Analysis 4.0.</p> <p>Parameters:</p> Name Type Description Default <code>azure_result</code> <code>dict</code> <p>The result from Azure Image Analysis. It should contain detected objects and their bounding box coordinates.</p> required <code>class_map</code> <code>Optional[Dict[int, str]]</code> <p>A mapping ofclass IDs (int) to class names (str). If None, a new mapping is created dynamically.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>&gt;&gt;&gt; import requests\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = open(input, \"rb\").read()\n\n&gt;&gt;&gt; endpoint = \"https://.cognitiveservices.azure.com/\"\n&gt;&gt;&gt; subscription_key = \"...\"\n\n&gt;&gt;&gt; headers = {\n...    \"Content-Type\": \"application/octet-stream\",\n...    \"Ocp-Apim-Subscription-Key\": subscription_key\n... }\n\n&gt;&gt;&gt; response = requests.post(endpoint,\n...     headers=self.headers,\n...     data=image\n... ).json()\n\n&gt;&gt;&gt; detections = sv.Detections.from_azure_analyze_image(response)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_azure_analyze_image(\n    cls, azure_result: dict, class_map: Optional[Dict[int, str]] = None\n) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from [Azure Image Analysis 4.0](\n    https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/\n    concept-object-detection-40).\n\n    Args:\n        azure_result (dict): The result from Azure Image Analysis. It should\n            contain detected objects and their bounding box coordinates.\n        class_map (Optional[Dict[int, str]]): A mapping ofclass IDs (int) to class\n            names (str). If None, a new mapping is created dynamically.\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import requests\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = open(input, \"rb\").read()\n\n        &gt;&gt;&gt; endpoint = \"https://.cognitiveservices.azure.com/\"\n        &gt;&gt;&gt; subscription_key = \"...\"\n\n        &gt;&gt;&gt; headers = {\n        ...    \"Content-Type\": \"application/octet-stream\",\n        ...    \"Ocp-Apim-Subscription-Key\": subscription_key\n        ... }\n\n        &gt;&gt;&gt; response = requests.post(endpoint,\n        ...     headers=self.headers,\n        ...     data=image\n        ... ).json()\n\n        &gt;&gt;&gt; detections = sv.Detections.from_azure_analyze_image(response)\n        ```\n    \"\"\"\n    if \"error\" in azure_result:\n        raise ValueError(\n            f'Azure API returned an error {azure_result[\"error\"][\"message\"]}'\n        )\n\n    xyxy, confidences, class_ids = [], [], []\n\n    is_dynamic_mapping = class_map is None\n    if is_dynamic_mapping:\n        class_map = {}\n\n    class_map = {value: key for key, value in class_map.items()}\n\n    for detection in azure_result[\"objectsResult\"][\"values\"]:\n        bbox = detection[\"boundingBox\"]\n\n        tags = detection[\"tags\"]\n\n        x0 = bbox[\"x\"]\n        y0 = bbox[\"y\"]\n        x1 = x0 + bbox[\"w\"]\n        y1 = y0 + bbox[\"h\"]\n\n        for tag in tags:\n            confidence = tag[\"confidence\"]\n            class_name = tag[\"name\"]\n            class_id = class_map.get(class_name, None)\n\n            if is_dynamic_mapping and class_id is None:\n                class_id = len(class_map)\n                class_map[class_name] = class_id\n\n            if class_id is not None:\n                xyxy.append([x0, y0, x1, y1])\n                confidences.append(confidence)\n                class_ids.append(class_id)\n\n    if len(xyxy) == 0:\n        return Detections.empty()\n\n    return cls(\n        xyxy=np.array(xyxy),\n        class_id=np.array(class_ids),\n        confidence=np.array(confidences),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_deepsparse","title":"<code>from_deepsparse(deepsparse_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a DeepSparse inference result.</p> <p>Parameters:</p> Name Type Description Default <code>deepsparse_results</code> <code>YOLOOutput</code> <p>The output Results instance from DeepSparse.</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>&gt;&gt;&gt; from deepsparse import Pipeline\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; yolo_pipeline = Pipeline.create(\n...     task=\"yolo\",\n...     model_path = \"zoo:cv/detection/yolov5-l/pytorch/\"             ...                  \"ultralytics/coco/pruned80_quant-none\"\n&gt;&gt;&gt; pipeline_outputs = yolo_pipeline(SOURCE_IMAGE_PATH,\n...                         iou_thres=0.6, conf_thres=0.001)\n&gt;&gt;&gt; detections = sv.Detections.from_deepsparse(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_deepsparse(cls, deepsparse_results) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from a\n    [DeepSparse](https://github.com/neuralmagic/deepsparse)\n    inference result.\n\n    Args:\n        deepsparse_results (deepsparse.yolo.schemas.YOLOOutput):\n            The output Results instance from DeepSparse.\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; from deepsparse import Pipeline\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; yolo_pipeline = Pipeline.create(\n        ...     task=\"yolo\",\n        ...     model_path = \"zoo:cv/detection/yolov5-l/pytorch/\" \\\n        ...                  \"ultralytics/coco/pruned80_quant-none\"\n        &gt;&gt;&gt; pipeline_outputs = yolo_pipeline(SOURCE_IMAGE_PATH,\n        ...                         iou_thres=0.6, conf_thres=0.001)\n        &gt;&gt;&gt; detections = sv.Detections.from_deepsparse(result)\n        ```\n    \"\"\"\n    if np.asarray(deepsparse_results.boxes[0]).shape[0] == 0:\n        return cls.empty()\n\n    return cls(\n        xyxy=np.array(deepsparse_results.boxes[0]),\n        confidence=np.array(deepsparse_results.scores[0]),\n        class_id=np.array(deepsparse_results.labels[0]).astype(float).astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_detectron2","title":"<code>from_detectron2(detectron2_results)</code>  <code>classmethod</code>","text":"<p>Create a Detections object from the Detectron2 inference result.</p> <p>Parameters:</p> Name Type Description Default <code>detectron2_results</code> <p>The output of a Detectron2 model containing instances with prediction data.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>A Detections object containing the bounding boxes, class IDs, and confidences of the predictions.</p> Example <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; from detectron2.engine import DefaultPredictor\n&gt;&gt;&gt; from detectron2.config import get_cfg\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n&gt;&gt;&gt; cfg = get_cfg()\n&gt;&gt;&gt; cfg.merge_from_file(\"path/to/config.yaml\")\n&gt;&gt;&gt; cfg.MODEL.WEIGHTS = \"path/to/model_weights.pth\"\n&gt;&gt;&gt; predictor = DefaultPredictor(cfg)\n&gt;&gt;&gt; result = predictor(image)\n&gt;&gt;&gt; detections = sv.Detections.from_detectron2(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_detectron2(cls, detectron2_results) -&gt; Detections:\n    \"\"\"\n    Create a Detections object from the\n    [Detectron2](https://github.com/facebookresearch/detectron2) inference result.\n\n    Args:\n        detectron2_results: The output of a\n            Detectron2 model containing instances with prediction data.\n\n    Returns:\n        (Detections): A Detections object containing the bounding boxes,\n            class IDs, and confidences of the predictions.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import cv2\n        &gt;&gt;&gt; from detectron2.engine import DefaultPredictor\n        &gt;&gt;&gt; from detectron2.config import get_cfg\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n        &gt;&gt;&gt; cfg = get_cfg()\n        &gt;&gt;&gt; cfg.merge_from_file(\"path/to/config.yaml\")\n        &gt;&gt;&gt; cfg.MODEL.WEIGHTS = \"path/to/model_weights.pth\"\n        &gt;&gt;&gt; predictor = DefaultPredictor(cfg)\n        &gt;&gt;&gt; result = predictor(image)\n        &gt;&gt;&gt; detections = sv.Detections.from_detectron2(result)\n        ```\n    \"\"\"\n\n    return cls(\n        xyxy=detectron2_results[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n        confidence=detectron2_results[\"instances\"].scores.cpu().numpy(),\n        class_id=detectron2_results[\"instances\"]\n        .pred_classes.cpu()\n        .numpy()\n        .astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_mmdetection","title":"<code>from_mmdetection(mmdet_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a mmdetection inference result. Also supported for mmyolo</p> <p>Parameters:</p> Name Type Description Default <code>mmdet_results</code> <code>DetDataSample</code> <p>The output Results instance from MMDetection.</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from mmdet.apis import DetInferencer\n\n&gt;&gt;&gt; inferencer = DetInferencer(model_name, checkpoint, device)\n&gt;&gt;&gt; mmdet_result = inferencer(SOURCE_IMAGE_PATH, out_dir='./output',\n...                           return_datasamples=True)[\"predictions\"][0]\n&gt;&gt;&gt; detections = sv.Detections.from_mmdetection(mmdet_result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_mmdetection(cls, mmdet_results) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from\n    a [mmdetection](https://github.com/open-mmlab/mmdetection) inference result.\n    Also supported for [mmyolo](https://github.com/open-mmlab/mmyolo)\n\n    Args:\n        mmdet_results (mmdet.structures.DetDataSample):\n            The output Results instance from MMDetection.\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import cv2\n        &gt;&gt;&gt; import supervision as sv\n        &gt;&gt;&gt; from mmdet.apis import DetInferencer\n\n        &gt;&gt;&gt; inferencer = DetInferencer(model_name, checkpoint, device)\n        &gt;&gt;&gt; mmdet_result = inferencer(SOURCE_IMAGE_PATH, out_dir='./output',\n        ...                           return_datasamples=True)[\"predictions\"][0]\n        &gt;&gt;&gt; detections = sv.Detections.from_mmdetection(mmdet_result)\n        ```\n    \"\"\"\n\n    return cls(\n        xyxy=mmdet_results.pred_instances.bboxes.cpu().numpy(),\n        confidence=mmdet_results.pred_instances.scores.cpu().numpy(),\n        class_id=mmdet_results.pred_instances.labels.cpu().numpy().astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_paddledet","title":"<code>from_paddledet(paddledet_result)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from     PaddleDetection     inference result.</p> <p>Parameters:</p> Name Type Description Default <code>paddledet_result</code> <code>List[dict]</code> <p>The output Results instance from PaddleDet</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; import paddle\n&gt;&gt;&gt; from ppdet.engine import Trainer\n&gt;&gt;&gt; from ppdet.core.workspace import load_config\n\n&gt;&gt;&gt; weights = (...)\n&gt;&gt;&gt; config = (...)\n\n&gt;&gt;&gt; cfg = load_config(config)\n&gt;&gt;&gt; trainer = Trainer(cfg, mode='test')\n&gt;&gt;&gt; trainer.load_weights(weights)\n\n&gt;&gt;&gt; paddledet_result = trainer.predict([images])[0]\n\n&gt;&gt;&gt; detections = sv.Detections.from_paddledet(paddledet_result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_paddledet(cls, paddledet_result) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from\n        [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)\n        inference result.\n\n    Args:\n        paddledet_result (List[dict]): The output Results instance from PaddleDet\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n        &gt;&gt;&gt; import paddle\n        &gt;&gt;&gt; from ppdet.engine import Trainer\n        &gt;&gt;&gt; from ppdet.core.workspace import load_config\n\n        &gt;&gt;&gt; weights = (...)\n        &gt;&gt;&gt; config = (...)\n\n        &gt;&gt;&gt; cfg = load_config(config)\n        &gt;&gt;&gt; trainer = Trainer(cfg, mode='test')\n        &gt;&gt;&gt; trainer.load_weights(weights)\n\n        &gt;&gt;&gt; paddledet_result = trainer.predict([images])[0]\n\n        &gt;&gt;&gt; detections = sv.Detections.from_paddledet(paddledet_result)\n        ```\n    \"\"\"\n\n    if np.asarray(paddledet_result[\"bbox\"][:, 2:6]).shape[0] == 0:\n        return cls.empty()\n\n    return cls(\n        xyxy=paddledet_result[\"bbox\"][:, 2:6],\n        confidence=paddledet_result[\"bbox\"][:, 1],\n        class_id=paddledet_result[\"bbox\"][:, 0].astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_roboflow","title":"<code>from_roboflow(roboflow_result)</code>  <code>classmethod</code>","text":"<p>Create a Detections object from the Roboflow     API inference result.</p> <p>Parameters:</p> Name Type Description Default <code>roboflow_result</code> <code>dict</code> <p>The result from the Roboflow API containing predictions.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>A Detections object containing the bounding boxes, class IDs, and confidences of the predictions.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; roboflow_result = {\n...     \"predictions\": [\n...         {\n...             \"x\": 0.5,\n...             \"y\": 0.5,\n...             \"width\": 0.2,\n...             \"height\": 0.3,\n...             \"class_id\": 0,\n...             \"class\": \"person\",\n...             \"confidence\": 0.9\n...         },\n...         # ... more predictions ...\n...     ]\n... }\n\n&gt;&gt;&gt; detections = sv.Detections.from_roboflow(roboflow_result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_roboflow(cls, roboflow_result: dict) -&gt; Detections:\n    \"\"\"\n    Create a Detections object from the [Roboflow](https://roboflow.com/)\n        API inference result.\n\n    Args:\n        roboflow_result (dict): The result from the\n            Roboflow API containing predictions.\n\n    Returns:\n        (Detections): A Detections object containing the bounding boxes, class IDs,\n            and confidences of the predictions.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; roboflow_result = {\n        ...     \"predictions\": [\n        ...         {\n        ...             \"x\": 0.5,\n        ...             \"y\": 0.5,\n        ...             \"width\": 0.2,\n        ...             \"height\": 0.3,\n        ...             \"class_id\": 0,\n        ...             \"class\": \"person\",\n        ...             \"confidence\": 0.9\n        ...         },\n        ...         # ... more predictions ...\n        ...     ]\n        ... }\n\n        &gt;&gt;&gt; detections = sv.Detections.from_roboflow(roboflow_result)\n        ```\n    \"\"\"\n    xyxy, confidence, class_id, masks, trackers = process_roboflow_result(\n        roboflow_result=roboflow_result\n    )\n\n    if np.asarray(xyxy).shape[0] == 0:\n        return cls.empty()\n\n    return cls(\n        xyxy=xyxy,\n        confidence=confidence,\n        class_id=class_id,\n        mask=masks,\n        tracker_id=trackers,\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_sam","title":"<code>from_sam(sam_result)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from Segment Anything Model inference result.</p> <p>Parameters:</p> Name Type Description Default <code>sam_result</code> <code>List[dict]</code> <p>The output Results instance from SAM</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from segment_anything import (\n...     sam_model_registry,\n...     SamAutomaticMaskGenerator\n... )\n\n&gt;&gt;&gt; sam_model_reg = sam_model_registry[MODEL_TYPE]\n&gt;&gt;&gt; sam = sam_model_reg(checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n&gt;&gt;&gt; mask_generator = SamAutomaticMaskGenerator(sam)\n&gt;&gt;&gt; sam_result = mask_generator.generate(IMAGE)\n&gt;&gt;&gt; detections = sv.Detections.from_sam(sam_result=sam_result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_sam(cls, sam_result: List[dict]) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from\n    [Segment Anything Model](https://github.com/facebookresearch/segment-anything)\n    inference result.\n\n    Args:\n        sam_result (List[dict]): The output Results instance from SAM\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n        &gt;&gt;&gt; from segment_anything import (\n        ...     sam_model_registry,\n        ...     SamAutomaticMaskGenerator\n        ... )\n\n        &gt;&gt;&gt; sam_model_reg = sam_model_registry[MODEL_TYPE]\n        &gt;&gt;&gt; sam = sam_model_reg(checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n        &gt;&gt;&gt; mask_generator = SamAutomaticMaskGenerator(sam)\n        &gt;&gt;&gt; sam_result = mask_generator.generate(IMAGE)\n        &gt;&gt;&gt; detections = sv.Detections.from_sam(sam_result=sam_result)\n        ```\n    \"\"\"\n\n    sorted_generated_masks = sorted(\n        sam_result, key=lambda x: x[\"area\"], reverse=True\n    )\n\n    xywh = np.array([mask[\"bbox\"] for mask in sorted_generated_masks])\n    mask = np.array([mask[\"segmentation\"] for mask in sorted_generated_masks])\n\n    if np.asarray(xywh).shape[0] == 0:\n        return cls.empty()\n\n    xyxy = xywh_to_xyxy(boxes_xywh=xywh)\n    return cls(xyxy=xyxy, mask=mask)\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_tensorflow","title":"<code>from_tensorflow(tensorflow_results, resolution_wh)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a Tensorflow Hub inference result.</p> <p>Parameters:</p> Name Type Description Default <code>tensorflow_results</code> <code>dict</code> <p>The output results from Tensorflow Hub.</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; import tensorflow_hub as hub\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import cv2\n\n&gt;&gt;&gt; module_handle = \"https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1\"\n\n&gt;&gt;&gt; model = hub.load(module_handle)\n\n&gt;&gt;&gt; img = np.array(cv2.imread(SOURCE_IMAGE_PATH))\n\n&gt;&gt;&gt; result = model(img)\n\n&gt;&gt;&gt; detections = sv.Detections.from_tensorflow(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_tensorflow(\n    cls, tensorflow_results: dict, resolution_wh: tuple\n) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from a\n    [Tensorflow Hub](https://www.tensorflow.org/hub/tutorials/tf2_object_detection)\n    inference result.\n\n    Args:\n        tensorflow_results (dict):\n            The output results from Tensorflow Hub.\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import tensorflow as tf\n        &gt;&gt;&gt; import tensorflow_hub as hub\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import cv2\n\n        &gt;&gt;&gt; module_handle = \"https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1\"\n\n        &gt;&gt;&gt; model = hub.load(module_handle)\n\n        &gt;&gt;&gt; img = np.array(cv2.imread(SOURCE_IMAGE_PATH))\n\n        &gt;&gt;&gt; result = model(img)\n\n        &gt;&gt;&gt; detections = sv.Detections.from_tensorflow(result)\n        ```\n    \"\"\"  # noqa: E501 // docs\n\n    boxes = tensorflow_results[\"detection_boxes\"][0].numpy()\n    boxes[:, [0, 2]] *= resolution_wh[0]\n    boxes[:, [1, 3]] *= resolution_wh[1]\n    boxes = boxes[:, [1, 0, 3, 2]]\n    return cls(\n        xyxy=boxes,\n        confidence=tensorflow_results[\"detection_scores\"][0].numpy(),\n        class_id=tensorflow_results[\"detection_classes\"][0].numpy().astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_transformers","title":"<code>from_transformers(transformers_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from object detection transformer inference result.</p> <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_transformers(cls, transformers_results: dict) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from object detection\n    [transformer](https://github.com/huggingface/transformers) inference result.\n\n    Returns:\n        Detections: A new Detections object.\n    \"\"\"\n\n    return cls(\n        xyxy=transformers_results[\"boxes\"].cpu().numpy(),\n        confidence=transformers_results[\"scores\"].cpu().numpy(),\n        class_id=transformers_results[\"labels\"].cpu().numpy().astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_ultralytics","title":"<code>from_ultralytics(ultralytics_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a     YOLOv8 inference result.</p> <p>Parameters:</p> Name Type Description Default <code>ultralytics_results</code> <code>Results</code> <p>The output Results instance from YOLOv8</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; from ultralytics import YOLO, FastSAM, SAM, RTDETR\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n&gt;&gt;&gt; model = YOLO('yolov8s.pt')\n&gt;&gt;&gt; model = SAM('sam_b.pt')\n&gt;&gt;&gt; model = SAM('mobile_sam.pt')\n&gt;&gt;&gt; model = FastSAM('FastSAM-s.pt')\n&gt;&gt;&gt; model = RTDETR('rtdetr-l.pt')\n&gt;&gt;&gt; # model inferences\n&gt;&gt;&gt; result = model(image)[0]\n&gt;&gt;&gt; # if tracker is enabled\n&gt;&gt;&gt; result = model.track(image)[0]\n&gt;&gt;&gt; detections = sv.Detections.from_ultralytics(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_ultralytics(cls, ultralytics_results) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from a\n        [YOLOv8](https://github.com/ultralytics/ultralytics) inference result.\n\n    Args:\n        ultralytics_results (ultralytics.yolo.engine.results.Results):\n            The output Results instance from YOLOv8\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import cv2\n        &gt;&gt;&gt; from ultralytics import YOLO, FastSAM, SAM, RTDETR\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n        &gt;&gt;&gt; model = YOLO('yolov8s.pt')\n        &gt;&gt;&gt; model = SAM('sam_b.pt')\n        &gt;&gt;&gt; model = SAM('mobile_sam.pt')\n        &gt;&gt;&gt; model = FastSAM('FastSAM-s.pt')\n        &gt;&gt;&gt; model = RTDETR('rtdetr-l.pt')\n        &gt;&gt;&gt; # model inferences\n        &gt;&gt;&gt; result = model(image)[0]\n        &gt;&gt;&gt; # if tracker is enabled\n        &gt;&gt;&gt; result = model.track(image)[0]\n        &gt;&gt;&gt; detections = sv.Detections.from_ultralytics(result)\n        ```\n    \"\"\"\n\n    return cls(\n        xyxy=ultralytics_results.boxes.xyxy.cpu().numpy(),\n        confidence=ultralytics_results.boxes.conf.cpu().numpy(),\n        class_id=ultralytics_results.boxes.cls.cpu().numpy().astype(int),\n        mask=extract_ultralytics_masks(ultralytics_results),\n        tracker_id=ultralytics_results.boxes.id.int().cpu().numpy()\n        if ultralytics_results.boxes.id is not None\n        else None,\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_yolo_nas","title":"<code>from_yolo_nas(yolo_nas_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a YOLO-NAS inference result.</p> <p>Parameters:</p> Name Type Description Default <code>yolo_nas_results</code> <code>ImageDetectionPrediction</code> <p>The output Results instance from YOLO-NAS ImageDetectionPrediction is coming from 'super_gradients.training.models.prediction_results'</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; from super_gradients.training import models\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n&gt;&gt;&gt; model = models.get('yolo_nas_l', pretrained_weights=\"coco\")\n&gt;&gt;&gt; result = list(model.predict(image, conf=0.35))[0]\n&gt;&gt;&gt; detections = sv.Detections.from_yolo_nas(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_yolo_nas(cls, yolo_nas_results) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from a\n    [YOLO-NAS](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md)\n    inference result.\n\n    Args:\n        yolo_nas_results (ImageDetectionPrediction):\n            The output Results instance from YOLO-NAS\n            ImageDetectionPrediction is coming from\n            'super_gradients.training.models.prediction_results'\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import cv2\n        &gt;&gt;&gt; from super_gradients.training import models\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n        &gt;&gt;&gt; model = models.get('yolo_nas_l', pretrained_weights=\"coco\")\n        &gt;&gt;&gt; result = list(model.predict(image, conf=0.35))[0]\n        &gt;&gt;&gt; detections = sv.Detections.from_yolo_nas(result)\n        ```\n    \"\"\"\n    if np.asarray(yolo_nas_results.prediction.bboxes_xyxy).shape[0] == 0:\n        return cls.empty()\n\n    return cls(\n        xyxy=yolo_nas_results.prediction.bboxes_xyxy,\n        confidence=yolo_nas_results.prediction.confidence,\n        class_id=yolo_nas_results.prediction.labels.astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_yolov5","title":"<code>from_yolov5(yolov5_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a YOLOv5 inference result.</p> <p>Parameters:</p> Name Type Description Default <code>yolov5_results</code> <code>Detections</code> <p>The output Detections instance from YOLOv5</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n&gt;&gt;&gt; model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n&gt;&gt;&gt; result = model(image)\n&gt;&gt;&gt; detections = sv.Detections.from_yolov5(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_yolov5(cls, yolov5_results) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from a\n    [YOLOv5](https://github.com/ultralytics/yolov5) inference result.\n\n    Args:\n        yolov5_results (yolov5.models.common.Detections):\n            The output Detections instance from YOLOv5\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import cv2\n        &gt;&gt;&gt; import torch\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n        &gt;&gt;&gt; model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n        &gt;&gt;&gt; result = model(image)\n        &gt;&gt;&gt; detections = sv.Detections.from_yolov5(result)\n        ```\n    \"\"\"\n    yolov5_detections_predictions = yolov5_results.pred[0].cpu().cpu().numpy()\n\n    return cls(\n        xyxy=yolov5_detections_predictions[:, :4],\n        confidence=yolov5_detections_predictions[:, 4],\n        class_id=yolov5_detections_predictions[:, 5].astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.get_anchors_coordinates","title":"<code>get_anchors_coordinates(anchor)</code>","text":"<p>Calculates and returns the coordinates of a specific anchor point within the bounding boxes defined by the <code>xyxy</code> attribute. The anchor point can be any of the predefined positions in the <code>Position</code> enum, such as <code>CENTER</code>, <code>CENTER_LEFT</code>, <code>BOTTOM_RIGHT</code>, etc.</p> <p>Parameters:</p> Name Type Description Default <code>anchor</code> <code>Position</code> <p>An enum specifying the position of the anchor point within the bounding box. Supported positions are defined in the <code>Position</code> enum.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of shape <code>(n, 2)</code>, where <code>n</code> is the number of bounding boxes. Each row contains the <code>[x, y]</code> coordinates of the specified anchor point for the corresponding bounding box.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided <code>anchor</code> is not supported.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def get_anchors_coordinates(self, anchor: Position) -&gt; np.ndarray:\n    \"\"\"\n    Calculates and returns the coordinates of a specific anchor point\n    within the bounding boxes defined by the `xyxy` attribute. The anchor\n    point can be any of the predefined positions in the `Position` enum,\n    such as `CENTER`, `CENTER_LEFT`, `BOTTOM_RIGHT`, etc.\n\n    Args:\n        anchor (Position): An enum specifying the position of the anchor point\n            within the bounding box. Supported positions are defined in the\n            `Position` enum.\n\n    Returns:\n        np.ndarray: An array of shape `(n, 2)`, where `n` is the number of bounding\n            boxes. Each row contains the `[x, y]` coordinates of the specified\n            anchor point for the corresponding bounding box.\n\n    Raises:\n        ValueError: If the provided `anchor` is not supported.\n    \"\"\"\n    if anchor == Position.CENTER:\n        return np.array(\n            [\n                (self.xyxy[:, 0] + self.xyxy[:, 2]) / 2,\n                (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n            ]\n        ).transpose()\n    elif anchor == Position.CENTER_OF_MASS:\n        if self.mask is None:\n            raise ValueError(\n                \"Cannot use `Position.CENTER_OF_MASS` without a detection mask.\"\n            )\n        return calculate_masks_centroids(masks=self.mask)\n    elif anchor == Position.CENTER_LEFT:\n        return np.array(\n            [\n                self.xyxy[:, 0],\n                (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n            ]\n        ).transpose()\n    elif anchor == Position.CENTER_RIGHT:\n        return np.array(\n            [\n                self.xyxy[:, 2],\n                (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n            ]\n        ).transpose()\n    elif anchor == Position.BOTTOM_CENTER:\n        return np.array(\n            [(self.xyxy[:, 0] + self.xyxy[:, 2]) / 2, self.xyxy[:, 3]]\n        ).transpose()\n    elif anchor == Position.BOTTOM_LEFT:\n        return np.array([self.xyxy[:, 0], self.xyxy[:, 3]]).transpose()\n    elif anchor == Position.BOTTOM_RIGHT:\n        return np.array([self.xyxy[:, 2], self.xyxy[:, 3]]).transpose()\n    elif anchor == Position.TOP_CENTER:\n        return np.array(\n            [(self.xyxy[:, 0] + self.xyxy[:, 2]) / 2, self.xyxy[:, 1]]\n        ).transpose()\n    elif anchor == Position.TOP_LEFT:\n        return np.array([self.xyxy[:, 0], self.xyxy[:, 1]]).transpose()\n    elif anchor == Position.TOP_RIGHT:\n        return np.array([self.xyxy[:, 2], self.xyxy[:, 1]]).transpose()\n\n    raise ValueError(f\"{anchor} is not supported.\")\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.merge","title":"<code>merge(detections_list)</code>  <code>classmethod</code>","text":"<p>Merge a list of Detections objects into a single Detections object.</p> <p>This method takes a list of Detections objects and combines their respective fields (<code>xyxy</code>, <code>mask</code>, <code>confidence</code>, <code>class_id</code>, and <code>tracker_id</code>) into a single Detections object. If all elements in a field are not <code>None</code>, the corresponding field will be stacked. Otherwise, the field will be set to <code>None</code>.</p> <p>Parameters:</p> Name Type Description Default <code>detections_list</code> <code>List[Detections]</code> <p>A list of Detections objects to merge.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>A single Detections object containing the merged data from the input list.</p> Example <pre><code>&gt;&gt;&gt; from supervision import Detections\n\n&gt;&gt;&gt; detections_1 = Detections(...)\n&gt;&gt;&gt; detections_2 = Detections(...)\n\n&gt;&gt;&gt; merged_detections = Detections.merge([detections_1, detections_2])\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef merge(cls, detections_list: List[Detections]) -&gt; Detections:\n    \"\"\"\n    Merge a list of Detections objects into a single Detections object.\n\n    This method takes a list of Detections objects and combines their\n    respective fields (`xyxy`, `mask`, `confidence`, `class_id`, and `tracker_id`)\n    into a single Detections object. If all elements in a field are not\n    `None`, the corresponding field will be stacked.\n    Otherwise, the field will be set to `None`.\n\n    Args:\n        detections_list (List[Detections]): A list of Detections objects to merge.\n\n    Returns:\n        (Detections): A single Detections object containing\n            the merged data from the input list.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; from supervision import Detections\n\n        &gt;&gt;&gt; detections_1 = Detections(...)\n        &gt;&gt;&gt; detections_2 = Detections(...)\n\n        &gt;&gt;&gt; merged_detections = Detections.merge([detections_1, detections_2])\n        ```\n    \"\"\"\n    if len(detections_list) == 0:\n        return Detections.empty()\n\n    detections_tuples_list = [astuple(detection) for detection in detections_list]\n    xyxy, mask, confidence, class_id, tracker_id = [\n        list(field) for field in zip(*detections_tuples_list)\n    ]\n\n    def __all_not_none(item_list: List[Any]):\n        return all(x is not None for x in item_list)\n\n    xyxy = np.vstack(xyxy)\n    mask = np.vstack(mask) if __all_not_none(mask) else None\n    confidence = np.hstack(confidence) if __all_not_none(confidence) else None\n    class_id = np.hstack(class_id) if __all_not_none(class_id) else None\n    tracker_id = np.hstack(tracker_id) if __all_not_none(tracker_id) else None\n\n    return cls(\n        xyxy=xyxy,\n        mask=mask,\n        confidence=confidence,\n        class_id=class_id,\n        tracker_id=tracker_id,\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.with_nms","title":"<code>with_nms(threshold=0.5, class_agnostic=False)</code>","text":"<p>Perform non-maximum suppression on the current set of object detections.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The intersection-over-union threshold to use for non-maximum suppression. Defaults to 0.5.</p> <code>0.5</code> <code>class_agnostic</code> <code>bool</code> <p>Whether to perform class-agnostic non-maximum suppression. If True, the class_id of each detection will be ignored. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object containing the subset of detections after non-maximum suppression.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>confidence</code> is None and class_agnostic is False. If <code>class_id</code> is None and class_agnostic is False.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def with_nms(\n    self, threshold: float = 0.5, class_agnostic: bool = False\n) -&gt; Detections:\n    \"\"\"\n    Perform non-maximum suppression on the current set of object detections.\n\n    Args:\n        threshold (float, optional): The intersection-over-union threshold\n            to use for non-maximum suppression. Defaults to 0.5.\n        class_agnostic (bool, optional): Whether to perform class-agnostic\n            non-maximum suppression. If True, the class_id of each detection\n            will be ignored. Defaults to False.\n\n    Returns:\n        Detections: A new Detections object containing the subset of detections\n            after non-maximum suppression.\n\n    Raises:\n        AssertionError: If `confidence` is None and class_agnostic is False.\n            If `class_id` is None and class_agnostic is False.\n    \"\"\"\n    if len(self) == 0:\n        return self\n\n    assert (\n        self.confidence is not None\n    ), \"Detections confidence must be given for NMS to be executed.\"\n\n    if class_agnostic:\n        predictions = np.hstack((self.xyxy, self.confidence.reshape(-1, 1)))\n        indices = non_max_suppression(\n            predictions=predictions, iou_threshold=threshold\n        )\n        return self[indices]\n\n    assert self.class_id is not None, (\n        \"Detections class_id must be given for NMS to be executed. If you intended\"\n        \" to perform class agnostic NMS set class_agnostic=True.\"\n    )\n\n    predictions = np.hstack(\n        (self.xyxy, self.confidence.reshape(-1, 1), self.class_id.reshape(-1, 1))\n    )\n    indices = non_max_suppression(predictions=predictions, iou_threshold=threshold)\n    return self[indices]\n</code></pre>"},{"location":"detection/utils/","title":"Utils","text":""},{"location":"detection/utils/#box_iou_batch","title":"box_iou_batch","text":"<p>Compute Intersection over Union (IoU) of two sets of bounding boxes -     <code>boxes_true</code> and <code>boxes_detection</code>. Both sets     of boxes are expected to be in <code>(x_min, y_min, x_max, y_max)</code> format.</p> <p>Parameters:</p> Name Type Description Default <code>boxes_true</code> <code>ndarray</code> <p>2D <code>np.ndarray</code> representing ground-truth boxes. <code>shape = (N, 4)</code> where <code>N</code> is number of true objects.</p> required <code>boxes_detection</code> <code>ndarray</code> <p>2D <code>np.ndarray</code> representing detection boxes. <code>shape = (M, 4)</code> where <code>M</code> is number of detected objects.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Pairwise IoU of boxes from <code>boxes_true</code> and <code>boxes_detection</code>. <code>shape = (N, M)</code> where <code>N</code> is number of true objects and <code>M</code> is number of detected objects.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def box_iou_batch(boxes_true: np.ndarray, boxes_detection: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute Intersection over Union (IoU) of two sets of bounding boxes -\n        `boxes_true` and `boxes_detection`. Both sets\n        of boxes are expected to be in `(x_min, y_min, x_max, y_max)` format.\n\n    Args:\n        boxes_true (np.ndarray): 2D `np.ndarray` representing ground-truth boxes.\n            `shape = (N, 4)` where `N` is number of true objects.\n        boxes_detection (np.ndarray): 2D `np.ndarray` representing detection boxes.\n            `shape = (M, 4)` where `M` is number of detected objects.\n\n    Returns:\n        np.ndarray: Pairwise IoU of boxes from `boxes_true` and `boxes_detection`.\n            `shape = (N, M)` where `N` is number of true objects and\n            `M` is number of detected objects.\n    \"\"\"\n\n    def box_area(box):\n        return (box[2] - box[0]) * (box[3] - box[1])\n\n    area_true = box_area(boxes_true.T)\n    area_detection = box_area(boxes_detection.T)\n\n    top_left = np.maximum(boxes_true[:, None, :2], boxes_detection[:, :2])\n    bottom_right = np.minimum(boxes_true[:, None, 2:], boxes_detection[:, 2:])\n\n    area_inter = np.prod(np.clip(bottom_right - top_left, a_min=0, a_max=None), 2)\n    return area_inter / (area_true[:, None] + area_detection - area_inter)\n</code></pre>"},{"location":"detection/utils/#non_max_suppression","title":"non_max_suppression","text":"<p>Perform Non-Maximum Suppression (NMS) on object detection predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>An array of object detection predictions in the format of <code>(x_min, y_min, x_max, y_max, score)</code> or <code>(x_min, y_min, x_max, y_max, score, class)</code>.</p> required <code>iou_threshold</code> <code>float</code> <p>The intersection-over-union threshold to use for non-maximum suppression.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A boolean array indicating which predictions to keep after n on-maximum suppression.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>iou_threshold</code> is not within the closed range from <code>0</code> to <code>1</code>.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def non_max_suppression(\n    predictions: np.ndarray, iou_threshold: float = 0.5\n) -&gt; np.ndarray:\n    \"\"\"\n    Perform Non-Maximum Suppression (NMS) on object detection predictions.\n\n    Args:\n        predictions (np.ndarray): An array of object detection predictions in\n            the format of `(x_min, y_min, x_max, y_max, score)`\n            or `(x_min, y_min, x_max, y_max, score, class)`.\n        iou_threshold (float, optional): The intersection-over-union threshold\n            to use for non-maximum suppression.\n\n    Returns:\n        np.ndarray: A boolean array indicating which predictions to keep after n\n            on-maximum suppression.\n\n    Raises:\n        AssertionError: If `iou_threshold` is not within the\n            closed range from `0` to `1`.\n    \"\"\"\n    assert 0 &lt;= iou_threshold &lt;= 1, (\n        \"Value of `iou_threshold` must be in the closed range from 0 to 1, \"\n        f\"{iou_threshold} given.\"\n    )\n    rows, columns = predictions.shape\n\n    # add column #5 - category filled with zeros for agnostic nms\n    if columns == 5:\n        predictions = np.c_[predictions, np.zeros(rows)]\n\n    # sort predictions column #4 - score\n    sort_index = np.flip(predictions[:, 4].argsort())\n    predictions = predictions[sort_index]\n\n    boxes = predictions[:, :4]\n    categories = predictions[:, 5]\n    ious = box_iou_batch(boxes, boxes)\n    ious = ious - np.eye(rows)\n\n    keep = np.ones(rows, dtype=bool)\n\n    for index, (iou, category) in enumerate(zip(ious, categories)):\n        if not keep[index]:\n            continue\n\n        # drop detections with iou &gt; iou_threshold and\n        # same category as current detections\n        condition = (iou &gt; iou_threshold) &amp; (categories == category)\n        keep = keep &amp; ~condition\n\n    return keep[sort_index.argsort()]\n</code></pre>"},{"location":"detection/utils/#polygon_to_mask","title":"polygon_to_mask","text":"<p>Generate a mask from a polygon.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>ndarray</code> <p>The polygon for which the mask should be generated, given as a list of vertices.</p> required <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>The width and height of the desired resolution.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The generated 2D mask, where the polygon is marked with <code>1</code>'s and the rest is filled with <code>0</code>'s.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def polygon_to_mask(polygon: np.ndarray, resolution_wh: Tuple[int, int]) -&gt; np.ndarray:\n    \"\"\"Generate a mask from a polygon.\n\n    Args:\n        polygon (np.ndarray): The polygon for which the mask should be generated,\n            given as a list of vertices.\n        resolution_wh (Tuple[int, int]): The width and height of the desired resolution.\n\n    Returns:\n        np.ndarray: The generated 2D mask, where the polygon is marked with\n            `1`'s and the rest is filled with `0`'s.\n    \"\"\"\n    width, height = resolution_wh\n    mask = np.zeros((height, width))\n\n    cv2.fillPoly(mask, [polygon], color=1)\n    return mask\n</code></pre>"},{"location":"detection/utils/#mask_to_xyxy","title":"mask_to_xyxy","text":"<p>Converts a 3D <code>np.array</code> of 2D bool masks into a 2D <code>np.array</code> of bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>ndarray</code> <p>A 3D <code>np.array</code> of shape <code>(N, W, H)</code> containing 2D bool masks</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 2D <code>np.array</code> of shape <code>(N, 4)</code> containing the bounding boxes <code>(x_min, y_min, x_max, y_max)</code> for each mask</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def mask_to_xyxy(masks: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Converts a 3D `np.array` of 2D bool masks into a 2D `np.array` of bounding boxes.\n\n    Parameters:\n        masks (np.ndarray): A 3D `np.array` of shape `(N, W, H)`\n            containing 2D bool masks\n\n    Returns:\n        np.ndarray: A 2D `np.array` of shape `(N, 4)` containing the bounding boxes\n            `(x_min, y_min, x_max, y_max)` for each mask\n    \"\"\"\n    n = masks.shape[0]\n    bboxes = np.zeros((n, 4), dtype=int)\n\n    for i, mask in enumerate(masks):\n        rows, cols = np.where(mask)\n\n        if len(rows) &gt; 0 and len(cols) &gt; 0:\n            x_min, x_max = np.min(cols), np.max(cols)\n            y_min, y_max = np.min(rows), np.max(rows)\n            bboxes[i, :] = [x_min, y_min, x_max, y_max]\n\n    return bboxes\n</code></pre>"},{"location":"detection/utils/#mask_to_polygons","title":"mask_to_polygons","text":"<p>Converts a binary mask to a list of polygons.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>A binary mask represented as a 2D NumPy array of shape <code>(H, W)</code>, where H and W are the height and width of the mask, respectively.</p> required <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List[np.ndarray]: A list of polygons, where each polygon is represented by a NumPy array of shape <code>(N, 2)</code>, containing the <code>x</code>, <code>y</code> coordinates of the points. Polygons with fewer points than <code>MIN_POLYGON_POINT_COUNT = 3</code> are excluded from the output.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def mask_to_polygons(mask: np.ndarray) -&gt; List[np.ndarray]:\n    \"\"\"\n    Converts a binary mask to a list of polygons.\n\n    Parameters:\n        mask (np.ndarray): A binary mask represented as a 2D NumPy array of\n            shape `(H, W)`, where H and W are the height and width of\n            the mask, respectively.\n\n    Returns:\n        List[np.ndarray]: A list of polygons, where each polygon is represented by a\n            NumPy array of shape `(N, 2)`, containing the `x`, `y` coordinates\n            of the points. Polygons with fewer points than `MIN_POLYGON_POINT_COUNT = 3`\n            are excluded from the output.\n    \"\"\"\n\n    contours, _ = cv2.findContours(\n        mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n    )\n    return [\n        np.squeeze(contour, axis=1)\n        for contour in contours\n        if contour.shape[0] &gt;= MIN_POLYGON_POINT_COUNT\n    ]\n</code></pre>"},{"location":"detection/utils/#polygon_to_xyxy","title":"polygon_to_xyxy","text":"<p>Converts a polygon represented by a NumPy array into a bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>ndarray</code> <p>A polygon represented by a NumPy array of shape <code>(N, 2)</code>, containing the <code>x</code>, <code>y</code> coordinates of the points.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D NumPy array containing the bounding box <code>(x_min, y_min, x_max, y_max)</code> of the input polygon.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def polygon_to_xyxy(polygon: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Converts a polygon represented by a NumPy array into a bounding box.\n\n    Parameters:\n        polygon (np.ndarray): A polygon represented by a NumPy array of shape `(N, 2)`,\n            containing the `x`, `y` coordinates of the points.\n\n    Returns:\n        np.ndarray: A 1D NumPy array containing the bounding box\n            `(x_min, y_min, x_max, y_max)` of the input polygon.\n    \"\"\"\n    x_min, y_min = np.min(polygon, axis=0)\n    x_max, y_max = np.max(polygon, axis=0)\n    return np.array([x_min, y_min, x_max, y_max])\n</code></pre>"},{"location":"detection/utils/#filter_polygons_by_area","title":"filter_polygons_by_area","text":"<p>Filters a list of polygons based on their area.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>List[ndarray]</code> <p>A list of polygons, where each polygon is represented by a NumPy array of shape <code>(N, 2)</code>, containing the <code>x</code>, <code>y</code> coordinates of the points.</p> required <code>min_area</code> <code>Optional[float]</code> <p>The minimum area threshold. Only polygons with an area greater than or equal to this value will be included in the output. If set to None, no minimum area constraint will be applied.</p> <code>None</code> <code>max_area</code> <code>Optional[float]</code> <p>The maximum area threshold. Only polygons with an area less than or equal to this value will be included in the output. If set to None, no maximum area constraint will be applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List[np.ndarray]: A new list of polygons containing only those with areas within the specified thresholds.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def filter_polygons_by_area(\n    polygons: List[np.ndarray],\n    min_area: Optional[float] = None,\n    max_area: Optional[float] = None,\n) -&gt; List[np.ndarray]:\n    \"\"\"\n    Filters a list of polygons based on their area.\n\n    Parameters:\n        polygons (List[np.ndarray]): A list of polygons, where each polygon is\n            represented by a NumPy array of shape `(N, 2)`,\n            containing the `x`, `y` coordinates of the points.\n        min_area (Optional[float]): The minimum area threshold.\n            Only polygons with an area greater than or equal to this value\n            will be included in the output. If set to None,\n            no minimum area constraint will be applied.\n        max_area (Optional[float]): The maximum area threshold.\n            Only polygons with an area less than or equal to this value\n            will be included in the output. If set to None,\n            no maximum area constraint will be applied.\n\n    Returns:\n        List[np.ndarray]: A new list of polygons containing only those with\n            areas within the specified thresholds.\n    \"\"\"\n    if min_area is None and max_area is None:\n        return polygons\n    ares = [cv2.contourArea(polygon) for polygon in polygons]\n    return [\n        polygon\n        for polygon, area in zip(polygons, ares)\n        if (min_area is None or area &gt;= min_area)\n        and (max_area is None or area &lt;= max_area)\n    ]\n</code></pre>"},{"location":"detection/utils/#move_boxes","title":"move_boxes","text":"<p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>ndarray</code> <p>An array of shape <code>(n, 4)</code> containing the bounding boxes coordinates in format <code>[x1, y1, x2, y2]</code></p> required <code>offset</code> <code>array</code> <p>An array of shape <code>(2,)</code> containing offset values in format is <code>[dx, dy]</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Repositioned bounding boxes.</p> Example <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; boxes = np.array([[10, 10, 20, 20], [30, 30, 40, 40]])\n&gt;&gt;&gt; offset = np.array([5, 5])\n&gt;&gt;&gt; sv.move_boxes(boxes, offset)\n... array([\n...     [15, 15, 25, 25],\n...     [35, 35, 45, 45]\n... ])\n</code></pre> Source code in <code>supervision/detection/utils.py</code> <pre><code>def move_boxes(xyxy: np.ndarray, offset: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Parameters:\n        xyxy (np.ndarray): An array of shape `(n, 4)` containing the bounding boxes\n            coordinates in format `[x1, y1, x2, y2]`\n        offset (np.array): An array of shape `(2,)` containing offset values in format\n            is `[dx, dy]`.\n\n    Returns:\n        np.ndarray: Repositioned bounding boxes.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; boxes = np.array([[10, 10, 20, 20], [30, 30, 40, 40]])\n        &gt;&gt;&gt; offset = np.array([5, 5])\n        &gt;&gt;&gt; sv.move_boxes(boxes, offset)\n        ... array([\n        ...     [15, 15, 25, 25],\n        ...     [35, 35, 45, 45]\n        ... ])\n        ```\n    \"\"\"\n    return xyxy + np.hstack([offset, offset])\n</code></pre>"},{"location":"detection/utils/#scale_boxes","title":"scale_boxes","text":"<p>Scale the dimensions of bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>ndarray</code> <p>An array of shape <code>(n, 4)</code> containing the bounding boxes coordinates in format <code>[x1, y1, x2, y2]</code></p> required <code>factor</code> <code>float</code> <p>A float value representing the factor by which the box dimensions are scaled. A factor greater than 1 enlarges the boxes, while a factor less than 1 shrinks them.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Scaled bounding boxes.</p> Example <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; boxes = np.array([[10, 10, 20, 20], [30, 30, 40, 40]])\n&gt;&gt;&gt; factor = 1.5\n&gt;&gt;&gt; sv.scale_boxes(boxes, factor)\n... array([\n...     [ 7.5,  7.5, 22.5, 22.5],\n...     [27.5, 27.5, 42.5, 42.5]\n... ])\n</code></pre> Source code in <code>supervision/detection/utils.py</code> <pre><code>def scale_boxes(xyxy: np.ndarray, factor: float) -&gt; np.ndarray:\n    \"\"\"\n    Scale the dimensions of bounding boxes.\n\n    Parameters:\n        xyxy (np.ndarray): An array of shape `(n, 4)` containing the bounding boxes\n            coordinates in format `[x1, y1, x2, y2]`\n        factor (float): A float value representing the factor by which the box\n            dimensions are scaled. A factor greater than 1 enlarges the boxes, while a\n            factor less than 1 shrinks them.\n\n    Returns:\n        np.ndarray: Scaled bounding boxes.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; boxes = np.array([[10, 10, 20, 20], [30, 30, 40, 40]])\n        &gt;&gt;&gt; factor = 1.5\n        &gt;&gt;&gt; sv.scale_boxes(boxes, factor)\n        ... array([\n        ...     [ 7.5,  7.5, 22.5, 22.5],\n        ...     [27.5, 27.5, 42.5, 42.5]\n        ... ])\n        ```\n    \"\"\"\n    centers = (xyxy[:, :2] + xyxy[:, 2:]) / 2\n    new_sizes = (xyxy[:, 2:] - xyxy[:, :2]) * factor\n    return np.concatenate((centers - new_sizes / 2, centers + new_sizes / 2), axis=1)\n</code></pre>"},{"location":"detection/tools/inference_slicer/","title":"Inference Slicer","text":""},{"location":"detection/tools/inference_slicer/#inferenceslicer","title":"InferenceSlicer","text":"<p>InferenceSlicer performs slicing-based inference for small target detection. This method, often referred to as Slicing Adaptive Inference (SAHI), involves dividing a larger image into smaller slices, performing inference on each slice, and then merging the detections.</p> <p>Parameters:</p> Name Type Description Default <code>slice_wh</code> <code>Tuple[int, int]</code> <p>Dimensions of each slice in the format <code>(width, height)</code>.</p> <code>(320, 320)</code> <code>overlap_ratio_wh</code> <code>Tuple[float, float]</code> <p>Overlap ratio between consecutive slices in the format <code>(width_ratio, height_ratio)</code>.</p> <code>(0.2, 0.2)</code> <code>iou_threshold</code> <code>Optional[float]</code> <p>Intersection over Union (IoU) threshold used for non-max suppression.</p> <code>0.5</code> <code>callback</code> <code>Callable</code> <p>A function that performs inference on a given image slice and returns detections.</p> required <code>thread_workers</code> <code>int</code> <p>Number of threads for parallel execution.</p> <code>1</code> Note <p>The class ensures that slices do not exceed the boundaries of the original image. As a result, the final slices in the row and column dimensions might be smaller than the specified slice dimensions if the image's width or height is not a multiple of the slice's width or height minus the overlap.</p> Source code in <code>supervision/detection/tools/inference_slicer.py</code> <pre><code>class InferenceSlicer:\n    \"\"\"\n    InferenceSlicer performs slicing-based inference for small target detection. This\n    method, often referred to as\n    [Slicing Adaptive Inference (SAHI)](https://ieeexplore.ieee.org/document/9897990),\n    involves dividing a larger image into smaller slices, performing inference on each\n    slice, and then merging the detections.\n\n    Args:\n        slice_wh (Tuple[int, int]): Dimensions of each slice in the format\n            `(width, height)`.\n        overlap_ratio_wh (Tuple[float, float]): Overlap ratio between consecutive\n            slices in the format `(width_ratio, height_ratio)`.\n        iou_threshold (Optional[float]): Intersection over Union (IoU) threshold\n            used for non-max suppression.\n        callback (Callable): A function that performs inference on a given image\n            slice and returns detections.\n        thread_workers (int): Number of threads for parallel execution.\n\n    Note:\n        The class ensures that slices do not exceed the boundaries of the original\n        image. As a result, the final slices in the row and column dimensions might be\n        smaller than the specified slice dimensions if the image's width or height is\n        not a multiple of the slice's width or height minus the overlap.\n    \"\"\"\n\n    def __init__(\n        self,\n        callback: Callable[[np.ndarray], Detections],\n        slice_wh: Tuple[int, int] = (320, 320),\n        overlap_ratio_wh: Tuple[float, float] = (0.2, 0.2),\n        iou_threshold: Optional[float] = 0.5,\n        thread_workers: int = 1,\n    ):\n        self.slice_wh = slice_wh\n        self.overlap_ratio_wh = overlap_ratio_wh\n        self.iou_threshold = iou_threshold\n        self.callback = callback\n        self.thread_workers = thread_workers\n        validate_inference_callback(callback=callback)\n\n    def __call__(self, image: np.ndarray) -&gt; Detections:\n        \"\"\"\n        Performs slicing-based inference on the provided image using the specified\n            callback.\n\n        Args:\n            image (np.ndarray): The input image on which inference needs to be\n                performed. The image should be in the format\n                `(height, width, channels)`.\n\n        Returns:\n            Detections: A collection of detections for the entire image after merging\n                results from all slices and applying NMS.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import cv2\n            &gt;&gt;&gt; import supervision as sv\n            &gt;&gt;&gt; from ultralytics import YOLO\n\n            &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n            &gt;&gt;&gt; model = YOLO(...)\n\n            &gt;&gt;&gt; def callback(image_slice: np.ndarray) -&gt; sv.Detections:\n            ...     result = model(image_slice)[0]\n            ...     return sv.Detections.from_ultralytics(result)\n\n            &gt;&gt;&gt; slicer = sv.InferenceSlicer(callback = callback)\n\n            &gt;&gt;&gt; detections = slicer(image)\n            ```\n        \"\"\"\n        detections_list = []\n        resolution_wh = (image.shape[1], image.shape[0])\n        offsets = self._generate_offset(\n            resolution_wh=resolution_wh,\n            slice_wh=self.slice_wh,\n            overlap_ratio_wh=self.overlap_ratio_wh,\n        )\n\n        with ThreadPoolExecutor(max_workers=self.thread_workers) as executor:\n            futures = [\n                executor.submit(self._run_callback, image, offset) for offset in offsets\n            ]\n            for future in as_completed(futures):\n                detections_list.append(future.result())\n\n        return Detections.merge(detections_list=detections_list).with_nms(\n            threshold=self.iou_threshold\n        )\n\n    def _run_callback(self, image, offset) -&gt; Detections:\n        \"\"\"\n        Run the provided callback on a slice of an image.\n\n        Args:\n            image (np.ndarray): The input image on which inference needs to run\n            offset (np.ndarray): An array of shape `(4,)` containing coordinates\n                for the slice.\n\n        Returns:\n            Detections: A collection of detections for the slice.\n        \"\"\"\n        image_slice = crop_image(image=image, xyxy=offset)\n        detections = self.callback(image_slice)\n        detections = move_detections(detections=detections, offset=offset[:2])\n\n        return detections\n\n    @staticmethod\n    def _generate_offset(\n        resolution_wh: Tuple[int, int],\n        slice_wh: Tuple[int, int],\n        overlap_ratio_wh: Tuple[float, float],\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Generate offset coordinates for slicing an image based on the given resolution,\n        slice dimensions, and overlap ratios.\n\n        Args:\n            resolution_wh (Tuple[int, int]): A tuple representing the width and height\n                of the image to be sliced.\n            slice_wh (Tuple[int, int]): A tuple representing the desired width and\n                height of each slice.\n            overlap_ratio_wh (Tuple[float, float]): A tuple representing the desired\n                overlap ratio for width and height between consecutive slices. Each\n                value should be in the range [0, 1), where 0 means no overlap and a\n                value close to 1 means high overlap.\n\n        Returns:\n            np.ndarray: An array of shape `(n, 4)` containing coordinates for each\n                slice in the format `[xmin, ymin, xmax, ymax]`.\n\n        Note:\n            The function ensures that slices do not exceed the boundaries of the\n                original image. As a result, the final slices in the row and column\n                dimensions might be smaller than the specified slice dimensions if the\n                image's width or height is not a multiple of the slice's width or\n                height minus the overlap.\n        \"\"\"\n        slice_width, slice_height = slice_wh\n        image_width, image_height = resolution_wh\n        overlap_ratio_width, overlap_ratio_height = overlap_ratio_wh\n\n        width_stride = slice_width - int(overlap_ratio_width * slice_width)\n        height_stride = slice_height - int(overlap_ratio_height * slice_height)\n\n        ws = np.arange(0, image_width, width_stride)\n        hs = np.arange(0, image_height, height_stride)\n\n        xmin, ymin = np.meshgrid(ws, hs)\n        xmax = np.clip(xmin + slice_width, 0, image_width)\n        ymax = np.clip(ymin + slice_height, 0, image_height)\n\n        offsets = np.stack([xmin, ymin, xmax, ymax], axis=-1).reshape(-1, 4)\n\n        return offsets\n</code></pre>"},{"location":"detection/tools/inference_slicer/#supervision.detection.tools.inference_slicer.InferenceSlicer.__call__","title":"<code>__call__(image)</code>","text":"<p>Performs slicing-based inference on the provided image using the specified     callback.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image on which inference needs to be performed. The image should be in the format <code>(height, width, channels)</code>.</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A collection of detections for the entire image after merging results from all slices and applying NMS.</p> Example <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n&gt;&gt;&gt; model = YOLO(...)\n\n&gt;&gt;&gt; def callback(image_slice: np.ndarray) -&gt; sv.Detections:\n...     result = model(image_slice)[0]\n...     return sv.Detections.from_ultralytics(result)\n\n&gt;&gt;&gt; slicer = sv.InferenceSlicer(callback = callback)\n\n&gt;&gt;&gt; detections = slicer(image)\n</code></pre> Source code in <code>supervision/detection/tools/inference_slicer.py</code> <pre><code>def __call__(self, image: np.ndarray) -&gt; Detections:\n    \"\"\"\n    Performs slicing-based inference on the provided image using the specified\n        callback.\n\n    Args:\n        image (np.ndarray): The input image on which inference needs to be\n            performed. The image should be in the format\n            `(height, width, channels)`.\n\n    Returns:\n        Detections: A collection of detections for the entire image after merging\n            results from all slices and applying NMS.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import cv2\n        &gt;&gt;&gt; import supervision as sv\n        &gt;&gt;&gt; from ultralytics import YOLO\n\n        &gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n        &gt;&gt;&gt; model = YOLO(...)\n\n        &gt;&gt;&gt; def callback(image_slice: np.ndarray) -&gt; sv.Detections:\n        ...     result = model(image_slice)[0]\n        ...     return sv.Detections.from_ultralytics(result)\n\n        &gt;&gt;&gt; slicer = sv.InferenceSlicer(callback = callback)\n\n        &gt;&gt;&gt; detections = slicer(image)\n        ```\n    \"\"\"\n    detections_list = []\n    resolution_wh = (image.shape[1], image.shape[0])\n    offsets = self._generate_offset(\n        resolution_wh=resolution_wh,\n        slice_wh=self.slice_wh,\n        overlap_ratio_wh=self.overlap_ratio_wh,\n    )\n\n    with ThreadPoolExecutor(max_workers=self.thread_workers) as executor:\n        futures = [\n            executor.submit(self._run_callback, image, offset) for offset in offsets\n        ]\n        for future in as_completed(futures):\n            detections_list.append(future.result())\n\n    return Detections.merge(detections_list=detections_list).with_nms(\n        threshold=self.iou_threshold\n    )\n</code></pre>"},{"location":"detection/tools/line_zone/","title":"Line Zone","text":""},{"location":"detection/tools/line_zone/#linezone","title":"LineZone","text":"<p>This class is responsible for counting the number of objects that cross a predefined line.</p> <p>Warning</p> <p>LineZone utilizes the <code>tracker_id</code>. Read here to learn how to plug tracking into your inference pipeline.</p> <p>Attributes:</p> Name Type Description <code>in_count</code> <code>int</code> <p>The number of objects that have crossed the line from outside to inside.</p> <code>out_count</code> <code>int</code> <p>The number of objects that have crossed the line from inside to outside.</p> Source code in <code>supervision/detection/line_counter.py</code> <pre><code>class LineZone:\n    \"\"\"\n    This class is responsible for counting the number of objects that cross a\n    predefined line.\n\n    !!! warning\n\n        LineZone utilizes the `tracker_id`. Read\n        [here](https://supervision.roboflow.com/trackers/) to learn how to plug\n        tracking into your inference pipeline.\n\n    Attributes:\n        in_count (int): The number of objects that have crossed the line from outside\n            to inside.\n        out_count (int): The number of objects that have crossed the line from inside\n            to outside.\n    \"\"\"\n\n    def __init__(self, start: Point, end: Point):\n        \"\"\"\n        Args:\n            start (Point): The starting point of the line.\n            end (Point): The ending point of the line.\n        \"\"\"\n        self.vector = Vector(start=start, end=end)\n        self.tracker_state: Dict[str, bool] = {}\n        self.in_count: int = 0\n        self.out_count: int = 0\n\n    def trigger(self, detections: Detections) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Update the `in_count` and `out_count` based on the objects that cross the line.\n\n        Args:\n            detections (Detections): A list of detections for which to update the\n                counts.\n\n        Returns:\n            A tuple of two boolean NumPy arrays. The first array indicates which\n                detections have crossed the line from outside to inside. The second\n                array indicates which detections have crossed the line from inside to\n                outside.\n        \"\"\"\n        crossed_in = np.full(len(detections), False)\n        crossed_out = np.full(len(detections), False)\n\n        for i, (xyxy, _, confidence, class_id, tracker_id) in enumerate(detections):\n            if tracker_id is None:\n                continue\n\n            x1, y1, x2, y2 = xyxy\n            anchors = [\n                Point(x=x1, y=y1),\n                Point(x=x1, y=y2),\n                Point(x=x2, y=y1),\n                Point(x=x2, y=y2),\n            ]\n            triggers = [self.vector.is_in(point=anchor) for anchor in anchors]\n\n            if len(set(triggers)) == 2:\n                continue\n\n            tracker_state = triggers[0]\n\n            if tracker_id not in self.tracker_state:\n                self.tracker_state[tracker_id] = tracker_state\n                continue\n\n            if self.tracker_state.get(tracker_id) == tracker_state:\n                continue\n\n            self.tracker_state[tracker_id] = tracker_state\n            if tracker_state:\n                self.in_count += 1\n                crossed_in[i] = True\n            else:\n                self.out_count += 1\n                crossed_out[i] = True\n\n        return crossed_in, crossed_out\n</code></pre>"},{"location":"detection/tools/line_zone/#supervision.detection.line_counter.LineZone.__init__","title":"<code>__init__(start, end)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>start</code> <code>Point</code> <p>The starting point of the line.</p> required <code>end</code> <code>Point</code> <p>The ending point of the line.</p> required Source code in <code>supervision/detection/line_counter.py</code> <pre><code>def __init__(self, start: Point, end: Point):\n    \"\"\"\n    Args:\n        start (Point): The starting point of the line.\n        end (Point): The ending point of the line.\n    \"\"\"\n    self.vector = Vector(start=start, end=end)\n    self.tracker_state: Dict[str, bool] = {}\n    self.in_count: int = 0\n    self.out_count: int = 0\n</code></pre>"},{"location":"detection/tools/line_zone/#supervision.detection.line_counter.LineZone.trigger","title":"<code>trigger(detections)</code>","text":"<p>Update the <code>in_count</code> and <code>out_count</code> based on the objects that cross the line.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>A list of detections for which to update the counts.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple of two boolean NumPy arrays. The first array indicates which detections have crossed the line from outside to inside. The second array indicates which detections have crossed the line from inside to outside.</p> Source code in <code>supervision/detection/line_counter.py</code> <pre><code>def trigger(self, detections: Detections) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Update the `in_count` and `out_count` based on the objects that cross the line.\n\n    Args:\n        detections (Detections): A list of detections for which to update the\n            counts.\n\n    Returns:\n        A tuple of two boolean NumPy arrays. The first array indicates which\n            detections have crossed the line from outside to inside. The second\n            array indicates which detections have crossed the line from inside to\n            outside.\n    \"\"\"\n    crossed_in = np.full(len(detections), False)\n    crossed_out = np.full(len(detections), False)\n\n    for i, (xyxy, _, confidence, class_id, tracker_id) in enumerate(detections):\n        if tracker_id is None:\n            continue\n\n        x1, y1, x2, y2 = xyxy\n        anchors = [\n            Point(x=x1, y=y1),\n            Point(x=x1, y=y2),\n            Point(x=x2, y=y1),\n            Point(x=x2, y=y2),\n        ]\n        triggers = [self.vector.is_in(point=anchor) for anchor in anchors]\n\n        if len(set(triggers)) == 2:\n            continue\n\n        tracker_state = triggers[0]\n\n        if tracker_id not in self.tracker_state:\n            self.tracker_state[tracker_id] = tracker_state\n            continue\n\n        if self.tracker_state.get(tracker_id) == tracker_state:\n            continue\n\n        self.tracker_state[tracker_id] = tracker_state\n        if tracker_state:\n            self.in_count += 1\n            crossed_in[i] = True\n        else:\n            self.out_count += 1\n            crossed_out[i] = True\n\n    return crossed_in, crossed_out\n</code></pre>"},{"location":"detection/tools/polygon_zone/","title":"Polygon Zone","text":""},{"location":"detection/tools/polygon_zone/#polygonzone","title":"PolygonZone","text":"<p>A class for defining a polygon-shaped zone within a frame for detecting objects.</p> <p>Attributes:</p> Name Type Description <code>polygon</code> <code>ndarray</code> <p>A polygon represented by a numpy array of shape <code>(N, 2)</code>, containing the <code>x</code>, <code>y</code> coordinates of the points.</p> <code>frame_resolution_wh</code> <code>Tuple[int, int]</code> <p>The frame resolution (width, height)</p> <code>triggering_position</code> <code>Position</code> <p>The position within the bounding box that triggers the zone (default: Position.BOTTOM_CENTER)</p> <code>current_count</code> <code>int</code> <p>The current count of detected objects within the zone</p> <code>mask</code> <code>ndarray</code> <p>The 2D bool mask for the polygon zone</p> Source code in <code>supervision/detection/tools/polygon_zone.py</code> <pre><code>class PolygonZone:\n    \"\"\"\n    A class for defining a polygon-shaped zone within a frame for detecting objects.\n\n    Attributes:\n        polygon (np.ndarray): A polygon represented by a numpy array of shape\n            `(N, 2)`, containing the `x`, `y` coordinates of the points.\n        frame_resolution_wh (Tuple[int, int]): The frame resolution (width, height)\n        triggering_position (Position): The position within the bounding\n            box that triggers the zone (default: Position.BOTTOM_CENTER)\n        current_count (int): The current count of detected objects within the zone\n        mask (np.ndarray): The 2D bool mask for the polygon zone\n    \"\"\"\n\n    def __init__(\n        self,\n        polygon: np.ndarray,\n        frame_resolution_wh: Tuple[int, int],\n        triggering_position: Position = Position.BOTTOM_CENTER,\n    ):\n        self.polygon = polygon.astype(int)\n        self.frame_resolution_wh = frame_resolution_wh\n        self.triggering_position = triggering_position\n        self.current_count = 0\n\n        width, height = frame_resolution_wh\n        self.mask = polygon_to_mask(\n            polygon=polygon, resolution_wh=(width + 1, height + 1)\n        )\n\n    def trigger(self, detections: Detections) -&gt; np.ndarray:\n        \"\"\"\n        Determines if the detections are within the polygon zone.\n\n        Parameters:\n            detections (Detections): The detections\n                to be checked against the polygon zone\n\n        Returns:\n            np.ndarray: A boolean numpy array indicating\n                if each detection is within the polygon zone\n        \"\"\"\n\n        clipped_xyxy = clip_boxes(\n            xyxy=detections.xyxy, resolution_wh=self.frame_resolution_wh\n        )\n        clipped_detections = replace(detections, xyxy=clipped_xyxy)\n        clipped_anchors = np.ceil(\n            clipped_detections.get_anchors_coordinates(anchor=self.triggering_position)\n        ).astype(int)\n        is_in_zone = self.mask[clipped_anchors[:, 1], clipped_anchors[:, 0]]\n        self.current_count = int(np.sum(is_in_zone))\n        return is_in_zone.astype(bool)\n</code></pre>"},{"location":"detection/tools/polygon_zone/#supervision.detection.tools.polygon_zone.PolygonZone.trigger","title":"<code>trigger(detections)</code>","text":"<p>Determines if the detections are within the polygon zone.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The detections to be checked against the polygon zone</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A boolean numpy array indicating if each detection is within the polygon zone</p> Source code in <code>supervision/detection/tools/polygon_zone.py</code> <pre><code>def trigger(self, detections: Detections) -&gt; np.ndarray:\n    \"\"\"\n    Determines if the detections are within the polygon zone.\n\n    Parameters:\n        detections (Detections): The detections\n            to be checked against the polygon zone\n\n    Returns:\n        np.ndarray: A boolean numpy array indicating\n            if each detection is within the polygon zone\n    \"\"\"\n\n    clipped_xyxy = clip_boxes(\n        xyxy=detections.xyxy, resolution_wh=self.frame_resolution_wh\n    )\n    clipped_detections = replace(detections, xyxy=clipped_xyxy)\n    clipped_anchors = np.ceil(\n        clipped_detections.get_anchors_coordinates(anchor=self.triggering_position)\n    ).astype(int)\n    is_in_zone = self.mask[clipped_anchors[:, 1], clipped_anchors[:, 0]]\n    self.current_count = int(np.sum(is_in_zone))\n    return is_in_zone.astype(bool)\n</code></pre>"},{"location":"detection/tools/polygon_zone/#polygonzoneannotator","title":"PolygonZoneAnnotator","text":"<p>A class for annotating a polygon-shaped zone within a     frame with a count of detected objects.</p> <p>Attributes:</p> Name Type Description <code>zone</code> <code>PolygonZone</code> <p>The polygon zone to be annotated</p> <code>color</code> <code>Color</code> <p>The color to draw the polygon lines</p> <code>thickness</code> <code>int</code> <p>The thickness of the polygon lines, default is 2</p> <code>text_color</code> <code>Color</code> <p>The color of the text on the polygon, default is black</p> <code>text_scale</code> <code>float</code> <p>The scale of the text on the polygon, default is 0.5</p> <code>text_thickness</code> <code>int</code> <p>The thickness of the text on the polygon, default is 1</p> <code>text_padding</code> <code>int</code> <p>The padding around the text on the polygon, default is 10</p> <code>font</code> <code>int</code> <p>The font type for the text on the polygon, default is cv2.FONT_HERSHEY_SIMPLEX</p> <code>center</code> <code>Tuple[int, int]</code> <p>The center of the polygon for text placement</p> Source code in <code>supervision/detection/tools/polygon_zone.py</code> <pre><code>class PolygonZoneAnnotator:\n    \"\"\"\n    A class for annotating a polygon-shaped zone within a\n        frame with a count of detected objects.\n\n    Attributes:\n        zone (PolygonZone): The polygon zone to be annotated\n        color (Color): The color to draw the polygon lines\n        thickness (int): The thickness of the polygon lines, default is 2\n        text_color (Color): The color of the text on the polygon, default is black\n        text_scale (float): The scale of the text on the polygon, default is 0.5\n        text_thickness (int): The thickness of the text on the polygon, default is 1\n        text_padding (int): The padding around the text on the polygon, default is 10\n        font (int): The font type for the text on the polygon,\n            default is cv2.FONT_HERSHEY_SIMPLEX\n        center (Tuple[int, int]): The center of the polygon for text placement\n    \"\"\"\n\n    def __init__(\n        self,\n        zone: PolygonZone,\n        color: Color,\n        thickness: int = 2,\n        text_color: Color = Color.black(),\n        text_scale: float = 0.5,\n        text_thickness: int = 1,\n        text_padding: int = 10,\n    ):\n        self.zone = zone\n        self.color = color\n        self.thickness = thickness\n        self.text_color = text_color\n        self.text_scale = text_scale\n        self.text_thickness = text_thickness\n        self.text_padding = text_padding\n        self.font = cv2.FONT_HERSHEY_SIMPLEX\n        self.center = get_polygon_center(polygon=zone.polygon)\n\n    def annotate(self, scene: np.ndarray, label: Optional[str] = None) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the polygon zone within a frame with a count of detected objects.\n\n        Parameters:\n            scene (np.ndarray): The image on which the polygon zone will be annotated\n            label (Optional[str]): An optional label for the count of detected objects\n                within the polygon zone (default: None)\n\n        Returns:\n            np.ndarray: The image with the polygon zone and count of detected objects\n        \"\"\"\n        annotated_frame = draw_polygon(\n            scene=scene,\n            polygon=self.zone.polygon,\n            color=self.color,\n            thickness=self.thickness,\n        )\n\n        annotated_frame = draw_text(\n            scene=annotated_frame,\n            text=str(self.zone.current_count) if label is None else label,\n            text_anchor=self.center,\n            background_color=self.color,\n            text_color=self.text_color,\n            text_scale=self.text_scale,\n            text_thickness=self.text_thickness,\n            text_padding=self.text_padding,\n            text_font=self.font,\n        )\n\n        return annotated_frame\n</code></pre>"},{"location":"detection/tools/polygon_zone/#supervision.detection.tools.polygon_zone.PolygonZoneAnnotator.annotate","title":"<code>annotate(scene, label=None)</code>","text":"<p>Annotates the polygon zone within a frame with a count of detected objects.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image on which the polygon zone will be annotated</p> required <code>label</code> <code>Optional[str]</code> <p>An optional label for the count of detected objects within the polygon zone (default: None)</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The image with the polygon zone and count of detected objects</p> Source code in <code>supervision/detection/tools/polygon_zone.py</code> <pre><code>def annotate(self, scene: np.ndarray, label: Optional[str] = None) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the polygon zone within a frame with a count of detected objects.\n\n    Parameters:\n        scene (np.ndarray): The image on which the polygon zone will be annotated\n        label (Optional[str]): An optional label for the count of detected objects\n            within the polygon zone (default: None)\n\n    Returns:\n        np.ndarray: The image with the polygon zone and count of detected objects\n    \"\"\"\n    annotated_frame = draw_polygon(\n        scene=scene,\n        polygon=self.zone.polygon,\n        color=self.color,\n        thickness=self.thickness,\n    )\n\n    annotated_frame = draw_text(\n        scene=annotated_frame,\n        text=str(self.zone.current_count) if label is None else label,\n        text_anchor=self.center,\n        background_color=self.color,\n        text_color=self.text_color,\n        text_scale=self.text_scale,\n        text_thickness=self.text_thickness,\n        text_padding=self.text_padding,\n        text_font=self.font,\n    )\n\n    return annotated_frame\n</code></pre>"},{"location":"draw/color/","title":"Color","text":""},{"location":"draw/color/#color","title":"Color","text":"<p>Represents a color in RGB format.</p> <p>Attributes:</p> Name Type Description <code>r</code> <code>int</code> <p>Red channel.</p> <code>g</code> <code>int</code> <p>Green channel.</p> <code>b</code> <code>int</code> <p>Blue channel.</p> Source code in <code>supervision/draw/color.py</code> <pre><code>@dataclass\nclass Color:\n    \"\"\"\n    Represents a color in RGB format.\n\n    Attributes:\n        r (int): Red channel.\n        g (int): Green channel.\n        b (int): Blue channel.\n    \"\"\"\n\n    r: int\n    g: int\n    b: int\n\n    @classmethod\n    def from_hex(cls, color_hex: str) -&gt; Color:\n        \"\"\"\n        Create a Color instance from a hex string.\n\n        Args:\n            color_hex (str): Hex string of the color.\n\n        Returns:\n            Color: Instance representing the color.\n\n        Example:\n            ```\n            &gt;&gt;&gt; Color.from_hex('#ff00ff')\n            Color(r=255, g=0, b=255)\n            ```\n        \"\"\"\n        _validate_color_hex(color_hex)\n        color_hex = color_hex.lstrip(\"#\")\n        if len(color_hex) == 3:\n            color_hex = \"\".join(c * 2 for c in color_hex)\n        r, g, b = (int(color_hex[i : i + 2], 16) for i in range(0, 6, 2))\n        return cls(r, g, b)\n\n    def as_hex(self) -&gt; str:\n        \"\"\"\n        Converts the Color instance to a hex string.\n\n        Returns:\n            str: The hexadecimal color string.\n\n        Example:\n            ```\n            &gt;&gt;&gt; Color(r=255, g=0, b=255).as_hex()\n            '#ff00ff'\n            ```\n        \"\"\"\n        return f\"#{self.r:02x}{self.g:02x}{self.b:02x}\"\n\n    def as_rgb(self) -&gt; Tuple[int, int, int]:\n        \"\"\"\n        Returns the color as an RGB tuple.\n\n        Returns:\n            Tuple[int, int, int]: RGB tuple.\n\n        Example:\n            ```\n            &gt;&gt;&gt; color.as_rgb()\n            (255, 0, 255)\n            ```\n        \"\"\"\n        return self.r, self.g, self.b\n\n    def as_bgr(self) -&gt; Tuple[int, int, int]:\n        \"\"\"\n        Returns the color as a BGR tuple.\n\n        Returns:\n            Tuple[int, int, int]: BGR tuple.\n\n        Example:\n            ```\n            &gt;&gt;&gt; color.as_bgr()\n            (255, 0, 255)\n            ```\n        \"\"\"\n        return self.b, self.g, self.r\n\n    @classmethod\n    def white(cls) -&gt; Color:\n        return Color.from_hex(color_hex=\"#ffffff\")\n\n    @classmethod\n    def black(cls) -&gt; Color:\n        return Color.from_hex(color_hex=\"#000000\")\n\n    @classmethod\n    def red(cls) -&gt; Color:\n        return Color.from_hex(color_hex=\"#ff0000\")\n\n    @classmethod\n    def green(cls) -&gt; Color:\n        return Color.from_hex(color_hex=\"#00ff00\")\n\n    @classmethod\n    def blue(cls) -&gt; Color:\n        return Color.from_hex(color_hex=\"#0000ff\")\n</code></pre>"},{"location":"draw/color/#supervision.draw.color.Color.as_bgr","title":"<code>as_bgr()</code>","text":"<p>Returns the color as a BGR tuple.</p> <p>Returns:</p> Type Description <code>Tuple[int, int, int]</code> <p>Tuple[int, int, int]: BGR tuple.</p> Example <pre><code>&gt;&gt;&gt; color.as_bgr()\n(255, 0, 255)\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>def as_bgr(self) -&gt; Tuple[int, int, int]:\n    \"\"\"\n    Returns the color as a BGR tuple.\n\n    Returns:\n        Tuple[int, int, int]: BGR tuple.\n\n    Example:\n        ```\n        &gt;&gt;&gt; color.as_bgr()\n        (255, 0, 255)\n        ```\n    \"\"\"\n    return self.b, self.g, self.r\n</code></pre>"},{"location":"draw/color/#supervision.draw.color.Color.as_hex","title":"<code>as_hex()</code>","text":"<p>Converts the Color instance to a hex string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The hexadecimal color string.</p> Example <pre><code>&gt;&gt;&gt; Color(r=255, g=0, b=255).as_hex()\n'#ff00ff'\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>def as_hex(self) -&gt; str:\n    \"\"\"\n    Converts the Color instance to a hex string.\n\n    Returns:\n        str: The hexadecimal color string.\n\n    Example:\n        ```\n        &gt;&gt;&gt; Color(r=255, g=0, b=255).as_hex()\n        '#ff00ff'\n        ```\n    \"\"\"\n    return f\"#{self.r:02x}{self.g:02x}{self.b:02x}\"\n</code></pre>"},{"location":"draw/color/#supervision.draw.color.Color.as_rgb","title":"<code>as_rgb()</code>","text":"<p>Returns the color as an RGB tuple.</p> <p>Returns:</p> Type Description <code>Tuple[int, int, int]</code> <p>Tuple[int, int, int]: RGB tuple.</p> Example <pre><code>&gt;&gt;&gt; color.as_rgb()\n(255, 0, 255)\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>def as_rgb(self) -&gt; Tuple[int, int, int]:\n    \"\"\"\n    Returns the color as an RGB tuple.\n\n    Returns:\n        Tuple[int, int, int]: RGB tuple.\n\n    Example:\n        ```\n        &gt;&gt;&gt; color.as_rgb()\n        (255, 0, 255)\n        ```\n    \"\"\"\n    return self.r, self.g, self.b\n</code></pre>"},{"location":"draw/color/#supervision.draw.color.Color.from_hex","title":"<code>from_hex(color_hex)</code>  <code>classmethod</code>","text":"<p>Create a Color instance from a hex string.</p> <p>Parameters:</p> Name Type Description Default <code>color_hex</code> <code>str</code> <p>Hex string of the color.</p> required <p>Returns:</p> Name Type Description <code>Color</code> <code>Color</code> <p>Instance representing the color.</p> Example <pre><code>&gt;&gt;&gt; Color.from_hex('#ff00ff')\nColor(r=255, g=0, b=255)\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>@classmethod\ndef from_hex(cls, color_hex: str) -&gt; Color:\n    \"\"\"\n    Create a Color instance from a hex string.\n\n    Args:\n        color_hex (str): Hex string of the color.\n\n    Returns:\n        Color: Instance representing the color.\n\n    Example:\n        ```\n        &gt;&gt;&gt; Color.from_hex('#ff00ff')\n        Color(r=255, g=0, b=255)\n        ```\n    \"\"\"\n    _validate_color_hex(color_hex)\n    color_hex = color_hex.lstrip(\"#\")\n    if len(color_hex) == 3:\n        color_hex = \"\".join(c * 2 for c in color_hex)\n    r, g, b = (int(color_hex[i : i + 2], 16) for i in range(0, 6, 2))\n    return cls(r, g, b)\n</code></pre>"},{"location":"draw/color/#colorpalette","title":"ColorPalette","text":"Source code in <code>supervision/draw/color.py</code> <pre><code>@dataclass\nclass ColorPalette:\n    colors: List[Color]\n\n    @classmethod\n    def default(cls) -&gt; ColorPalette:\n        \"\"\"\n        Returns a default color palette.\n\n        Returns:\n            ColorPalette: A ColorPalette instance with default colors.\n\n        Example:\n            ```\n            &gt;&gt;&gt; ColorPalette.default()\n            ColorPalette(colors=[Color(r=255, g=0, b=0), Color(r=0, g=255, b=0), ...])\n            ```\n        \"\"\"\n        return ColorPalette.from_hex(color_hex_list=DEFAULT_COLOR_PALETTE)\n\n    @classmethod\n    def from_hex(cls, color_hex_list: List[str]) -&gt; ColorPalette:\n        \"\"\"\n        Create a ColorPalette instance from a list of hex strings.\n\n        Args:\n            color_hex_list (List[str]): List of color hex strings.\n\n        Returns:\n            ColorPalette: A ColorPalette instance.\n\n        Example:\n            ```\n            &gt;&gt;&gt; ColorPalette.from_hex(['#ff0000', '#00ff00', '#0000ff'])\n            ColorPalette(colors=[Color(r=255, g=0, b=0), Color(r=0, g=255, b=0), ...])\n            ```\n        \"\"\"\n        colors = [Color.from_hex(color_hex) for color_hex in color_hex_list]\n        return cls(colors)\n\n    def by_idx(self, idx: int) -&gt; Color:\n        \"\"\"\n        Return the color at a given index in the palette.\n\n        Args:\n            idx (int): Index of the color in the palette.\n\n        Returns:\n            Color: Color at the given index.\n\n        Example:\n            ```\n            &gt;&gt;&gt; color_palette.by_idx(1)\n            Color(r=0, g=255, b=0)\n            ```\n        \"\"\"\n        if idx &lt; 0:\n            raise ValueError(\"idx argument should not be negative\")\n        idx = idx % len(self.colors)\n        return self.colors[idx]\n</code></pre>"},{"location":"draw/color/#supervision.draw.color.ColorPalette.by_idx","title":"<code>by_idx(idx)</code>","text":"<p>Return the color at a given index in the palette.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the color in the palette.</p> required <p>Returns:</p> Name Type Description <code>Color</code> <code>Color</code> <p>Color at the given index.</p> Example <pre><code>&gt;&gt;&gt; color_palette.by_idx(1)\nColor(r=0, g=255, b=0)\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>def by_idx(self, idx: int) -&gt; Color:\n    \"\"\"\n    Return the color at a given index in the palette.\n\n    Args:\n        idx (int): Index of the color in the palette.\n\n    Returns:\n        Color: Color at the given index.\n\n    Example:\n        ```\n        &gt;&gt;&gt; color_palette.by_idx(1)\n        Color(r=0, g=255, b=0)\n        ```\n    \"\"\"\n    if idx &lt; 0:\n        raise ValueError(\"idx argument should not be negative\")\n    idx = idx % len(self.colors)\n    return self.colors[idx]\n</code></pre>"},{"location":"draw/color/#supervision.draw.color.ColorPalette.default","title":"<code>default()</code>  <code>classmethod</code>","text":"<p>Returns a default color palette.</p> <p>Returns:</p> Name Type Description <code>ColorPalette</code> <code>ColorPalette</code> <p>A ColorPalette instance with default colors.</p> Example <pre><code>&gt;&gt;&gt; ColorPalette.default()\nColorPalette(colors=[Color(r=255, g=0, b=0), Color(r=0, g=255, b=0), ...])\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>@classmethod\ndef default(cls) -&gt; ColorPalette:\n    \"\"\"\n    Returns a default color palette.\n\n    Returns:\n        ColorPalette: A ColorPalette instance with default colors.\n\n    Example:\n        ```\n        &gt;&gt;&gt; ColorPalette.default()\n        ColorPalette(colors=[Color(r=255, g=0, b=0), Color(r=0, g=255, b=0), ...])\n        ```\n    \"\"\"\n    return ColorPalette.from_hex(color_hex_list=DEFAULT_COLOR_PALETTE)\n</code></pre>"},{"location":"draw/color/#supervision.draw.color.ColorPalette.from_hex","title":"<code>from_hex(color_hex_list)</code>  <code>classmethod</code>","text":"<p>Create a ColorPalette instance from a list of hex strings.</p> <p>Parameters:</p> Name Type Description Default <code>color_hex_list</code> <code>List[str]</code> <p>List of color hex strings.</p> required <p>Returns:</p> Name Type Description <code>ColorPalette</code> <code>ColorPalette</code> <p>A ColorPalette instance.</p> Example <pre><code>&gt;&gt;&gt; ColorPalette.from_hex(['#ff0000', '#00ff00', '#0000ff'])\nColorPalette(colors=[Color(r=255, g=0, b=0), Color(r=0, g=255, b=0), ...])\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>@classmethod\ndef from_hex(cls, color_hex_list: List[str]) -&gt; ColorPalette:\n    \"\"\"\n    Create a ColorPalette instance from a list of hex strings.\n\n    Args:\n        color_hex_list (List[str]): List of color hex strings.\n\n    Returns:\n        ColorPalette: A ColorPalette instance.\n\n    Example:\n        ```\n        &gt;&gt;&gt; ColorPalette.from_hex(['#ff0000', '#00ff00', '#0000ff'])\n        ColorPalette(colors=[Color(r=255, g=0, b=0), Color(r=0, g=255, b=0), ...])\n        ```\n    \"\"\"\n    colors = [Color.from_hex(color_hex) for color_hex in color_hex_list]\n    return cls(colors)\n</code></pre>"},{"location":"draw/utils/","title":"Utils","text":""},{"location":"draw/utils/#draw_line","title":"draw_line","text":"<p>Draws a line on a given scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The scene on which the line will be drawn</p> required <code>start</code> <code>Point</code> <p>The starting point of the line</p> required <code>end</code> <code>Point</code> <p>The end point of the line</p> required <code>color</code> <code>Color</code> <p>The color of the line</p> required <code>thickness</code> <code>int</code> <p>The thickness of the line</p> <code>2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The scene with the line drawn on it</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_line(\n    scene: np.ndarray, start: Point, end: Point, color: Color, thickness: int = 2\n) -&gt; np.ndarray:\n    \"\"\"\n    Draws a line on a given scene.\n\n    Parameters:\n        scene (np.ndarray): The scene on which the line will be drawn\n        start (Point): The starting point of the line\n        end (Point): The end point of the line\n        color (Color): The color of the line\n        thickness (int): The thickness of the line\n\n    Returns:\n        np.ndarray: The scene with the line drawn on it\n    \"\"\"\n    cv2.line(\n        scene,\n        start.as_xy_int_tuple(),\n        end.as_xy_int_tuple(),\n        color.as_bgr(),\n        thickness=thickness,\n    )\n    return scene\n</code></pre>"},{"location":"draw/utils/#draw_rectangle","title":"draw_rectangle","text":"<p>Draws a rectangle on an image.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The scene on which the rectangle will be drawn</p> required <code>rect</code> <code>Rect</code> <p>The rectangle to be drawn</p> required <code>color</code> <code>Color</code> <p>The color of the rectangle</p> required <code>thickness</code> <code>int</code> <p>The thickness of the rectangle border</p> <code>2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The scene with the rectangle drawn on it</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_rectangle(\n    scene: np.ndarray, rect: Rect, color: Color, thickness: int = 2\n) -&gt; np.ndarray:\n    \"\"\"\n    Draws a rectangle on an image.\n\n    Parameters:\n        scene (np.ndarray): The scene on which the rectangle will be drawn\n        rect (Rect): The rectangle to be drawn\n        color (Color): The color of the rectangle\n        thickness (int): The thickness of the rectangle border\n\n    Returns:\n        np.ndarray: The scene with the rectangle drawn on it\n    \"\"\"\n    cv2.rectangle(\n        scene,\n        rect.top_left.as_xy_int_tuple(),\n        rect.bottom_right.as_xy_int_tuple(),\n        color.as_bgr(),\n        thickness=thickness,\n    )\n    return scene\n</code></pre>"},{"location":"draw/utils/#draw_filled_rectangle","title":"draw_filled_rectangle","text":"<p>Draws a filled rectangle on an image.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The scene on which the rectangle will be drawn</p> required <code>rect</code> <code>Rect</code> <p>The rectangle to be drawn</p> required <code>color</code> <code>Color</code> <p>The color of the rectangle</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The scene with the rectangle drawn on it</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_filled_rectangle(scene: np.ndarray, rect: Rect, color: Color) -&gt; np.ndarray:\n    \"\"\"\n    Draws a filled rectangle on an image.\n\n    Parameters:\n        scene (np.ndarray): The scene on which the rectangle will be drawn\n        rect (Rect): The rectangle to be drawn\n        color (Color): The color of the rectangle\n\n    Returns:\n        np.ndarray: The scene with the rectangle drawn on it\n    \"\"\"\n    cv2.rectangle(\n        scene,\n        rect.top_left.as_xy_int_tuple(),\n        rect.bottom_right.as_xy_int_tuple(),\n        color.as_bgr(),\n        -1,\n    )\n    return scene\n</code></pre>"},{"location":"draw/utils/#draw_polygon","title":"draw_polygon","text":"<p>Draw a polygon on a scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The scene to draw the polygon on.</p> required <code>polygon</code> <code>ndarray</code> <p>The polygon to be drawn, given as a list of vertices.</p> required <code>color</code> <code>Color</code> <p>The color of the polygon.</p> required <code>thickness</code> <code>int</code> <p>The thickness of the polygon lines, by default 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The scene with the polygon drawn on it.</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_polygon(\n    scene: np.ndarray, polygon: np.ndarray, color: Color, thickness: int = 2\n) -&gt; np.ndarray:\n    \"\"\"Draw a polygon on a scene.\n\n    Parameters:\n        scene (np.ndarray): The scene to draw the polygon on.\n        polygon (np.ndarray): The polygon to be drawn, given as a list of vertices.\n        color (Color): The color of the polygon.\n        thickness (int, optional): The thickness of the polygon lines, by default 2.\n\n    Returns:\n        np.ndarray: The scene with the polygon drawn on it.\n    \"\"\"\n    cv2.polylines(\n        scene, [polygon], isClosed=True, color=color.as_bgr(), thickness=thickness\n    )\n    return scene\n</code></pre>"},{"location":"draw/utils/#draw_text","title":"draw_text","text":"<p>Draw text with background on a scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>A 2-dimensional numpy ndarray representing an image or scene</p> required <code>text</code> <code>str</code> <p>The text to be drawn.</p> required <code>text_anchor</code> <code>Point</code> <p>The anchor point for the text, represented as a Point object with x and y attributes.</p> required <code>text_color</code> <code>Color</code> <p>The color of the text. Defaults to black.</p> <code>black()</code> <code>text_scale</code> <code>float</code> <p>The scale of the text. Defaults to 0.5.</p> <code>0.5</code> <code>text_thickness</code> <code>int</code> <p>The thickness of the text. Defaults to 1.</p> <code>1</code> <code>text_padding</code> <code>int</code> <p>The amount of padding to add around the text when drawing a rectangle in the background. Defaults to 10.</p> <code>10</code> <code>text_font</code> <code>int</code> <p>The font to use for the text. Defaults to cv2.FONT_HERSHEY_SIMPLEX.</p> <code>FONT_HERSHEY_SIMPLEX</code> <code>background_color</code> <code>Color</code> <p>The color of the background rectangle, if one is to be drawn. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The input scene with the text drawn on it.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; scene = np.zeros((100, 100, 3), dtype=np.uint8)\n&gt;&gt;&gt; text_anchor = Point(x=50, y=50)\n&gt;&gt;&gt; scene = draw_text(scene=scene, text=\"Hello, world!\",text_anchor=text_anchor)\n</code></pre> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_text(\n    scene: np.ndarray,\n    text: str,\n    text_anchor: Point,\n    text_color: Color = Color.black(),\n    text_scale: float = 0.5,\n    text_thickness: int = 1,\n    text_padding: int = 10,\n    text_font: int = cv2.FONT_HERSHEY_SIMPLEX,\n    background_color: Optional[Color] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Draw text with background on a scene.\n\n    Parameters:\n        scene (np.ndarray): A 2-dimensional numpy ndarray representing an image or scene\n        text (str): The text to be drawn.\n        text_anchor (Point): The anchor point for the text, represented as a\n            Point object with x and y attributes.\n        text_color (Color, optional): The color of the text. Defaults to black.\n        text_scale (float, optional): The scale of the text. Defaults to 0.5.\n        text_thickness (int, optional): The thickness of the text. Defaults to 1.\n        text_padding (int, optional): The amount of padding to add around the text\n            when drawing a rectangle in the background. Defaults to 10.\n        text_font (int, optional): The font to use for the text.\n            Defaults to cv2.FONT_HERSHEY_SIMPLEX.\n        background_color (Color, optional): The color of the background rectangle,\n            if one is to be drawn. Defaults to None.\n\n    Returns:\n        np.ndarray: The input scene with the text drawn on it.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; scene = np.zeros((100, 100, 3), dtype=np.uint8)\n        &gt;&gt;&gt; text_anchor = Point(x=50, y=50)\n        &gt;&gt;&gt; scene = draw_text(scene=scene, text=\"Hello, world!\",text_anchor=text_anchor)\n        ```\n    \"\"\"\n    text_width, text_height = cv2.getTextSize(\n        text=text,\n        fontFace=text_font,\n        fontScale=text_scale,\n        thickness=text_thickness,\n    )[0]\n    text_rect = Rect(\n        x=text_anchor.x - text_width // 2,\n        y=text_anchor.y - text_height // 2,\n        width=text_width,\n        height=text_height,\n    ).pad(text_padding)\n\n    if background_color is not None:\n        scene = draw_filled_rectangle(\n            scene=scene, rect=text_rect, color=background_color\n        )\n\n    cv2.putText(\n        img=scene,\n        text=text,\n        org=(text_anchor.x - text_width // 2, text_anchor.y + text_height // 2),\n        fontFace=text_font,\n        fontScale=text_scale,\n        color=text_color.as_bgr(),\n        thickness=text_thickness,\n        lineType=cv2.LINE_AA,\n    )\n    return scene\n</code></pre>"},{"location":"draw/utils/#draw_image","title":"draw_image","text":"<p>Draws an image onto a given scene with specified opacity and dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>Background image where the new image will be drawn.</p> required <code>image</code> <code>Union[str, ndarray]</code> <p>Image to draw.</p> required <code>opacity</code> <code>float</code> <p>Opacity of the image to be drawn.</p> required <code>rect</code> <code>Rect</code> <p>Rectangle specifying where to draw the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The updated scene.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the image path does not exist.</p> <code>ValueError</code> <p>For invalid opacity or rectangle dimensions.</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_image(\n    scene: np.ndarray, image: Union[str, np.ndarray], opacity: float, rect: Rect\n) -&gt; np.ndarray:\n    \"\"\"\n    Draws an image onto a given scene with specified opacity and dimensions.\n\n    Args:\n        scene (np.ndarray): Background image where the new image will be drawn.\n        image (Union[str, np.ndarray]): Image to draw.\n        opacity (float): Opacity of the image to be drawn.\n        rect (Rect): Rectangle specifying where to draw the image.\n\n    Returns:\n        np.ndarray: The updated scene.\n\n    Raises:\n        FileNotFoundError: If the image path does not exist.\n        ValueError: For invalid opacity or rectangle dimensions.\n    \"\"\"\n\n    # Validate and load image\n    if isinstance(image, str):\n        if not os.path.exists(image):\n            raise FileNotFoundError(f\"Image path ('{image}') does not exist.\")\n        image = cv2.imread(image, cv2.IMREAD_UNCHANGED)\n\n    # Validate opacity\n    if not 0.0 &lt;= opacity &lt;= 1.0:\n        raise ValueError(\"Opacity must be between 0.0 and 1.0.\")\n\n    # Validate rectangle dimensions\n    if (\n        rect.x &lt; 0\n        or rect.y &lt; 0\n        or rect.x + rect.width &gt; scene.shape[1]\n        or rect.y + rect.height &gt; scene.shape[0]\n    ):\n        raise ValueError(\"Invalid rectangle dimensions.\")\n\n    # Resize and isolate alpha channel\n    image = cv2.resize(image, (rect.width, rect.height))\n    alpha_channel = (\n        image[:, :, 3]\n        if image.shape[2] == 4\n        else np.ones((rect.height, rect.width), dtype=image.dtype) * 255\n    )\n    alpha_scaled = cv2.convertScaleAbs(alpha_channel * opacity)\n\n    # Perform blending\n    scene_roi = scene[rect.y : rect.y + rect.height, rect.x : rect.x + rect.width]\n    alpha_float = alpha_scaled.astype(np.float32) / 255.0\n    blended_roi = cv2.convertScaleAbs(\n        (1 - alpha_float[..., np.newaxis]) * scene_roi\n        + alpha_float[..., np.newaxis] * image[:, :, :3]\n    )\n\n    # Update the scene\n    scene[rect.y : rect.y + rect.height, rect.x : rect.x + rect.width] = blended_roi\n\n    return scene\n</code></pre>"},{"location":"draw/utils/#calculate_dynamic_font_scale","title":"calculate_dynamic_font_scale","text":"<p>Calculate a dynamic font scale based on the resolution of an image.</p> <p>Parameters:</p> Name Type Description Default <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>A tuple representing the width and height    of the image.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The calculated font scale factor.</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def calculate_dynamic_text_scale(resolution_wh: Tuple[int, int]) -&gt; float:\n    \"\"\"\n    Calculate a dynamic font scale based on the resolution of an image.\n\n    Parameters:\n         resolution_wh (Tuple[int, int]): A tuple representing the width and height\n                 of the image.\n\n    Returns:\n         float: The calculated font scale factor.\n    \"\"\"\n    return min(resolution_wh) * 1e-3\n</code></pre>"},{"location":"draw/utils/#calculate_dynamic_line_thickness","title":"calculate_dynamic_line_thickness","text":"<p>Calculate a dynamic line thickness based on the resolution of an image.</p> <p>Parameters:</p> Name Type Description Default <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>A tuple representing the width and height     of the image.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The calculated line thickness in pixels.</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def calculate_dynamic_line_thickness(resolution_wh: Tuple[int, int]) -&gt; int:\n    \"\"\"\n    Calculate a dynamic line thickness based on the resolution of an image.\n\n    Parameters:\n        resolution_wh (Tuple[int, int]): A tuple representing the width and height\n                of the image.\n\n    Returns:\n        int: The calculated line thickness in pixels.\n    \"\"\"\n    min_dimension = min(resolution_wh)\n    if min_dimension &lt; 480:\n        return 2\n    if min_dimension &lt; 720:\n        return 2\n    if min_dimension &lt; 1080:\n        return 2\n    if min_dimension &lt; 2160:\n        return 4\n    else:\n        return 4\n</code></pre>"},{"location":"geometry/core/","title":"Core","text":""},{"location":"geometry/core/#position","title":"Position","text":"<p>             Bases: <code>Enum</code></p> <p>Enum representing the position of an anchor point.</p> Source code in <code>supervision/geometry/core.py</code> <pre><code>class Position(Enum):\n    \"\"\"\n    Enum representing the position of an anchor point.\n    \"\"\"\n\n    CENTER = \"CENTER\"\n    CENTER_LEFT = \"CENTER_LEFT\"\n    CENTER_RIGHT = \"CENTER_RIGHT\"\n    TOP_CENTER = \"TOP_CENTER\"\n    TOP_LEFT = \"TOP_LEFT\"\n    TOP_RIGHT = \"TOP_RIGHT\"\n    BOTTOM_LEFT = \"BOTTOM_LEFT\"\n    BOTTOM_CENTER = \"BOTTOM_CENTER\"\n    BOTTOM_RIGHT = \"BOTTOM_RIGHT\"\n    CENTER_OF_MASS = \"CENTER_OF_MASS\"\n\n    @classmethod\n    def list(cls):\n        return list(map(lambda c: c.value, cls))\n</code></pre>"},{"location":"how_to/detect_and_annotate/","title":"Detect and Annotate","text":"<p>With Supervision, you can easily annotate predictions obtained from a variety of object detection and segmentation models. This document outlines how to run inference using the Ultralytics YOLOv8 model, load these predictions into Supervision, and annotate the image.</p>"},{"location":"how_to/detect_and_annotate/#run-inference","title":"Run Inference","text":"<p>First, you'll need to obtain predictions from your object detection or segmentation model. <pre><code>import cv2\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nimage = cv2.imread(\"image.jpg\")\nresults = model(image)[0]\n</code></pre></p>"},{"location":"how_to/detect_and_annotate/#load-predictions-into-supervision","title":"Load Predictions into Supervision","text":"<p>Now that we have predictions from a model, we can load them into Supervision. We can do so using the <code>sv.Detections.from_ultralytics</code> method, which accepts model results from both detection and segmentation models.</p> <pre><code>import cv2\nfrom ultralytics import YOLO\nimport supervision as sv\n\nmodel = YOLO(\"yolov8n.pt\")\nimage = cv2.imread(\"image.jpg\")\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n</code></pre> <p>You can conveniently load predictions from other computer vision frameworks and libraries using:</p> <ul> <li><code>from_deepsparse</code> (Deepsparse)</li> <li><code>from_detectron2</code> (Detectron2)</li> <li><code>from_mmdetection</code> (MMDetection)</li> <li><code>from_roboflow</code> (Roboflow Inference)</li> <li><code>from_sam</code> (Segment Anything Model)</li> <li><code>from_transformers</code> (HuggingFace Transformers)</li> <li><code>from_yolo_nas</code> (YOLO-NAS)</li> </ul>"},{"location":"how_to/detect_and_annotate/#annotate-image","title":"Annotate Image","text":"<p>Finally, we can annotate the image with the predictions. Since we are working with an object detection model, we will use the <code>sv.BoundingBoxAnnotator</code> and <code>sv.LabelAnnotator</code> classes. If you are running the segmentation model <code>sv.MaskAnnotator</code> is a drop-in replacement for <code>sv.BoundingBoxAnnotator</code> that will allow you to draw masks instead of boxes.</p> <pre><code>import cv2\nfrom ultralytics import YOLO\nimport supervision as sv\n\nmodel = YOLO(\"yolov8n.pt\")\nimage = cv2.imread(\"image.jpg\")\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n\nbounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    results.names[class_id]\n    for class_id\n    in detections.class_id\n]\n\nannotated_image = bounding_box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n</code></pre> <p></p>"},{"location":"how_to/detect_and_annotate/#display-annotated-image","title":"Display Annotated Image","text":"<p>To display the annotated image in Jupyter Notebook or Google Colab, use the <code>sv.plot_image</code> function.</p> <pre><code>sv.plot_image(annotated_image)\n</code></pre>"},{"location":"how_to/evaluate_model/","title":"Evaluate Model","text":"<p>\ud83d\udea7 Page under construction.</p>"},{"location":"how_to/filter_detections/","title":"Filter Detections","text":"<p>The advanced filtering capabilities of the <code>Detections</code> class offer users a versatile and efficient way to narrow down and refine object detections. This section outlines various filtering methods, including filtering by specific class or a set of classes, confidence, object area, bounding box area, relative area, box dimensions, and designated zones. Each method is demonstrated with concise code examples to provide users with a clear understanding of how to implement the filters in their applications.</p>"},{"location":"how_to/filter_detections/#by-specific-class","title":"by specific class","text":"<p>Allows you to select detections that belong only to one selected class.</p> AfterBefore <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.class_id == 0]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.class_id == 0]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-set-of-classes","title":"by set of classes","text":"<p>Allows you to select detections that belong only to selected set of classes.</p> AfterBefore <pre><code>import numpy as np\nimport supervision as sv\n\nselected_classes = [0, 2, 3]\ndetections = sv.Detections(...)\ndetections = detections[np.isin(detections.class_id, selected_classes)]\n</code></pre> <p></p> <pre><code>import numpy as np\nimport supervision as sv\n\nclass_id = [0, 2, 3]\ndetections = sv.Detections(...)\ndetections = detections[np.isin(detections.class_id, class_id)]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-confidence","title":"by confidence","text":"<p>Allows you to select detections with specific confidence value, for example higher than selected threshold.</p> AfterBefore <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.confidence &gt; 0.5]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.confidence &gt; 0.5]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-area","title":"by area","text":"<p>Allows you to select detections based on their size. We define the area as the number of pixels occupied by the detection in the image. In the example below, we have sifted out the detections that are too small.</p> AfterBefore <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.area &gt; 1000]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.area &gt; 1000]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-relative-area","title":"by relative area","text":"<p>Allows you to select detections based on their size in relation to the size of whole image. Sometimes the concept of detection size changes depending on the image. Detection occupying 10000 square px can be large on a 1280x720 image but small on a 3840x2160 image. In such cases, we can filter out detections based on the percentage of the image area occupied by them. In the example below, we remove too large detections.</p> AfterBefore <pre><code>import supervision as sv\n\nimage = ...\nheight, width, channels = image.shape\nimage_area = height * width\n\ndetections = sv.Detections(...)\ndetections = detections[(detections.area / image_area) &lt; 0.8]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\nimage = ...\nheight, width, channels = image.shape\nimage_area = height * width\n\ndetections = sv.Detections(...)\ndetections = detections[(detections.area / image_area) &lt; 0.8]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-box-dimensions","title":"by box dimensions","text":"<p>Allows you to select detections based on their dimensions. The size of the bounding box, as well as its coordinates, can be criteria for rejecting detection. Implementing such filtering requires a bit of custom code but is relatively simple and fast.</p> AfterBefore <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\nw = detections.xyxy[:, 2] - detections.xyxy[:, 0]\nh = detections.xyxy[:, 3] - detections.xyxy[:, 1]\ndetections = detections[(w &gt; 200) &amp; (h &gt; 200)]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\nw = detections.xyxy[:, 2] - detections.xyxy[:, 0]\nh = detections.xyxy[:, 3] - detections.xyxy[:, 1]\ndetections = detections[(w &gt; 200) &amp; (h &gt; 200)]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-polygonzone","title":"by <code>PolygonZone</code>","text":"<p>Allows you to use <code>Detections</code> in combination with <code>PolygonZone</code> to weed out bounding boxes that are in and out of the zone. In the example below you can see how to filter out all detections located in the lower part of the image.</p> AfterBefore <pre><code>import supervision as sv\n\nzone = sv.PolygonZone(...)\ndetections = sv.Detections(...)\nmask = zone.trigger(detections=detections)\ndetections = detections[mask]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\nzone = sv.PolygonZone(...)\ndetections = sv.Detections(...)\nmask = zone.trigger(detections=detections)\ndetections = detections[mask]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-mixed-conditions","title":"by mixed conditions","text":"<p><code>Detections</code>' greatest strength, however, is that you can build arbitrarily complex logical conditions by simply combining separate conditions using <code>&amp;</code> or <code>|</code>.</p> AfterBefore <pre><code>import supervision as sv\n\nzone = sv.PolygonZone(...)\ndetections = sv.Detections(...)\nmask = zone.trigger(detections=detections)\ndetections = detections[(detections.confidence &gt; 0.7) &amp; mask]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\nzone = sv.PolygonZone(...)\ndetections = sv.Detections(...)\nmask = zone.trigger(detections=detections)\ndetections = detections[mask]\n</code></pre> <p></p>"},{"location":"how_to/process_video/","title":"Process Video","text":"<p>\ud83d\udea7 Page under construction.</p>"},{"location":"how_to/track_objects/","title":"Track Objects","text":"<p>Utilize Supervision to elevate your video analysis capabilities by effortlessly tracking objects identified by various object detection and segmentation models. This guide will walk you through the process of running inference using the Ultralytics YOLOv8 model, subsequently tracking these objects, and annotating the video.</p> <p>To make it easier for you to follow our tutorial download the video we will use as an example. You can do this using <code>supervision[assets]</code> extension.</p> <pre><code>from supervision.assets import download_assets, VideoAssets\n\ndownload_assets(VideoAssets.PEOPLE_WALKING)\n</code></pre>"},{"location":"how_to/track_objects/#run-inference","title":"Run Inference","text":"<p>First, you'll need to obtain predictions from your object detection or segmentation model. In this tutorial, we are using the YOLOv8 model as an example. However, Supervision is versatile and compatible with various models. Check this link for guidance on how to plug in other models.</p> <p>We will define a <code>callback</code> function, which will process each frame of the video by obtaining model predictions and then annotating the frame based on these predictions. This <code>callback</code> function will be essential in the subsequent steps of the tutorial, as it will be modified to include tracking, labeling, and trace annotations.</p> <pre><code>import numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nbox_annotator = sv.BoundingBoxAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -&gt; np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    return box_annotator.annotate(frame.copy(), detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n</code></pre>"},{"location":"how_to/track_objects/#tracking","title":"Tracking","text":"<p>After running inference and obtaining predictions, the next step is to track the detected objects throughout the video. Utilizing Supervision\u2019s <code>sv.ByteTrack</code> functionality, each detected object is assigned a unique tracker ID, enabling the continuous following of the object's motion path across different frames.</p> <pre><code>import numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoundingBoxAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -&gt; np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    detections = tracker.update_with_detections(detections)\n    return box_annotator.annotate(frame.copy(), detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n</code></pre>"},{"location":"how_to/track_objects/#annotate-video-with-tracking-ids","title":"Annotate Video with Tracking IDs","text":"<p>Annotating the video with tracking IDs helps in distinguishing and following each object distinctly. With the <code>sv.LabelAnnotator</code> in Supervision, we can overlay the tracker IDs and class labels on the detected objects, offering a clear visual representation of each object's class and unique identifier.</p> <pre><code>import numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -&gt; np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    detections = tracker.update_with_detections(detections)\n\n    labels = [\n        f\"#{tracker_id} {results.names[class_id]}\"\n        for class_id, tracker_id\n        in zip(detections.class_id, detections.tracker_id)\n    ]\n\n    annotated_frame = box_annotator.annotate(\n        frame.copy(), detections=detections)\n    return label_annotator.annotate(\n        annotated_frame, detections=detections, labels=labels)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n</code></pre>"},{"location":"how_to/track_objects/#annotate-video-with-traces","title":"Annotate Video with Traces","text":"<p>Adding traces to the video involves overlaying the historical paths of the detected objects. This feature, powered by the <code>sv.TraceAnnotator</code>, allows for visualizing the trajectories of objects, helping in understanding the movement patterns and interactions between objects in the video.</p> <pre><code>import numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\ntrace_annotator = sv.TraceAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -&gt; np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    detections = tracker.update_with_detections(detections)\n\n    labels = [\n        f\"#{tracker_id} {results.names[class_id]}\"\n        for class_id, tracker_id\n        in zip(detections.class_id, detections.tracker_id)\n    ]\n\n    annotated_frame = box_annotator.annotate(\n        frame.copy(), detections=detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame, detections=detections, labels=labels)\n    return trace_annotator.annotate(\n        annotated_frame, detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n</code></pre> <p>This structured walkthrough should give a detailed pathway to annotate videos effectively using Supervision\u2019s various functionalities, including object tracking and trace annotations.</p>"},{"location":"metrics/detection/","title":"Object Detection","text":"<p>Warning</p> <p>Evaluation API is still fluid and may change. If you use Evaluation API in your project until further notice, freeze the <code>supervision</code> version in your <code>requirements.txt</code> or <code>setup.py</code>.</p>"},{"location":"metrics/detection/#confusionmatrix","title":"ConfusionMatrix","text":"<p>Confusion matrix for object detection tasks.</p> <p>Attributes:</p> Name Type Description <code>matrix</code> <code>ndarray</code> <p>An 2D <code>np.ndarray</code> of shape <code>(len(classes) + 1, len(classes) + 1)</code> containing the number of <code>TP</code>, <code>FP</code>, <code>FN</code> and <code>TN</code> for each class.</p> <code>classes</code> <code>List[str]</code> <p>Model class names.</p> <code>conf_threshold</code> <code>float</code> <p>Detection confidence threshold between <code>0</code> and <code>1</code>. Detections with lower confidence will be excluded from the matrix.</p> <code>iou_threshold</code> <code>float</code> <p>Detection IoU threshold between <code>0</code> and <code>1</code>. Detections with lower IoU will be classified as <code>FP</code>.</p> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@dataclass\nclass ConfusionMatrix:\n    \"\"\"\n    Confusion matrix for object detection tasks.\n\n    Attributes:\n        matrix (np.ndarray): An 2D `np.ndarray` of shape\n            `(len(classes) + 1, len(classes) + 1)`\n            containing the number of `TP`, `FP`, `FN` and `TN` for each class.\n        classes (List[str]): Model class names.\n        conf_threshold (float): Detection confidence threshold between `0` and `1`.\n            Detections with lower confidence will be excluded from the matrix.\n        iou_threshold (float): Detection IoU threshold between `0` and `1`.\n            Detections with lower IoU will be classified as `FP`.\n    \"\"\"\n\n    matrix: np.ndarray\n    classes: List[str]\n    conf_threshold: float\n    iou_threshold: float\n\n    @classmethod\n    def from_detections(\n        cls,\n        predictions: List[Detections],\n        targets: List[Detections],\n        classes: List[str],\n        conf_threshold: float = 0.3,\n        iou_threshold: float = 0.5,\n    ) -&gt; ConfusionMatrix:\n        \"\"\"\n        Calculate confusion matrix based on predicted and ground-truth detections.\n\n        Args:\n            targets (List[Detections]): Detections objects from ground-truth.\n            predictions (List[Detections]): Detections objects predicted by the model.\n            classes (List[str]): Model class names.\n            conf_threshold (float): Detection confidence threshold between `0` and `1`.\n                Detections with lower confidence will be excluded.\n            iou_threshold (float): Detection IoU threshold between `0` and `1`.\n                Detections with lower IoU will be classified as `FP`.\n\n        Returns:\n            ConfusionMatrix: New instance of ConfusionMatrix.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; targets = [\n            ...     sv.Detections(...),\n            ...     sv.Detections(...)\n            ... ]\n\n            &gt;&gt;&gt; predictions = [\n            ...     sv.Detections(...),\n            ...     sv.Detections(...)\n            ... ]\n\n            &gt;&gt;&gt; confusion_matrix = sv.ConfusionMatrix.from_detections(\n            ...     predictions=predictions,\n            ...     targets=target,\n            ...     classes=['person', ...]\n            ... )\n\n            &gt;&gt;&gt; confusion_matrix.matrix\n            array([\n                [0., 0., 0., 0.],\n                [0., 1., 0., 1.],\n                [0., 1., 1., 0.],\n                [1., 1., 0., 0.]\n            ])\n            ```\n        \"\"\"\n\n        prediction_tensors = []\n        target_tensors = []\n        for prediction, target in zip(predictions, targets):\n            prediction_tensors.append(\n                detections_to_tensor(prediction, with_confidence=True)\n            )\n            target_tensors.append(detections_to_tensor(target, with_confidence=False))\n        return cls.from_tensors(\n            predictions=prediction_tensors,\n            targets=target_tensors,\n            classes=classes,\n            conf_threshold=conf_threshold,\n            iou_threshold=iou_threshold,\n        )\n\n    @classmethod\n    def from_tensors(\n        cls,\n        predictions: List[np.ndarray],\n        targets: List[np.ndarray],\n        classes: List[str],\n        conf_threshold: float = 0.3,\n        iou_threshold: float = 0.5,\n    ) -&gt; ConfusionMatrix:\n        \"\"\"\n        Calculate confusion matrix based on predicted and ground-truth detections.\n\n        Args:\n            predictions (List[np.ndarray]): Each element of the list describes a single\n                image and has `shape = (M, 6)` where `M` is the number of detected\n                objects. Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class, conf)` format.\n            targets (List[np.ndarray]): Each element of the list describes a single\n                image and has `shape = (N, 5)` where `N` is the number of\n                ground-truth objects. Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class)` format.\n            classes (List[str]): Model class names.\n            conf_threshold (float): Detection confidence threshold between `0` and `1`.\n                Detections with lower confidence will be excluded.\n            iou_threshold (float): Detection iou  threshold between `0` and `1`.\n                Detections with lower iou will be classified as `FP`.\n\n        Returns:\n            ConfusionMatrix: New instance of ConfusionMatrix.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; targets = (\n            ...     [\n            ...         array(\n            ...             [\n            ...                 [0.0, 0.0, 3.0, 3.0, 1],\n            ...                 [2.0, 2.0, 5.0, 5.0, 1],\n            ...                 [6.0, 1.0, 8.0, 3.0, 2],\n            ...             ]\n            ...         ),\n            ...         array([1.0, 1.0, 2.0, 2.0, 2]),\n            ...     ]\n            ... )\n\n            &gt;&gt;&gt; predictions = [\n            ...     array(\n            ...         [\n            ...             [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n            ...             [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n            ...             [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n            ...             [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n            ...         ]\n            ...     ),\n            ...     array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n            ... ]\n\n            &gt;&gt;&gt; confusion_matrix = sv.ConfusionMatrix.from_tensors(\n            ...     predictions=predictions,\n            ...     targets=targets,\n            ...     classes=['person', ...]\n            ... )\n\n            &gt;&gt;&gt; confusion_matrix.matrix\n            array([\n                [0., 0., 0., 0.],\n                [0., 1., 0., 1.],\n                [0., 1., 1., 0.],\n                [1., 1., 0., 0.]\n            ])\n            ```\n        \"\"\"\n        validate_input_tensors(predictions, targets)\n\n        num_classes = len(classes)\n        matrix = np.zeros((num_classes + 1, num_classes + 1))\n        for true_batch, detection_batch in zip(targets, predictions):\n            matrix += cls.evaluate_detection_batch(\n                predictions=detection_batch,\n                targets=true_batch,\n                num_classes=num_classes,\n                conf_threshold=conf_threshold,\n                iou_threshold=iou_threshold,\n            )\n        return cls(\n            matrix=matrix,\n            classes=classes,\n            conf_threshold=conf_threshold,\n            iou_threshold=iou_threshold,\n        )\n\n    @staticmethod\n    def evaluate_detection_batch(\n        predictions: np.ndarray,\n        targets: np.ndarray,\n        num_classes: int,\n        conf_threshold: float,\n        iou_threshold: float,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Calculate confusion matrix for a batch of detections for a single image.\n\n        Args:\n            predictions (np.ndarray): Batch prediction. Describes a single image and\n                has `shape = (M, 6)` where `M` is the number of detected objects.\n                Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class, conf)` format.\n            targets (np.ndarray): Batch target labels. Describes a single image and\n                has `shape = (N, 5)` where `N` is the number of ground-truth objects.\n                Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class)` format.\n            num_classes (int): Number of classes.\n            conf_threshold (float): Detection confidence threshold between `0` and `1`.\n                Detections with lower confidence will be excluded.\n            iou_threshold (float): Detection iou  threshold between `0` and `1`.\n                Detections with lower iou will be classified as `FP`.\n\n        Returns:\n            np.ndarray: Confusion matrix based on a single image.\n        \"\"\"\n        result_matrix = np.zeros((num_classes + 1, num_classes + 1))\n\n        conf_idx = 5\n        confidence = predictions[:, conf_idx]\n        detection_batch_filtered = predictions[confidence &gt; conf_threshold]\n\n        class_id_idx = 4\n        true_classes = np.array(targets[:, class_id_idx], dtype=np.int16)\n        detection_classes = np.array(\n            detection_batch_filtered[:, class_id_idx], dtype=np.int16\n        )\n        true_boxes = targets[:, :class_id_idx]\n        detection_boxes = detection_batch_filtered[:, :class_id_idx]\n\n        iou_batch = box_iou_batch(\n            boxes_true=true_boxes, boxes_detection=detection_boxes\n        )\n        matched_idx = np.asarray(iou_batch &gt; iou_threshold).nonzero()\n\n        if matched_idx[0].shape[0]:\n            matches = np.stack(\n                (matched_idx[0], matched_idx[1], iou_batch[matched_idx]), axis=1\n            )\n            matches = ConfusionMatrix._drop_extra_matches(matches=matches)\n        else:\n            matches = np.zeros((0, 3))\n\n        matched_true_idx, matched_detection_idx, _ = matches.transpose().astype(\n            np.int16\n        )\n\n        for i, true_class_value in enumerate(true_classes):\n            j = matched_true_idx == i\n            if matches.shape[0] &gt; 0 and sum(j) == 1:\n                result_matrix[\n                    true_class_value, detection_classes[matched_detection_idx[j]]\n                ] += 1  # TP\n            else:\n                result_matrix[true_class_value, num_classes] += 1  # FN\n\n        for i, detection_class_value in enumerate(detection_classes):\n            if not any(matched_detection_idx == i):\n                result_matrix[num_classes, detection_class_value] += 1  # FP\n\n        return result_matrix\n\n    @staticmethod\n    def _drop_extra_matches(matches: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Deduplicate matches. If there are multiple matches for the same true or\n        predicted box, only the one with the highest IoU is kept.\n        \"\"\"\n        if matches.shape[0] &gt; 0:\n            matches = matches[matches[:, 2].argsort()[::-1]]\n            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n            matches = matches[matches[:, 2].argsort()[::-1]]\n            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n        return matches\n\n    @classmethod\n    def benchmark(\n        cls,\n        dataset: DetectionDataset,\n        callback: Callable[[np.ndarray], Detections],\n        conf_threshold: float = 0.3,\n        iou_threshold: float = 0.5,\n    ) -&gt; ConfusionMatrix:\n        \"\"\"\n        Calculate confusion matrix from dataset and callback function.\n\n        Args:\n            dataset (DetectionDataset): Object detection dataset used for evaluation.\n            callback (Callable[[np.ndarray], Detections]): Function that takes an image\n                as input and returns Detections object.\n            conf_threshold (float): Detection confidence threshold between `0` and `1`.\n                Detections with lower confidence will be excluded.\n            iou_threshold (float): Detection IoU threshold between `0` and `1`.\n                Detections with lower IoU will be classified as `FP`.\n\n        Returns:\n            ConfusionMatrix: New instance of ConfusionMatrix.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n            &gt;&gt;&gt; from ultralytics import YOLO\n\n            &gt;&gt;&gt; dataset = sv.DetectionDataset.from_yolo(...)\n\n            &gt;&gt;&gt; model = YOLO(...)\n            &gt;&gt;&gt; def callback(image: np.ndarray) -&gt; sv.Detections:\n            ...     result = model(image)[0]\n            ...     return sv.Detections.from_ultralytics(result)\n\n            &gt;&gt;&gt; confusion_matrix = sv.ConfusionMatrix.benchmark(\n            ...     dataset = dataset,\n            ...     callback = callback\n            ... )\n\n            &gt;&gt;&gt; confusion_matrix.matrix\n            array([\n                [0., 0., 0., 0.],\n                [0., 1., 0., 1.],\n                [0., 1., 1., 0.],\n                [1., 1., 0., 0.]\n            ])\n            ```\n        \"\"\"\n        predictions, targets = [], []\n        for img_name, img in dataset.images.items():\n            predictions_batch = callback(img)\n            predictions.append(predictions_batch)\n            targets_batch = dataset.annotations[img_name]\n            targets.append(targets_batch)\n        return cls.from_detections(\n            predictions=predictions,\n            targets=targets,\n            classes=dataset.classes,\n            conf_threshold=conf_threshold,\n            iou_threshold=iou_threshold,\n        )\n\n    def plot(\n        self,\n        save_path: Optional[str] = None,\n        title: Optional[str] = None,\n        classes: Optional[List[str]] = None,\n        normalize: bool = False,\n        fig_size: Tuple[int, int] = (12, 10),\n    ) -&gt; matplotlib.figure.Figure:\n        \"\"\"\n        Create confusion matrix plot and save it at selected location.\n\n        Args:\n            save_path (Optional[str]): Path to save the plot. If not provided,\n                plot will be displayed.\n            title (Optional[str]): Title of the plot.\n            classes (Optional[List[str]]): List of classes to be displayed on the plot.\n                If not provided, all classes will be displayed.\n            normalize (bool): If True, normalize the confusion matrix.\n            fig_size (Tuple[int, int]): Size of the plot.\n\n        Returns:\n            matplotlib.figure.Figure: Confusion matrix plot.\n        \"\"\"\n\n        array = self.matrix.copy()\n\n        if normalize:\n            eps = 1e-8\n            array = array / (array.sum(0).reshape(1, -1) + eps)\n\n        array[array &lt; 0.005] = np.nan\n\n        fig, ax = plt.subplots(figsize=fig_size, tight_layout=True, facecolor=\"white\")\n\n        class_names = classes if classes is not None else self.classes\n        use_labels_for_ticks = class_names is not None and (0 &lt; len(class_names) &lt; 99)\n        if use_labels_for_ticks:\n            x_tick_labels = class_names + [\"FN\"]\n            y_tick_labels = class_names + [\"FP\"]\n            num_ticks = len(x_tick_labels)\n        else:\n            x_tick_labels = None\n            y_tick_labels = None\n            num_ticks = len(array)\n        im = ax.imshow(array, cmap=\"Blues\")\n\n        cbar = ax.figure.colorbar(im, ax=ax)\n        cbar.mappable.set_clim(vmin=0, vmax=np.nanmax(array))\n\n        if x_tick_labels is None:\n            tick_interval = 2\n        else:\n            tick_interval = 1\n        ax.set_xticks(np.arange(0, num_ticks, tick_interval), labels=x_tick_labels)\n        ax.set_yticks(np.arange(0, num_ticks, tick_interval), labels=y_tick_labels)\n\n        plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"default\")\n\n        labelsize = 10 if num_ticks &lt; 50 else 8\n        ax.tick_params(axis=\"both\", which=\"both\", labelsize=labelsize)\n\n        if num_ticks &lt; 30:\n            for i in range(array.shape[0]):\n                for j in range(array.shape[1]):\n                    n_preds = array[i, j]\n                    if not np.isnan(n_preds):\n                        ax.text(\n                            j,\n                            i,\n                            f\"{n_preds:.2f}\" if normalize else f\"{n_preds:.0f}\",\n                            ha=\"center\",\n                            va=\"center\",\n                            color=\"black\"\n                            if n_preds &lt; 0.5 * np.nanmax(array)\n                            else \"white\",\n                        )\n\n        if title:\n            ax.set_title(title, fontsize=20)\n\n        ax.set_xlabel(\"Predicted\")\n        ax.set_ylabel(\"True\")\n        ax.set_facecolor(\"white\")\n        if save_path:\n            fig.savefig(\n                save_path, dpi=250, facecolor=fig.get_facecolor(), transparent=True\n            )\n        return fig\n</code></pre>"},{"location":"metrics/detection/#supervision.metrics.detection.ConfusionMatrix.benchmark","title":"<code>benchmark(dataset, callback, conf_threshold=0.3, iou_threshold=0.5)</code>  <code>classmethod</code>","text":"<p>Calculate confusion matrix from dataset and callback function.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DetectionDataset</code> <p>Object detection dataset used for evaluation.</p> required <code>callback</code> <code>Callable[[ndarray], Detections]</code> <p>Function that takes an image as input and returns Detections object.</p> required <code>conf_threshold</code> <code>float</code> <p>Detection confidence threshold between <code>0</code> and <code>1</code>. Detections with lower confidence will be excluded.</p> <code>0.3</code> <code>iou_threshold</code> <code>float</code> <p>Detection IoU threshold between <code>0</code> and <code>1</code>. Detections with lower IoU will be classified as <code>FP</code>.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>ConfusionMatrix</code> <code>ConfusionMatrix</code> <p>New instance of ConfusionMatrix.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; dataset = sv.DetectionDataset.from_yolo(...)\n\n&gt;&gt;&gt; model = YOLO(...)\n&gt;&gt;&gt; def callback(image: np.ndarray) -&gt; sv.Detections:\n...     result = model(image)[0]\n...     return sv.Detections.from_ultralytics(result)\n\n&gt;&gt;&gt; confusion_matrix = sv.ConfusionMatrix.benchmark(\n...     dataset = dataset,\n...     callback = callback\n... )\n\n&gt;&gt;&gt; confusion_matrix.matrix\narray([\n    [0., 0., 0., 0.],\n    [0., 1., 0., 1.],\n    [0., 1., 1., 0.],\n    [1., 1., 0., 0.]\n])\n</code></pre> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@classmethod\ndef benchmark(\n    cls,\n    dataset: DetectionDataset,\n    callback: Callable[[np.ndarray], Detections],\n    conf_threshold: float = 0.3,\n    iou_threshold: float = 0.5,\n) -&gt; ConfusionMatrix:\n    \"\"\"\n    Calculate confusion matrix from dataset and callback function.\n\n    Args:\n        dataset (DetectionDataset): Object detection dataset used for evaluation.\n        callback (Callable[[np.ndarray], Detections]): Function that takes an image\n            as input and returns Detections object.\n        conf_threshold (float): Detection confidence threshold between `0` and `1`.\n            Detections with lower confidence will be excluded.\n        iou_threshold (float): Detection IoU threshold between `0` and `1`.\n            Detections with lower IoU will be classified as `FP`.\n\n    Returns:\n        ConfusionMatrix: New instance of ConfusionMatrix.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n        &gt;&gt;&gt; from ultralytics import YOLO\n\n        &gt;&gt;&gt; dataset = sv.DetectionDataset.from_yolo(...)\n\n        &gt;&gt;&gt; model = YOLO(...)\n        &gt;&gt;&gt; def callback(image: np.ndarray) -&gt; sv.Detections:\n        ...     result = model(image)[0]\n        ...     return sv.Detections.from_ultralytics(result)\n\n        &gt;&gt;&gt; confusion_matrix = sv.ConfusionMatrix.benchmark(\n        ...     dataset = dataset,\n        ...     callback = callback\n        ... )\n\n        &gt;&gt;&gt; confusion_matrix.matrix\n        array([\n            [0., 0., 0., 0.],\n            [0., 1., 0., 1.],\n            [0., 1., 1., 0.],\n            [1., 1., 0., 0.]\n        ])\n        ```\n    \"\"\"\n    predictions, targets = [], []\n    for img_name, img in dataset.images.items():\n        predictions_batch = callback(img)\n        predictions.append(predictions_batch)\n        targets_batch = dataset.annotations[img_name]\n        targets.append(targets_batch)\n    return cls.from_detections(\n        predictions=predictions,\n        targets=targets,\n        classes=dataset.classes,\n        conf_threshold=conf_threshold,\n        iou_threshold=iou_threshold,\n    )\n</code></pre>"},{"location":"metrics/detection/#supervision.metrics.detection.ConfusionMatrix.evaluate_detection_batch","title":"<code>evaluate_detection_batch(predictions, targets, num_classes, conf_threshold, iou_threshold)</code>  <code>staticmethod</code>","text":"<p>Calculate confusion matrix for a batch of detections for a single image.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Batch prediction. Describes a single image and has <code>shape = (M, 6)</code> where <code>M</code> is the number of detected objects. Each row is expected to be in <code>(x_min, y_min, x_max, y_max, class, conf)</code> format.</p> required <code>targets</code> <code>ndarray</code> <p>Batch target labels. Describes a single image and has <code>shape = (N, 5)</code> where <code>N</code> is the number of ground-truth objects. Each row is expected to be in <code>(x_min, y_min, x_max, y_max, class)</code> format.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes.</p> required <code>conf_threshold</code> <code>float</code> <p>Detection confidence threshold between <code>0</code> and <code>1</code>. Detections with lower confidence will be excluded.</p> required <code>iou_threshold</code> <code>float</code> <p>Detection iou  threshold between <code>0</code> and <code>1</code>. Detections with lower iou will be classified as <code>FP</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Confusion matrix based on a single image.</p> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@staticmethod\ndef evaluate_detection_batch(\n    predictions: np.ndarray,\n    targets: np.ndarray,\n    num_classes: int,\n    conf_threshold: float,\n    iou_threshold: float,\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculate confusion matrix for a batch of detections for a single image.\n\n    Args:\n        predictions (np.ndarray): Batch prediction. Describes a single image and\n            has `shape = (M, 6)` where `M` is the number of detected objects.\n            Each row is expected to be in\n            `(x_min, y_min, x_max, y_max, class, conf)` format.\n        targets (np.ndarray): Batch target labels. Describes a single image and\n            has `shape = (N, 5)` where `N` is the number of ground-truth objects.\n            Each row is expected to be in\n            `(x_min, y_min, x_max, y_max, class)` format.\n        num_classes (int): Number of classes.\n        conf_threshold (float): Detection confidence threshold between `0` and `1`.\n            Detections with lower confidence will be excluded.\n        iou_threshold (float): Detection iou  threshold between `0` and `1`.\n            Detections with lower iou will be classified as `FP`.\n\n    Returns:\n        np.ndarray: Confusion matrix based on a single image.\n    \"\"\"\n    result_matrix = np.zeros((num_classes + 1, num_classes + 1))\n\n    conf_idx = 5\n    confidence = predictions[:, conf_idx]\n    detection_batch_filtered = predictions[confidence &gt; conf_threshold]\n\n    class_id_idx = 4\n    true_classes = np.array(targets[:, class_id_idx], dtype=np.int16)\n    detection_classes = np.array(\n        detection_batch_filtered[:, class_id_idx], dtype=np.int16\n    )\n    true_boxes = targets[:, :class_id_idx]\n    detection_boxes = detection_batch_filtered[:, :class_id_idx]\n\n    iou_batch = box_iou_batch(\n        boxes_true=true_boxes, boxes_detection=detection_boxes\n    )\n    matched_idx = np.asarray(iou_batch &gt; iou_threshold).nonzero()\n\n    if matched_idx[0].shape[0]:\n        matches = np.stack(\n            (matched_idx[0], matched_idx[1], iou_batch[matched_idx]), axis=1\n        )\n        matches = ConfusionMatrix._drop_extra_matches(matches=matches)\n    else:\n        matches = np.zeros((0, 3))\n\n    matched_true_idx, matched_detection_idx, _ = matches.transpose().astype(\n        np.int16\n    )\n\n    for i, true_class_value in enumerate(true_classes):\n        j = matched_true_idx == i\n        if matches.shape[0] &gt; 0 and sum(j) == 1:\n            result_matrix[\n                true_class_value, detection_classes[matched_detection_idx[j]]\n            ] += 1  # TP\n        else:\n            result_matrix[true_class_value, num_classes] += 1  # FN\n\n    for i, detection_class_value in enumerate(detection_classes):\n        if not any(matched_detection_idx == i):\n            result_matrix[num_classes, detection_class_value] += 1  # FP\n\n    return result_matrix\n</code></pre>"},{"location":"metrics/detection/#supervision.metrics.detection.ConfusionMatrix.from_detections","title":"<code>from_detections(predictions, targets, classes, conf_threshold=0.3, iou_threshold=0.5)</code>  <code>classmethod</code>","text":"<p>Calculate confusion matrix based on predicted and ground-truth detections.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>List[Detections]</code> <p>Detections objects from ground-truth.</p> required <code>predictions</code> <code>List[Detections]</code> <p>Detections objects predicted by the model.</p> required <code>classes</code> <code>List[str]</code> <p>Model class names.</p> required <code>conf_threshold</code> <code>float</code> <p>Detection confidence threshold between <code>0</code> and <code>1</code>. Detections with lower confidence will be excluded.</p> <code>0.3</code> <code>iou_threshold</code> <code>float</code> <p>Detection IoU threshold between <code>0</code> and <code>1</code>. Detections with lower IoU will be classified as <code>FP</code>.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>ConfusionMatrix</code> <code>ConfusionMatrix</code> <p>New instance of ConfusionMatrix.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; targets = [\n...     sv.Detections(...),\n...     sv.Detections(...)\n... ]\n\n&gt;&gt;&gt; predictions = [\n...     sv.Detections(...),\n...     sv.Detections(...)\n... ]\n\n&gt;&gt;&gt; confusion_matrix = sv.ConfusionMatrix.from_detections(\n...     predictions=predictions,\n...     targets=target,\n...     classes=['person', ...]\n... )\n\n&gt;&gt;&gt; confusion_matrix.matrix\narray([\n    [0., 0., 0., 0.],\n    [0., 1., 0., 1.],\n    [0., 1., 1., 0.],\n    [1., 1., 0., 0.]\n])\n</code></pre> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@classmethod\ndef from_detections(\n    cls,\n    predictions: List[Detections],\n    targets: List[Detections],\n    classes: List[str],\n    conf_threshold: float = 0.3,\n    iou_threshold: float = 0.5,\n) -&gt; ConfusionMatrix:\n    \"\"\"\n    Calculate confusion matrix based on predicted and ground-truth detections.\n\n    Args:\n        targets (List[Detections]): Detections objects from ground-truth.\n        predictions (List[Detections]): Detections objects predicted by the model.\n        classes (List[str]): Model class names.\n        conf_threshold (float): Detection confidence threshold between `0` and `1`.\n            Detections with lower confidence will be excluded.\n        iou_threshold (float): Detection IoU threshold between `0` and `1`.\n            Detections with lower IoU will be classified as `FP`.\n\n    Returns:\n        ConfusionMatrix: New instance of ConfusionMatrix.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; targets = [\n        ...     sv.Detections(...),\n        ...     sv.Detections(...)\n        ... ]\n\n        &gt;&gt;&gt; predictions = [\n        ...     sv.Detections(...),\n        ...     sv.Detections(...)\n        ... ]\n\n        &gt;&gt;&gt; confusion_matrix = sv.ConfusionMatrix.from_detections(\n        ...     predictions=predictions,\n        ...     targets=target,\n        ...     classes=['person', ...]\n        ... )\n\n        &gt;&gt;&gt; confusion_matrix.matrix\n        array([\n            [0., 0., 0., 0.],\n            [0., 1., 0., 1.],\n            [0., 1., 1., 0.],\n            [1., 1., 0., 0.]\n        ])\n        ```\n    \"\"\"\n\n    prediction_tensors = []\n    target_tensors = []\n    for prediction, target in zip(predictions, targets):\n        prediction_tensors.append(\n            detections_to_tensor(prediction, with_confidence=True)\n        )\n        target_tensors.append(detections_to_tensor(target, with_confidence=False))\n    return cls.from_tensors(\n        predictions=prediction_tensors,\n        targets=target_tensors,\n        classes=classes,\n        conf_threshold=conf_threshold,\n        iou_threshold=iou_threshold,\n    )\n</code></pre>"},{"location":"metrics/detection/#supervision.metrics.detection.ConfusionMatrix.from_tensors","title":"<code>from_tensors(predictions, targets, classes, conf_threshold=0.3, iou_threshold=0.5)</code>  <code>classmethod</code>","text":"<p>Calculate confusion matrix based on predicted and ground-truth detections.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[ndarray]</code> <p>Each element of the list describes a single image and has <code>shape = (M, 6)</code> where <code>M</code> is the number of detected objects. Each row is expected to be in <code>(x_min, y_min, x_max, y_max, class, conf)</code> format.</p> required <code>targets</code> <code>List[ndarray]</code> <p>Each element of the list describes a single image and has <code>shape = (N, 5)</code> where <code>N</code> is the number of ground-truth objects. Each row is expected to be in <code>(x_min, y_min, x_max, y_max, class)</code> format.</p> required <code>classes</code> <code>List[str]</code> <p>Model class names.</p> required <code>conf_threshold</code> <code>float</code> <p>Detection confidence threshold between <code>0</code> and <code>1</code>. Detections with lower confidence will be excluded.</p> <code>0.3</code> <code>iou_threshold</code> <code>float</code> <p>Detection iou  threshold between <code>0</code> and <code>1</code>. Detections with lower iou will be classified as <code>FP</code>.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>ConfusionMatrix</code> <code>ConfusionMatrix</code> <p>New instance of ConfusionMatrix.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; targets = (\n...     [\n...         array(\n...             [\n...                 [0.0, 0.0, 3.0, 3.0, 1],\n...                 [2.0, 2.0, 5.0, 5.0, 1],\n...                 [6.0, 1.0, 8.0, 3.0, 2],\n...             ]\n...         ),\n...         array([1.0, 1.0, 2.0, 2.0, 2]),\n...     ]\n... )\n\n&gt;&gt;&gt; predictions = [\n...     array(\n...         [\n...             [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n...             [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n...             [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n...             [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n...         ]\n...     ),\n...     array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n... ]\n\n&gt;&gt;&gt; confusion_matrix = sv.ConfusionMatrix.from_tensors(\n...     predictions=predictions,\n...     targets=targets,\n...     classes=['person', ...]\n... )\n\n&gt;&gt;&gt; confusion_matrix.matrix\narray([\n    [0., 0., 0., 0.],\n    [0., 1., 0., 1.],\n    [0., 1., 1., 0.],\n    [1., 1., 0., 0.]\n])\n</code></pre> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@classmethod\ndef from_tensors(\n    cls,\n    predictions: List[np.ndarray],\n    targets: List[np.ndarray],\n    classes: List[str],\n    conf_threshold: float = 0.3,\n    iou_threshold: float = 0.5,\n) -&gt; ConfusionMatrix:\n    \"\"\"\n    Calculate confusion matrix based on predicted and ground-truth detections.\n\n    Args:\n        predictions (List[np.ndarray]): Each element of the list describes a single\n            image and has `shape = (M, 6)` where `M` is the number of detected\n            objects. Each row is expected to be in\n            `(x_min, y_min, x_max, y_max, class, conf)` format.\n        targets (List[np.ndarray]): Each element of the list describes a single\n            image and has `shape = (N, 5)` where `N` is the number of\n            ground-truth objects. Each row is expected to be in\n            `(x_min, y_min, x_max, y_max, class)` format.\n        classes (List[str]): Model class names.\n        conf_threshold (float): Detection confidence threshold between `0` and `1`.\n            Detections with lower confidence will be excluded.\n        iou_threshold (float): Detection iou  threshold between `0` and `1`.\n            Detections with lower iou will be classified as `FP`.\n\n    Returns:\n        ConfusionMatrix: New instance of ConfusionMatrix.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; targets = (\n        ...     [\n        ...         array(\n        ...             [\n        ...                 [0.0, 0.0, 3.0, 3.0, 1],\n        ...                 [2.0, 2.0, 5.0, 5.0, 1],\n        ...                 [6.0, 1.0, 8.0, 3.0, 2],\n        ...             ]\n        ...         ),\n        ...         array([1.0, 1.0, 2.0, 2.0, 2]),\n        ...     ]\n        ... )\n\n        &gt;&gt;&gt; predictions = [\n        ...     array(\n        ...         [\n        ...             [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n        ...             [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n        ...             [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n        ...             [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n        ...         ]\n        ...     ),\n        ...     array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n        ... ]\n\n        &gt;&gt;&gt; confusion_matrix = sv.ConfusionMatrix.from_tensors(\n        ...     predictions=predictions,\n        ...     targets=targets,\n        ...     classes=['person', ...]\n        ... )\n\n        &gt;&gt;&gt; confusion_matrix.matrix\n        array([\n            [0., 0., 0., 0.],\n            [0., 1., 0., 1.],\n            [0., 1., 1., 0.],\n            [1., 1., 0., 0.]\n        ])\n        ```\n    \"\"\"\n    validate_input_tensors(predictions, targets)\n\n    num_classes = len(classes)\n    matrix = np.zeros((num_classes + 1, num_classes + 1))\n    for true_batch, detection_batch in zip(targets, predictions):\n        matrix += cls.evaluate_detection_batch(\n            predictions=detection_batch,\n            targets=true_batch,\n            num_classes=num_classes,\n            conf_threshold=conf_threshold,\n            iou_threshold=iou_threshold,\n        )\n    return cls(\n        matrix=matrix,\n        classes=classes,\n        conf_threshold=conf_threshold,\n        iou_threshold=iou_threshold,\n    )\n</code></pre>"},{"location":"metrics/detection/#supervision.metrics.detection.ConfusionMatrix.plot","title":"<code>plot(save_path=None, title=None, classes=None, normalize=False, fig_size=(12, 10))</code>","text":"<p>Create confusion matrix plot and save it at selected location.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>Optional[str]</code> <p>Path to save the plot. If not provided, plot will be displayed.</p> <code>None</code> <code>title</code> <code>Optional[str]</code> <p>Title of the plot.</p> <code>None</code> <code>classes</code> <code>Optional[List[str]]</code> <p>List of classes to be displayed on the plot. If not provided, all classes will be displayed.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the confusion matrix.</p> <code>False</code> <code>fig_size</code> <code>Tuple[int, int]</code> <p>Size of the plot.</p> <code>(12, 10)</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: Confusion matrix plot.</p> Source code in <code>supervision/metrics/detection.py</code> <pre><code>def plot(\n    self,\n    save_path: Optional[str] = None,\n    title: Optional[str] = None,\n    classes: Optional[List[str]] = None,\n    normalize: bool = False,\n    fig_size: Tuple[int, int] = (12, 10),\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Create confusion matrix plot and save it at selected location.\n\n    Args:\n        save_path (Optional[str]): Path to save the plot. If not provided,\n            plot will be displayed.\n        title (Optional[str]): Title of the plot.\n        classes (Optional[List[str]]): List of classes to be displayed on the plot.\n            If not provided, all classes will be displayed.\n        normalize (bool): If True, normalize the confusion matrix.\n        fig_size (Tuple[int, int]): Size of the plot.\n\n    Returns:\n        matplotlib.figure.Figure: Confusion matrix plot.\n    \"\"\"\n\n    array = self.matrix.copy()\n\n    if normalize:\n        eps = 1e-8\n        array = array / (array.sum(0).reshape(1, -1) + eps)\n\n    array[array &lt; 0.005] = np.nan\n\n    fig, ax = plt.subplots(figsize=fig_size, tight_layout=True, facecolor=\"white\")\n\n    class_names = classes if classes is not None else self.classes\n    use_labels_for_ticks = class_names is not None and (0 &lt; len(class_names) &lt; 99)\n    if use_labels_for_ticks:\n        x_tick_labels = class_names + [\"FN\"]\n        y_tick_labels = class_names + [\"FP\"]\n        num_ticks = len(x_tick_labels)\n    else:\n        x_tick_labels = None\n        y_tick_labels = None\n        num_ticks = len(array)\n    im = ax.imshow(array, cmap=\"Blues\")\n\n    cbar = ax.figure.colorbar(im, ax=ax)\n    cbar.mappable.set_clim(vmin=0, vmax=np.nanmax(array))\n\n    if x_tick_labels is None:\n        tick_interval = 2\n    else:\n        tick_interval = 1\n    ax.set_xticks(np.arange(0, num_ticks, tick_interval), labels=x_tick_labels)\n    ax.set_yticks(np.arange(0, num_ticks, tick_interval), labels=y_tick_labels)\n\n    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"default\")\n\n    labelsize = 10 if num_ticks &lt; 50 else 8\n    ax.tick_params(axis=\"both\", which=\"both\", labelsize=labelsize)\n\n    if num_ticks &lt; 30:\n        for i in range(array.shape[0]):\n            for j in range(array.shape[1]):\n                n_preds = array[i, j]\n                if not np.isnan(n_preds):\n                    ax.text(\n                        j,\n                        i,\n                        f\"{n_preds:.2f}\" if normalize else f\"{n_preds:.0f}\",\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"black\"\n                        if n_preds &lt; 0.5 * np.nanmax(array)\n                        else \"white\",\n                    )\n\n    if title:\n        ax.set_title(title, fontsize=20)\n\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"True\")\n    ax.set_facecolor(\"white\")\n    if save_path:\n        fig.savefig(\n            save_path, dpi=250, facecolor=fig.get_facecolor(), transparent=True\n        )\n    return fig\n</code></pre>"},{"location":"metrics/detection/#meanaverageprecision","title":"MeanAveragePrecision","text":"<p>Mean Average Precision for object detection tasks.</p> <p>Attributes:</p> Name Type Description <code>map50_95</code> <code>float</code> <p>Mean Average Precision (mAP) calculated over IoU thresholds ranging from <code>0.50</code> to <code>0.95</code> with a step size of <code>0.05</code>.</p> <code>map50</code> <code>float</code> <p>Mean Average Precision (mAP) calculated specifically at an IoU threshold of <code>0.50</code>.</p> <code>map75</code> <code>float</code> <p>Mean Average Precision (mAP) calculated specifically at an IoU threshold of <code>0.75</code>.</p> <code>per_class_ap50_95</code> <code>ndarray</code> <p>Average Precision (AP) values calculated over IoU thresholds ranging from <code>0.50</code> to <code>0.95</code> with a step size of <code>0.05</code>, provided for each individual class.</p> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@dataclass(frozen=True)\nclass MeanAveragePrecision:\n    \"\"\"\n    Mean Average Precision for object detection tasks.\n\n    Attributes:\n        map50_95 (float): Mean Average Precision (mAP) calculated over IoU thresholds\n            ranging from `0.50` to `0.95` with a step size of `0.05`.\n        map50 (float): Mean Average Precision (mAP) calculated specifically at\n            an IoU threshold of `0.50`.\n        map75 (float): Mean Average Precision (mAP) calculated specifically at\n            an IoU threshold of `0.75`.\n        per_class_ap50_95 (np.ndarray): Average Precision (AP) values calculated over\n            IoU thresholds ranging from `0.50` to `0.95` with a step size of `0.05`,\n            provided for each individual class.\n    \"\"\"\n\n    map50_95: float\n    map50: float\n    map75: float\n    per_class_ap50_95: np.ndarray\n\n    @classmethod\n    def from_detections(\n        cls,\n        predictions: List[Detections],\n        targets: List[Detections],\n    ) -&gt; MeanAveragePrecision:\n        \"\"\"\n        Calculate mean average precision based on predicted and ground-truth detections.\n\n        Args:\n            targets (List[Detections]): Detections objects from ground-truth.\n            predictions (List[Detections]): Detections objects predicted by the model.\n        Returns:\n            MeanAveragePrecision: New instance of ConfusionMatrix.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; targets = [\n            ...     sv.Detections(...),\n            ...     sv.Detections(...)\n            ... ]\n\n            &gt;&gt;&gt; predictions = [\n            ...     sv.Detections(...),\n            ...     sv.Detections(...)\n            ... ]\n\n            &gt;&gt;&gt; mean_average_precision = sv.MeanAveragePrecision.from_detections(\n            ...     predictions=predictions,\n            ...     targets=target,\n            ... )\n\n            &gt;&gt;&gt; mean_average_precison.map50_95\n            0.2899\n            ```\n        \"\"\"\n        prediction_tensors = []\n        target_tensors = []\n        for prediction, target in zip(predictions, targets):\n            prediction_tensors.append(\n                detections_to_tensor(prediction, with_confidence=True)\n            )\n            target_tensors.append(detections_to_tensor(target, with_confidence=False))\n        return cls.from_tensors(\n            predictions=prediction_tensors,\n            targets=target_tensors,\n        )\n\n    @classmethod\n    def benchmark(\n        cls,\n        dataset: DetectionDataset,\n        callback: Callable[[np.ndarray], Detections],\n    ) -&gt; MeanAveragePrecision:\n        \"\"\"\n        Calculate mean average precision from dataset and callback function.\n\n        Args:\n            dataset (DetectionDataset): Object detection dataset used for evaluation.\n            callback (Callable[[np.ndarray], Detections]): Function that takes\n                an image as input and returns Detections object.\n        Returns:\n            MeanAveragePrecision: New instance of MeanAveragePrecision.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n            &gt;&gt;&gt; from ultralytics import YOLO\n\n            &gt;&gt;&gt; dataset = sv.DetectionDataset.from_yolo(...)\n\n            &gt;&gt;&gt; model = YOLO(...)\n            &gt;&gt;&gt; def callback(image: np.ndarray) -&gt; sv.Detections:\n            ...     result = model(image)[0]\n            ...     return sv.Detections.from_ultralytics(result)\n\n            &gt;&gt;&gt; mean_average_precision = sv.MeanAveragePrecision.benchmark(\n            ...     dataset = dataset,\n            ...     callback = callback\n            ... )\n\n            &gt;&gt;&gt; mean_average_precision.map50_95\n            0.433\n            ```\n        \"\"\"\n        predictions, targets = [], []\n        for img_name, img in dataset.images.items():\n            predictions_batch = callback(img)\n            predictions.append(predictions_batch)\n            targets_batch = dataset.annotations[img_name]\n            targets.append(targets_batch)\n        return cls.from_detections(\n            predictions=predictions,\n            targets=targets,\n        )\n\n    @classmethod\n    def from_tensors(\n        cls,\n        predictions: List[np.ndarray],\n        targets: List[np.ndarray],\n    ) -&gt; MeanAveragePrecision:\n        \"\"\"\n        Calculate Mean Average Precision based on predicted and ground-truth\n            detections at different threshold.\n\n        Args:\n            predictions (List[np.ndarray]): Each element of the list describes\n                a single image and has `shape = (M, 6)` where `M` is\n                the number of detected objects. Each row is expected to be\n                in `(x_min, y_min, x_max, y_max, class, conf)` format.\n            targets (List[np.ndarray]): Each element of the list describes a single\n                image and has `shape = (N, 5)` where `N` is the\n                number of ground-truth objects. Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class)` format.\n        Returns:\n            MeanAveragePrecision: New instance of MeanAveragePrecision.\n\n        Example:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; targets = (\n            ...     [\n            ...         array(\n            ...             [\n            ...                 [0.0, 0.0, 3.0, 3.0, 1],\n            ...                 [2.0, 2.0, 5.0, 5.0, 1],\n            ...                 [6.0, 1.0, 8.0, 3.0, 2],\n            ...             ]\n            ...         ),\n            ...         array([1.0, 1.0, 2.0, 2.0, 2]),\n            ...     ]\n            ... )\n\n            &gt;&gt;&gt; predictions = [\n            ...     array(\n            ...         [\n            ...             [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n            ...             [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n            ...             [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n            ...             [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n            ...         ]\n            ...     ),\n            ...     array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n            ... ]\n\n            &gt;&gt;&gt; mean_average_precison = sv.MeanAveragePrecision.from_tensors(\n            ...     predictions=predictions,\n            ...     targets=targets,\n            ... )\n\n            &gt;&gt;&gt; mean_average_precison.map50_95\n            0.2899\n            ```\n        \"\"\"\n        validate_input_tensors(predictions, targets)\n        iou_thresholds = np.linspace(0.5, 0.95, 10)\n        stats = []\n\n        # Gather matching stats for predictions and targets\n        for true_objs, predicted_objs in zip(targets, predictions):\n            if predicted_objs.shape[0] == 0:\n                if true_objs.shape[0]:\n                    stats.append(\n                        (\n                            np.zeros((0, iou_thresholds.size), dtype=bool),\n                            *np.zeros((2, 0)),\n                            true_objs[:, 4],\n                        )\n                    )\n                continue\n\n            if true_objs.shape[0]:\n                matches = cls._match_detection_batch(\n                    predicted_objs, true_objs, iou_thresholds\n                )\n                stats.append(\n                    (\n                        matches,\n                        predicted_objs[:, 5],\n                        predicted_objs[:, 4],\n                        true_objs[:, 4],\n                    )\n                )\n\n        # Compute average precisions if any matches exist\n        if stats:\n            concatenated_stats = [np.concatenate(items, 0) for items in zip(*stats)]\n            average_precisions = cls._average_precisions_per_class(*concatenated_stats)\n            map50 = average_precisions[:, 0].mean()\n            map75 = average_precisions[:, 5].mean()\n            map50_95 = average_precisions.mean()\n        else:\n            map50, map75, map50_95 = 0, 0, 0\n            average_precisions = []\n\n        return cls(\n            map50_95=map50_95,\n            map50=map50,\n            map75=map75,\n            per_class_ap50_95=average_precisions,\n        )\n\n    @staticmethod\n    def compute_average_precision(recall: np.ndarray, precision: np.ndarray) -&gt; float:\n        \"\"\"\n        Compute the average precision using 101-point interpolation (COCO), given\n            the recall and precision curves.\n\n        Args:\n            recall (np.ndarray): The recall curve.\n            precision (np.ndarray): The precision curve.\n\n        Returns:\n            float: Average precision.\n        \"\"\"\n        extended_recall = np.concatenate(([0.0], recall, [1.0]))\n        extended_precision = np.concatenate(([1.0], precision, [0.0]))\n        max_accumulated_precision = np.flip(\n            np.maximum.accumulate(np.flip(extended_precision))\n        )\n        interpolated_recall_levels = np.linspace(0, 1, 101)\n        interpolated_precision = np.interp(\n            interpolated_recall_levels, extended_recall, max_accumulated_precision\n        )\n        average_precision = np.trapz(interpolated_precision, interpolated_recall_levels)\n        return average_precision\n\n    @staticmethod\n    def _match_detection_batch(\n        predictions: np.ndarray, targets: np.ndarray, iou_thresholds: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Match predictions with target labels based on IoU levels.\n\n        Args:\n            predictions (np.ndarray): Batch prediction. Describes a single image and\n                has `shape = (M, 6)` where `M` is the number of detected objects.\n                Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class, conf)` format.\n            targets (np.ndarray): Batch target labels. Describes a single image and\n                has `shape = (N, 5)` where `N` is the number of ground-truth objects.\n                Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class)` format.\n            iou_thresholds (np.ndarray): Array contains different IoU thresholds.\n\n        Returns:\n            np.ndarray: Matched prediction with target labels result.\n        \"\"\"\n        num_predictions, num_iou_levels = predictions.shape[0], iou_thresholds.shape[0]\n        correct = np.zeros((num_predictions, num_iou_levels), dtype=bool)\n        iou = box_iou_batch(targets[:, :4], predictions[:, :4])\n        correct_class = targets[:, 4:5] == predictions[:, 4]\n\n        for i, iou_level in enumerate(iou_thresholds):\n            matched_indices = np.where((iou &gt;= iou_level) &amp; correct_class)\n\n            if matched_indices[0].shape[0]:\n                combined_indices = np.stack(matched_indices, axis=1)\n                iou_values = iou[matched_indices][:, None]\n                matches = np.hstack([combined_indices, iou_values])\n\n                if matched_indices[0].shape[0] &gt; 1:\n                    matches = matches[matches[:, 2].argsort()[::-1]]\n                    matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n                    matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n\n                correct[matches[:, 1].astype(int), i] = True\n\n        return correct\n\n    @staticmethod\n    def _average_precisions_per_class(\n        matches: np.ndarray,\n        prediction_confidence: np.ndarray,\n        prediction_class_ids: np.ndarray,\n        true_class_ids: np.ndarray,\n        eps: float = 1e-16,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Compute the average precision, given the recall and precision curves.\n        Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n\n        Args:\n            matches (np.ndarray): True positives.\n            prediction_confidence (np.ndarray): Objectness value from 0-1.\n            prediction_class_ids (np.ndarray): Predicted object classes.\n            true_class_ids (np.ndarray): True object classes.\n            eps (float, optional): Small value to prevent division by zero.\n\n        Returns:\n            np.ndarray: Average precision for different IoU levels.\n        \"\"\"\n        sorted_indices = np.argsort(-prediction_confidence)\n        matches = matches[sorted_indices]\n        prediction_class_ids = prediction_class_ids[sorted_indices]\n\n        unique_classes, class_counts = np.unique(true_class_ids, return_counts=True)\n        num_classes = unique_classes.shape[0]\n\n        average_precisions = np.zeros((num_classes, matches.shape[1]))\n\n        for class_idx, class_id in enumerate(unique_classes):\n            is_class = prediction_class_ids == class_id\n            total_true = class_counts[class_idx]\n            total_prediction = is_class.sum()\n\n            if total_prediction == 0 or total_true == 0:\n                continue\n\n            false_positives = (1 - matches[is_class]).cumsum(0)\n            true_positives = matches[is_class].cumsum(0)\n            recall = true_positives / (total_true + eps)\n            precision = true_positives / (true_positives + false_positives)\n\n            for iou_level_idx in range(matches.shape[1]):\n                average_precisions[\n                    class_idx, iou_level_idx\n                ] = MeanAveragePrecision.compute_average_precision(\n                    recall[:, iou_level_idx], precision[:, iou_level_idx]\n                )\n\n        return average_precisions\n</code></pre>"},{"location":"metrics/detection/#supervision.metrics.detection.MeanAveragePrecision.benchmark","title":"<code>benchmark(dataset, callback)</code>  <code>classmethod</code>","text":"<p>Calculate mean average precision from dataset and callback function.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DetectionDataset</code> <p>Object detection dataset used for evaluation.</p> required <code>callback</code> <code>Callable[[ndarray], Detections]</code> <p>Function that takes an image as input and returns Detections object.</p> required <p>Returns:     MeanAveragePrecision: New instance of MeanAveragePrecision.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; dataset = sv.DetectionDataset.from_yolo(...)\n\n&gt;&gt;&gt; model = YOLO(...)\n&gt;&gt;&gt; def callback(image: np.ndarray) -&gt; sv.Detections:\n...     result = model(image)[0]\n...     return sv.Detections.from_ultralytics(result)\n\n&gt;&gt;&gt; mean_average_precision = sv.MeanAveragePrecision.benchmark(\n...     dataset = dataset,\n...     callback = callback\n... )\n\n&gt;&gt;&gt; mean_average_precision.map50_95\n0.433\n</code></pre> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@classmethod\ndef benchmark(\n    cls,\n    dataset: DetectionDataset,\n    callback: Callable[[np.ndarray], Detections],\n) -&gt; MeanAveragePrecision:\n    \"\"\"\n    Calculate mean average precision from dataset and callback function.\n\n    Args:\n        dataset (DetectionDataset): Object detection dataset used for evaluation.\n        callback (Callable[[np.ndarray], Detections]): Function that takes\n            an image as input and returns Detections object.\n    Returns:\n        MeanAveragePrecision: New instance of MeanAveragePrecision.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n        &gt;&gt;&gt; from ultralytics import YOLO\n\n        &gt;&gt;&gt; dataset = sv.DetectionDataset.from_yolo(...)\n\n        &gt;&gt;&gt; model = YOLO(...)\n        &gt;&gt;&gt; def callback(image: np.ndarray) -&gt; sv.Detections:\n        ...     result = model(image)[0]\n        ...     return sv.Detections.from_ultralytics(result)\n\n        &gt;&gt;&gt; mean_average_precision = sv.MeanAveragePrecision.benchmark(\n        ...     dataset = dataset,\n        ...     callback = callback\n        ... )\n\n        &gt;&gt;&gt; mean_average_precision.map50_95\n        0.433\n        ```\n    \"\"\"\n    predictions, targets = [], []\n    for img_name, img in dataset.images.items():\n        predictions_batch = callback(img)\n        predictions.append(predictions_batch)\n        targets_batch = dataset.annotations[img_name]\n        targets.append(targets_batch)\n    return cls.from_detections(\n        predictions=predictions,\n        targets=targets,\n    )\n</code></pre>"},{"location":"metrics/detection/#supervision.metrics.detection.MeanAveragePrecision.compute_average_precision","title":"<code>compute_average_precision(recall, precision)</code>  <code>staticmethod</code>","text":"<p>Compute the average precision using 101-point interpolation (COCO), given     the recall and precision curves.</p> <p>Parameters:</p> Name Type Description Default <code>recall</code> <code>ndarray</code> <p>The recall curve.</p> required <code>precision</code> <code>ndarray</code> <p>The precision curve.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Average precision.</p> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@staticmethod\ndef compute_average_precision(recall: np.ndarray, precision: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute the average precision using 101-point interpolation (COCO), given\n        the recall and precision curves.\n\n    Args:\n        recall (np.ndarray): The recall curve.\n        precision (np.ndarray): The precision curve.\n\n    Returns:\n        float: Average precision.\n    \"\"\"\n    extended_recall = np.concatenate(([0.0], recall, [1.0]))\n    extended_precision = np.concatenate(([1.0], precision, [0.0]))\n    max_accumulated_precision = np.flip(\n        np.maximum.accumulate(np.flip(extended_precision))\n    )\n    interpolated_recall_levels = np.linspace(0, 1, 101)\n    interpolated_precision = np.interp(\n        interpolated_recall_levels, extended_recall, max_accumulated_precision\n    )\n    average_precision = np.trapz(interpolated_precision, interpolated_recall_levels)\n    return average_precision\n</code></pre>"},{"location":"metrics/detection/#supervision.metrics.detection.MeanAveragePrecision.from_detections","title":"<code>from_detections(predictions, targets)</code>  <code>classmethod</code>","text":"<p>Calculate mean average precision based on predicted and ground-truth detections.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>List[Detections]</code> <p>Detections objects from ground-truth.</p> required <code>predictions</code> <code>List[Detections]</code> <p>Detections objects predicted by the model.</p> required <p>Returns:     MeanAveragePrecision: New instance of ConfusionMatrix.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; targets = [\n...     sv.Detections(...),\n...     sv.Detections(...)\n... ]\n\n&gt;&gt;&gt; predictions = [\n...     sv.Detections(...),\n...     sv.Detections(...)\n... ]\n\n&gt;&gt;&gt; mean_average_precision = sv.MeanAveragePrecision.from_detections(\n...     predictions=predictions,\n...     targets=target,\n... )\n\n&gt;&gt;&gt; mean_average_precison.map50_95\n0.2899\n</code></pre> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@classmethod\ndef from_detections(\n    cls,\n    predictions: List[Detections],\n    targets: List[Detections],\n) -&gt; MeanAveragePrecision:\n    \"\"\"\n    Calculate mean average precision based on predicted and ground-truth detections.\n\n    Args:\n        targets (List[Detections]): Detections objects from ground-truth.\n        predictions (List[Detections]): Detections objects predicted by the model.\n    Returns:\n        MeanAveragePrecision: New instance of ConfusionMatrix.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; targets = [\n        ...     sv.Detections(...),\n        ...     sv.Detections(...)\n        ... ]\n\n        &gt;&gt;&gt; predictions = [\n        ...     sv.Detections(...),\n        ...     sv.Detections(...)\n        ... ]\n\n        &gt;&gt;&gt; mean_average_precision = sv.MeanAveragePrecision.from_detections(\n        ...     predictions=predictions,\n        ...     targets=target,\n        ... )\n\n        &gt;&gt;&gt; mean_average_precison.map50_95\n        0.2899\n        ```\n    \"\"\"\n    prediction_tensors = []\n    target_tensors = []\n    for prediction, target in zip(predictions, targets):\n        prediction_tensors.append(\n            detections_to_tensor(prediction, with_confidence=True)\n        )\n        target_tensors.append(detections_to_tensor(target, with_confidence=False))\n    return cls.from_tensors(\n        predictions=prediction_tensors,\n        targets=target_tensors,\n    )\n</code></pre>"},{"location":"metrics/detection/#supervision.metrics.detection.MeanAveragePrecision.from_tensors","title":"<code>from_tensors(predictions, targets)</code>  <code>classmethod</code>","text":"<p>Calculate Mean Average Precision based on predicted and ground-truth     detections at different threshold.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[ndarray]</code> <p>Each element of the list describes a single image and has <code>shape = (M, 6)</code> where <code>M</code> is the number of detected objects. Each row is expected to be in <code>(x_min, y_min, x_max, y_max, class, conf)</code> format.</p> required <code>targets</code> <code>List[ndarray]</code> <p>Each element of the list describes a single image and has <code>shape = (N, 5)</code> where <code>N</code> is the number of ground-truth objects. Each row is expected to be in <code>(x_min, y_min, x_max, y_max, class)</code> format.</p> required <p>Returns:     MeanAveragePrecision: New instance of MeanAveragePrecision.</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; targets = (\n...     [\n...         array(\n...             [\n...                 [0.0, 0.0, 3.0, 3.0, 1],\n...                 [2.0, 2.0, 5.0, 5.0, 1],\n...                 [6.0, 1.0, 8.0, 3.0, 2],\n...             ]\n...         ),\n...         array([1.0, 1.0, 2.0, 2.0, 2]),\n...     ]\n... )\n\n&gt;&gt;&gt; predictions = [\n...     array(\n...         [\n...             [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n...             [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n...             [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n...             [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n...         ]\n...     ),\n...     array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n... ]\n\n&gt;&gt;&gt; mean_average_precison = sv.MeanAveragePrecision.from_tensors(\n...     predictions=predictions,\n...     targets=targets,\n... )\n\n&gt;&gt;&gt; mean_average_precison.map50_95\n0.2899\n</code></pre> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@classmethod\ndef from_tensors(\n    cls,\n    predictions: List[np.ndarray],\n    targets: List[np.ndarray],\n) -&gt; MeanAveragePrecision:\n    \"\"\"\n    Calculate Mean Average Precision based on predicted and ground-truth\n        detections at different threshold.\n\n    Args:\n        predictions (List[np.ndarray]): Each element of the list describes\n            a single image and has `shape = (M, 6)` where `M` is\n            the number of detected objects. Each row is expected to be\n            in `(x_min, y_min, x_max, y_max, class, conf)` format.\n        targets (List[np.ndarray]): Each element of the list describes a single\n            image and has `shape = (N, 5)` where `N` is the\n            number of ground-truth objects. Each row is expected to be in\n            `(x_min, y_min, x_max, y_max, class)` format.\n    Returns:\n        MeanAveragePrecision: New instance of MeanAveragePrecision.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; targets = (\n        ...     [\n        ...         array(\n        ...             [\n        ...                 [0.0, 0.0, 3.0, 3.0, 1],\n        ...                 [2.0, 2.0, 5.0, 5.0, 1],\n        ...                 [6.0, 1.0, 8.0, 3.0, 2],\n        ...             ]\n        ...         ),\n        ...         array([1.0, 1.0, 2.0, 2.0, 2]),\n        ...     ]\n        ... )\n\n        &gt;&gt;&gt; predictions = [\n        ...     array(\n        ...         [\n        ...             [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n        ...             [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n        ...             [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n        ...             [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n        ...         ]\n        ...     ),\n        ...     array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n        ... ]\n\n        &gt;&gt;&gt; mean_average_precison = sv.MeanAveragePrecision.from_tensors(\n        ...     predictions=predictions,\n        ...     targets=targets,\n        ... )\n\n        &gt;&gt;&gt; mean_average_precison.map50_95\n        0.2899\n        ```\n    \"\"\"\n    validate_input_tensors(predictions, targets)\n    iou_thresholds = np.linspace(0.5, 0.95, 10)\n    stats = []\n\n    # Gather matching stats for predictions and targets\n    for true_objs, predicted_objs in zip(targets, predictions):\n        if predicted_objs.shape[0] == 0:\n            if true_objs.shape[0]:\n                stats.append(\n                    (\n                        np.zeros((0, iou_thresholds.size), dtype=bool),\n                        *np.zeros((2, 0)),\n                        true_objs[:, 4],\n                    )\n                )\n            continue\n\n        if true_objs.shape[0]:\n            matches = cls._match_detection_batch(\n                predicted_objs, true_objs, iou_thresholds\n            )\n            stats.append(\n                (\n                    matches,\n                    predicted_objs[:, 5],\n                    predicted_objs[:, 4],\n                    true_objs[:, 4],\n                )\n            )\n\n    # Compute average precisions if any matches exist\n    if stats:\n        concatenated_stats = [np.concatenate(items, 0) for items in zip(*stats)]\n        average_precisions = cls._average_precisions_per_class(*concatenated_stats)\n        map50 = average_precisions[:, 0].mean()\n        map75 = average_precisions[:, 5].mean()\n        map50_95 = average_precisions.mean()\n    else:\n        map50, map75, map50_95 = 0, 0, 0\n        average_precisions = []\n\n    return cls(\n        map50_95=map50_95,\n        map50=map50,\n        map75=map75,\n        per_class_ap50_95=average_precisions,\n    )\n</code></pre>"},{"location":"utils/file/","title":"File","text":""},{"location":"utils/file/#list_files_with_extensions","title":"list_files_with_extensions","text":"<p>List files in a directory with specified extensions or     all files if no extensions are provided.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Union[str, Path]</code> <p>The directory path as a string or Path object.</p> required <code>extensions</code> <code>Optional[List[str]]</code> <p>A list of file extensions to filter. Default is None, which lists all files.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Path]</code> <p>A list of Path objects for the matching files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; # List all files in the directory\n&gt;&gt;&gt; files = sv.list_files_with_extensions(directory='my_directory')\n\n&gt;&gt;&gt; # List only files with '.txt' and '.md' extensions\n&gt;&gt;&gt; files = sv.list_files_with_extensions(\n...     directory='my_directory', extensions=['txt', 'md'])\n</code></pre> Source code in <code>supervision/utils/file.py</code> <pre><code>def list_files_with_extensions(\n    directory: Union[str, Path], extensions: Optional[List[str]] = None\n) -&gt; List[Path]:\n    \"\"\"\n    List files in a directory with specified extensions or\n        all files if no extensions are provided.\n\n    Args:\n        directory (Union[str, Path]): The directory path as a string or Path object.\n        extensions (Optional[List[str]]): A list of file extensions to filter.\n            Default is None, which lists all files.\n\n    Returns:\n        (List[Path]): A list of Path objects for the matching files.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; # List all files in the directory\n        &gt;&gt;&gt; files = sv.list_files_with_extensions(directory='my_directory')\n\n        &gt;&gt;&gt; # List only files with '.txt' and '.md' extensions\n        &gt;&gt;&gt; files = sv.list_files_with_extensions(\n        ...     directory='my_directory', extensions=['txt', 'md'])\n        ```\n    \"\"\"\n\n    directory = Path(directory)\n    files_with_extensions = []\n\n    if extensions is not None:\n        for ext in extensions:\n            files_with_extensions.extend(directory.glob(f\"*.{ext}\"))\n    else:\n        files_with_extensions.extend(directory.glob(\"*\"))\n\n    return files_with_extensions\n</code></pre>"},{"location":"utils/image/","title":"Image","text":""},{"location":"utils/image/#imagesink","title":"ImageSink","text":"Source code in <code>supervision/utils/image.py</code> <pre><code>class ImageSink:\n    def __init__(\n        self,\n        target_dir_path: str,\n        overwrite: bool = False,\n        image_name_pattern: str = \"image_{:05d}.png\",\n    ):\n        \"\"\"\n        Initialize a context manager for saving images.\n\n        Args:\n            target_dir_path (str): The target directory where images will be saved.\n            overwrite (bool, optional): Whether to overwrite the existing directory.\n                Defaults to False.\n            image_name_pattern (str, optional): The image file name pattern.\n                Defaults to \"image_{:05d}.png\".\n\n        Examples:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; with sv.ImageSink(target_dir_path='target/directory/path',\n            ...                   overwrite=True) as sink:\n            ...     for image in sv.get_video_frames_generator(\n            ...         source_path='source_video.mp4', stride=2):\n            ...         sink.save_image(image=image)\n            ```\n        \"\"\"\n\n        self.target_dir_path = target_dir_path\n        self.overwrite = overwrite\n        self.image_name_pattern = image_name_pattern\n        self.image_count = 0\n\n    def __enter__(self):\n        if os.path.exists(self.target_dir_path):\n            if self.overwrite:\n                shutil.rmtree(self.target_dir_path)\n                os.makedirs(self.target_dir_path)\n        else:\n            os.makedirs(self.target_dir_path)\n\n        return self\n\n    def save_image(self, image: np.ndarray, image_name: Optional[str] = None):\n        \"\"\"\n        Save a given image in the target directory.\n\n        Args:\n            image (np.ndarray): The image to be saved.\n            image_name (str, optional): The name to use for the saved image.\n                If not provided, a name will be\n                generated using the `image_name_pattern`.\n        \"\"\"\n        if image_name is None:\n            image_name = self.image_name_pattern.format(self.image_count)\n\n        image_path = os.path.join(self.target_dir_path, image_name)\n        cv2.imwrite(image_path, image)\n        self.image_count += 1\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        pass\n</code></pre>"},{"location":"utils/image/#supervision.utils.image.ImageSink.__init__","title":"<code>__init__(target_dir_path, overwrite=False, image_name_pattern='image_{:05d}.png')</code>","text":"<p>Initialize a context manager for saving images.</p> <p>Parameters:</p> Name Type Description Default <code>target_dir_path</code> <code>str</code> <p>The target directory where images will be saved.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the existing directory. Defaults to False.</p> <code>False</code> <code>image_name_pattern</code> <code>str</code> <p>The image file name pattern. Defaults to \"image_{:05d}.png\".</p> <code>'image_{:05d}.png'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; with sv.ImageSink(target_dir_path='target/directory/path',\n...                   overwrite=True) as sink:\n...     for image in sv.get_video_frames_generator(\n...         source_path='source_video.mp4', stride=2):\n...         sink.save_image(image=image)\n</code></pre> Source code in <code>supervision/utils/image.py</code> <pre><code>def __init__(\n    self,\n    target_dir_path: str,\n    overwrite: bool = False,\n    image_name_pattern: str = \"image_{:05d}.png\",\n):\n    \"\"\"\n    Initialize a context manager for saving images.\n\n    Args:\n        target_dir_path (str): The target directory where images will be saved.\n        overwrite (bool, optional): Whether to overwrite the existing directory.\n            Defaults to False.\n        image_name_pattern (str, optional): The image file name pattern.\n            Defaults to \"image_{:05d}.png\".\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; with sv.ImageSink(target_dir_path='target/directory/path',\n        ...                   overwrite=True) as sink:\n        ...     for image in sv.get_video_frames_generator(\n        ...         source_path='source_video.mp4', stride=2):\n        ...         sink.save_image(image=image)\n        ```\n    \"\"\"\n\n    self.target_dir_path = target_dir_path\n    self.overwrite = overwrite\n    self.image_name_pattern = image_name_pattern\n    self.image_count = 0\n</code></pre>"},{"location":"utils/image/#supervision.utils.image.ImageSink.save_image","title":"<code>save_image(image, image_name=None)</code>","text":"<p>Save a given image in the target directory.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The image to be saved.</p> required <code>image_name</code> <code>str</code> <p>The name to use for the saved image. If not provided, a name will be generated using the <code>image_name_pattern</code>.</p> <code>None</code> Source code in <code>supervision/utils/image.py</code> <pre><code>def save_image(self, image: np.ndarray, image_name: Optional[str] = None):\n    \"\"\"\n    Save a given image in the target directory.\n\n    Args:\n        image (np.ndarray): The image to be saved.\n        image_name (str, optional): The name to use for the saved image.\n            If not provided, a name will be\n            generated using the `image_name_pattern`.\n    \"\"\"\n    if image_name is None:\n        image_name = self.image_name_pattern.format(self.image_count)\n\n    image_path = os.path.join(self.target_dir_path, image_name)\n    cv2.imwrite(image_path, image)\n    self.image_count += 1\n</code></pre>"},{"location":"utils/image/#crop","title":"crop","text":"<p>Crops the given image based on the given bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The image to be cropped, represented as a numpy array.</p> required <code>xyxy</code> <code>ndarray</code> <p>A numpy array containing the bounding box coordinates in the format (x1, y1, x2, y2).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The cropped image as a numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; detection = sv.Detections(...)\n&gt;&gt;&gt; with sv.ImageSink(target_dir_path='target/directory/path') as sink:\n...     for xyxy in detection.xyxy:\n...         cropped_image = sv.crop_image(image=image, xyxy=xyxy)\n...         sink.save_image(image=image)\n</code></pre> Source code in <code>supervision/utils/image.py</code> <pre><code>def crop_image(image: np.ndarray, xyxy: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Crops the given image based on the given bounding box.\n\n    Args:\n        image (np.ndarray): The image to be cropped, represented as a numpy array.\n        xyxy (np.ndarray): A numpy array containing the bounding box coordinates\n            in the format (x1, y1, x2, y2).\n\n    Returns:\n        (np.ndarray): The cropped image as a numpy array.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; detection = sv.Detections(...)\n        &gt;&gt;&gt; with sv.ImageSink(target_dir_path='target/directory/path') as sink:\n        ...     for xyxy in detection.xyxy:\n        ...         cropped_image = sv.crop_image(image=image, xyxy=xyxy)\n        ...         sink.save_image(image=image)\n        ```\n    \"\"\"\n\n    xyxy = np.round(xyxy).astype(int)\n    x1, y1, x2, y2 = xyxy\n    cropped_img = image[y1:y2, x1:x2]\n    return cropped_img\n</code></pre>"},{"location":"utils/notebook/","title":"Notebook","text":""},{"location":"utils/notebook/#plot_image","title":"plot_image","text":"<p>Plots image using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The frame to be displayed.</p> required <code>size</code> <code>Tuple[int, int]</code> <p>The size of the plot.</p> <code>(12, 12)</code> <code>cmap</code> <code>str</code> <p>the colormap to use for single channel images.</p> <code>'gray'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = cv2.imread(\"path/to/image.jpg\")\n\n%matplotlib inline\n&gt;&gt;&gt; sv.plot_image(image=image, size=(16, 16))\n</code></pre> Source code in <code>supervision/utils/notebook.py</code> <pre><code>def plot_image(\n    image: np.ndarray, size: Tuple[int, int] = (12, 12), cmap: Optional[str] = \"gray\"\n) -&gt; None:\n    \"\"\"\n    Plots image using matplotlib.\n\n    Args:\n        image (np.ndarray): The frame to be displayed.\n        size (Tuple[int, int]): The size of the plot.\n        cmap (str): the colormap to use for single channel images.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import cv2\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image = cv2.imread(\"path/to/image.jpg\")\n\n        %matplotlib inline\n        &gt;&gt;&gt; sv.plot_image(image=image, size=(16, 16))\n        ```\n    \"\"\"\n    plt.figure(figsize=size)\n\n    if image.ndim == 2:\n        plt.imshow(image, cmap=cmap)\n    else:\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    plt.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"utils/notebook/#plot_images_grid","title":"plot_images_grid","text":"<p>Plots images in a grid using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[ndarray]</code> <p>A list of images as numpy arrays.</p> required <code>grid_size</code> <code>Tuple[int, int]</code> <p>A tuple specifying the number   of rows and columns for the grid.</p> required <code>titles</code> <code>Optional[List[str]]</code> <p>A list of titles for each image.   Defaults to None.</p> <code>None</code> <code>size</code> <code>Tuple[int, int]</code> <p>A tuple specifying the width and   height of the entire plot in inches.</p> <code>(12, 12)</code> <code>cmap</code> <code>str</code> <p>the colormap to use for single channel images.</p> <code>'gray'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of images exceeds the grid size.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image1 = cv2.imread(\"path/to/image1.jpg\")\n&gt;&gt;&gt; image2 = cv2.imread(\"path/to/image2.jpg\")\n&gt;&gt;&gt; image3 = cv2.imread(\"path/to/image3.jpg\")\n\n&gt;&gt;&gt; images = [image1, image2, image3]\n&gt;&gt;&gt; titles = [\"Image 1\", \"Image 2\", \"Image 3\"]\n\n%matplotlib inline\n&gt;&gt;&gt; plot_images_grid(images, grid_size=(2, 2), titles=titles, size=(16, 16))\n</code></pre> Source code in <code>supervision/utils/notebook.py</code> <pre><code>def plot_images_grid(\n    images: List[np.ndarray],\n    grid_size: Tuple[int, int],\n    titles: Optional[List[str]] = None,\n    size: Tuple[int, int] = (12, 12),\n    cmap: Optional[str] = \"gray\",\n) -&gt; None:\n    \"\"\"\n    Plots images in a grid using matplotlib.\n\n    Args:\n       images (List[np.ndarray]): A list of images as numpy arrays.\n       grid_size (Tuple[int, int]): A tuple specifying the number\n            of rows and columns for the grid.\n       titles (Optional[List[str]]): A list of titles for each image.\n            Defaults to None.\n       size (Tuple[int, int]): A tuple specifying the width and\n            height of the entire plot in inches.\n       cmap (str): the colormap to use for single channel images.\n\n    Raises:\n       ValueError: If the number of images exceeds the grid size.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import cv2\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; image1 = cv2.imread(\"path/to/image1.jpg\")\n        &gt;&gt;&gt; image2 = cv2.imread(\"path/to/image2.jpg\")\n        &gt;&gt;&gt; image3 = cv2.imread(\"path/to/image3.jpg\")\n\n        &gt;&gt;&gt; images = [image1, image2, image3]\n        &gt;&gt;&gt; titles = [\"Image 1\", \"Image 2\", \"Image 3\"]\n\n        %matplotlib inline\n        &gt;&gt;&gt; plot_images_grid(images, grid_size=(2, 2), titles=titles, size=(16, 16))\n        ```\n    \"\"\"\n    nrows, ncols = grid_size\n\n    if len(images) &gt; nrows * ncols:\n        raise ValueError(\n            \"The number of images exceeds the grid size. Please increase the grid size\"\n            \" or reduce the number of images.\"\n        )\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=size)\n\n    for idx, ax in enumerate(axes.flat):\n        if idx &lt; len(images):\n            if images[idx].ndim == 2:\n                ax.imshow(images[idx], cmap=cmap)\n            else:\n                ax.imshow(cv2.cvtColor(images[idx], cv2.COLOR_BGR2RGB))\n\n            if titles is not None and idx &lt; len(titles):\n                ax.set_title(titles[idx])\n\n        ax.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"utils/video/","title":"Video","text":""},{"location":"utils/video/#videoinfo","title":"VideoInfo","text":"<p>A class to store video information, including width, height, fps and     total number of frames.</p> <p>Attributes:</p> Name Type Description <code>width</code> <code>int</code> <p>width of the video in pixels</p> <code>height</code> <code>int</code> <p>height of the video in pixels</p> <code>fps</code> <code>int</code> <p>frames per second of the video</p> <code>total_frames</code> <code>int</code> <p>total number of frames in the video, default is None</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; video_info = sv.VideoInfo.from_video_path(video_path='video.mp4')\n\n&gt;&gt;&gt; video_info\nVideoInfo(width=3840, height=2160, fps=25, total_frames=538)\n\n&gt;&gt;&gt; video_info.resolution_wh\n(3840, 2160)\n</code></pre> Source code in <code>supervision/utils/video.py</code> <pre><code>@dataclass\nclass VideoInfo:\n    \"\"\"\n    A class to store video information, including width, height, fps and\n        total number of frames.\n\n    Attributes:\n        width (int): width of the video in pixels\n        height (int): height of the video in pixels\n        fps (int): frames per second of the video\n        total_frames (int, optional): total number of frames in the video,\n            default is None\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; video_info = sv.VideoInfo.from_video_path(video_path='video.mp4')\n\n        &gt;&gt;&gt; video_info\n        VideoInfo(width=3840, height=2160, fps=25, total_frames=538)\n\n        &gt;&gt;&gt; video_info.resolution_wh\n        (3840, 2160)\n        ```\n    \"\"\"\n\n    width: int\n    height: int\n    fps: int\n    total_frames: Optional[int] = None\n\n    @classmethod\n    def from_video_path(cls, video_path: str) -&gt; VideoInfo:\n        video = cv2.VideoCapture(video_path)\n        if not video.isOpened():\n            raise Exception(f\"Could not open video at {video_path}\")\n\n        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = int(video.get(cv2.CAP_PROP_FPS))\n        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n        video.release()\n        return VideoInfo(width, height, fps, total_frames)\n\n    @property\n    def resolution_wh(self) -&gt; Tuple[int, int]:\n        return self.width, self.height\n</code></pre>"},{"location":"utils/video/#videosink","title":"VideoSink","text":"<p>Context manager that saves video frames to a file using OpenCV.</p> <p>Attributes:</p> Name Type Description <code>target_path</code> <code>str</code> <p>The path to the output file where the video will be saved.</p> <code>video_info</code> <code>VideoInfo</code> <p>Information about the video resolution, fps, and total frame count.</p> <code>codec</code> <code>str</code> <p>FOURCC code for video format</p> Example <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; video_info = sv.VideoInfo.from_video_path('source.mp4')\n&gt;&gt;&gt; frames_generator = sv.get_video_frames_generator('source.mp4')\n\n&gt;&gt;&gt; with sv.VideoSink(target_path='target.mp4', video_info=video_info) as sink:\n...     for frame in frames_generator:\n...         sink.write_frame(frame=frame)\n</code></pre> Source code in <code>supervision/utils/video.py</code> <pre><code>class VideoSink:\n    \"\"\"\n    Context manager that saves video frames to a file using OpenCV.\n\n    Attributes:\n        target_path (str): The path to the output file where the video will be saved.\n        video_info (VideoInfo): Information about the video resolution, fps,\n            and total frame count.\n        codec (str): FOURCC code for video format\n\n    Example:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; video_info = sv.VideoInfo.from_video_path('source.mp4')\n        &gt;&gt;&gt; frames_generator = sv.get_video_frames_generator('source.mp4')\n\n        &gt;&gt;&gt; with sv.VideoSink(target_path='target.mp4', video_info=video_info) as sink:\n        ...     for frame in frames_generator:\n        ...         sink.write_frame(frame=frame)\n        ```\n    \"\"\"\n\n    def __init__(self, target_path: str, video_info: VideoInfo, codec: str = \"mp4v\"):\n        self.target_path = target_path\n        self.video_info = video_info\n        self.__codec = codec\n        self.__writer = None\n\n    def __enter__(self):\n        try:\n            self.__fourcc = cv2.VideoWriter_fourcc(*self.__codec)\n        except TypeError as e:\n            print(str(e) + \". Defaulting to mp4v...\")\n            self.__fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n        self.__writer = cv2.VideoWriter(\n            self.target_path,\n            self.__fourcc,\n            self.video_info.fps,\n            self.video_info.resolution_wh,\n        )\n        return self\n\n    def write_frame(self, frame: np.ndarray):\n        self.__writer.write(frame)\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        self.__writer.release()\n</code></pre>"},{"location":"utils/video/#fpsmonitor","title":"FPSMonitor","text":"<p>A class for monitoring frames per second (FPS) to benchmark latency.</p> Source code in <code>supervision/utils/video.py</code> <pre><code>class FPSMonitor:\n    \"\"\"\n    A class for monitoring frames per second (FPS) to benchmark latency.\n    \"\"\"\n\n    def __init__(self, sample_size: int = 30):\n        \"\"\"\n        Args:\n            sample_size (int): The maximum number of observations for latency\n                benchmarking.\n\n        Examples:\n            ```python\n            &gt;&gt;&gt; import supervision as sv\n\n            &gt;&gt;&gt; frames_generator = sv.get_video_frames_generator('source.mp4')\n            &gt;&gt;&gt; fps_monitor = sv.FPSMonitor()\n\n            &gt;&gt;&gt; for frame in frames_generator:\n            ...     # your processing code here\n            ...     fps_monitor.tick()\n            ...     fps = fps_monitor()\n            ```\n        \"\"\"\n        self.all_timestamps = deque(maxlen=sample_size)\n\n    def __call__(self) -&gt; float:\n        \"\"\"\n        Computes and returns the average FPS based on the stored time stamps.\n\n        Returns:\n            float: The average FPS. Returns 0.0 if no time stamps are stored.\n        \"\"\"\n\n        if not self.all_timestamps:\n            return 0.0\n        taken_time = self.all_timestamps[-1] - self.all_timestamps[0]\n        return (len(self.all_timestamps)) / taken_time if taken_time != 0 else 0.0\n\n    def tick(self) -&gt; None:\n        \"\"\"\n        Adds a new time stamp to the deque for FPS calculation.\n        \"\"\"\n        self.all_timestamps.append(time.monotonic())\n\n    def reset(self) -&gt; None:\n        \"\"\"\n        Clears all the time stamps from the deque.\n        \"\"\"\n        self.all_timestamps.clear()\n</code></pre>"},{"location":"utils/video/#supervision.utils.video.FPSMonitor.__call__","title":"<code>__call__()</code>","text":"<p>Computes and returns the average FPS based on the stored time stamps.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The average FPS. Returns 0.0 if no time stamps are stored.</p> Source code in <code>supervision/utils/video.py</code> <pre><code>def __call__(self) -&gt; float:\n    \"\"\"\n    Computes and returns the average FPS based on the stored time stamps.\n\n    Returns:\n        float: The average FPS. Returns 0.0 if no time stamps are stored.\n    \"\"\"\n\n    if not self.all_timestamps:\n        return 0.0\n    taken_time = self.all_timestamps[-1] - self.all_timestamps[0]\n    return (len(self.all_timestamps)) / taken_time if taken_time != 0 else 0.0\n</code></pre>"},{"location":"utils/video/#supervision.utils.video.FPSMonitor.__init__","title":"<code>__init__(sample_size=30)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>sample_size</code> <code>int</code> <p>The maximum number of observations for latency benchmarking.</p> <code>30</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; frames_generator = sv.get_video_frames_generator('source.mp4')\n&gt;&gt;&gt; fps_monitor = sv.FPSMonitor()\n\n&gt;&gt;&gt; for frame in frames_generator:\n...     # your processing code here\n...     fps_monitor.tick()\n...     fps = fps_monitor()\n</code></pre> Source code in <code>supervision/utils/video.py</code> <pre><code>def __init__(self, sample_size: int = 30):\n    \"\"\"\n    Args:\n        sample_size (int): The maximum number of observations for latency\n            benchmarking.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; frames_generator = sv.get_video_frames_generator('source.mp4')\n        &gt;&gt;&gt; fps_monitor = sv.FPSMonitor()\n\n        &gt;&gt;&gt; for frame in frames_generator:\n        ...     # your processing code here\n        ...     fps_monitor.tick()\n        ...     fps = fps_monitor()\n        ```\n    \"\"\"\n    self.all_timestamps = deque(maxlen=sample_size)\n</code></pre>"},{"location":"utils/video/#supervision.utils.video.FPSMonitor.reset","title":"<code>reset()</code>","text":"<p>Clears all the time stamps from the deque.</p> Source code in <code>supervision/utils/video.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"\n    Clears all the time stamps from the deque.\n    \"\"\"\n    self.all_timestamps.clear()\n</code></pre>"},{"location":"utils/video/#supervision.utils.video.FPSMonitor.tick","title":"<code>tick()</code>","text":"<p>Adds a new time stamp to the deque for FPS calculation.</p> Source code in <code>supervision/utils/video.py</code> <pre><code>def tick(self) -&gt; None:\n    \"\"\"\n    Adds a new time stamp to the deque for FPS calculation.\n    \"\"\"\n    self.all_timestamps.append(time.monotonic())\n</code></pre>"},{"location":"utils/video/#get_video_frames_generator","title":"get_video_frames_generator","text":"<p>Get a generator that yields the frames of the video.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>The path of the video file.</p> required <code>stride</code> <code>int</code> <p>Indicates the interval at which frames are returned, skipping stride - 1 frames between each.</p> <code>1</code> <code>start</code> <code>int</code> <p>Indicates the starting position from which video should generate frames</p> <code>0</code> <code>end</code> <code>Optional[int]</code> <p>Indicates the ending position at which video should stop generating frames. If None, video will be read to the end.</p> <code>None</code> <p>Returns:</p> Type Description <code>Generator[ndarray, None, None]</code> <p>A generator that yields the frames of the video.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; for frame in sv.get_video_frames_generator(source_path='source_video.mp4'):\n...     ...\n</code></pre> Source code in <code>supervision/utils/video.py</code> <pre><code>def get_video_frames_generator(\n    source_path: str, stride: int = 1, start: int = 0, end: Optional[int] = None\n) -&gt; Generator[np.ndarray, None, None]:\n    \"\"\"\n    Get a generator that yields the frames of the video.\n\n    Args:\n        source_path (str): The path of the video file.\n        stride (int): Indicates the interval at which frames are returned,\n            skipping stride - 1 frames between each.\n        start (int): Indicates the starting position from which\n            video should generate frames\n        end (Optional[int]): Indicates the ending position at which video\n            should stop generating frames. If None, video will be read to the end.\n\n    Returns:\n        (Generator[np.ndarray, None, None]): A generator that yields the\n            frames of the video.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; for frame in sv.get_video_frames_generator(source_path='source_video.mp4'):\n        ...     ...\n        ```\n    \"\"\"\n    video, start, end = _validate_and_setup_video(source_path, start, end)\n    frame_position = start\n    while True:\n        success, frame = video.read()\n        if not success or frame_position &gt;= end:\n            break\n        yield frame\n        for _ in range(stride - 1):\n            success = video.grab()\n            if not success:\n                break\n        frame_position += stride\n    video.release()\n</code></pre>"},{"location":"utils/video/#process_video","title":"process_video","text":"<p>Process a video file by applying a callback function on each frame     and saving the result to a target video file.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>The path to the source video file.</p> required <code>target_path</code> <code>str</code> <p>The path to the target video file.</p> required <code>callback</code> <code>Callable[[ndarray, int], ndarray]</code> <p>A function that takes in a numpy ndarray representation of a video frame and an int index of the frame and returns a processed numpy ndarray representation of the frame.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; def callback(scene: np.ndarray, index: int) -&gt; np.ndarray:\n...     ...\n\n&gt;&gt;&gt; process_video(\n...     source_path='...',\n...     target_path='...',\n...     callback=callback\n... )\n</code></pre> Source code in <code>supervision/utils/video.py</code> <pre><code>def process_video(\n    source_path: str,\n    target_path: str,\n    callback: Callable[[np.ndarray, int], np.ndarray],\n) -&gt; None:\n    \"\"\"\n    Process a video file by applying a callback function on each frame\n        and saving the result to a target video file.\n\n    Args:\n        source_path (str): The path to the source video file.\n        target_path (str): The path to the target video file.\n        callback (Callable[[np.ndarray, int], np.ndarray]): A function that takes in\n            a numpy ndarray representation of a video frame and an\n            int index of the frame and returns a processed numpy ndarray\n            representation of the frame.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import supervision as sv\n\n        &gt;&gt;&gt; def callback(scene: np.ndarray, index: int) -&gt; np.ndarray:\n        ...     ...\n\n        &gt;&gt;&gt; process_video(\n        ...     source_path='...',\n        ...     target_path='...',\n        ...     callback=callback\n        ... )\n        ```\n    \"\"\"\n    source_video_info = VideoInfo.from_video_path(video_path=source_path)\n    with VideoSink(target_path=target_path, video_info=source_video_info) as sink:\n        for index, frame in enumerate(\n            get_video_frames_generator(source_path=source_path)\n        ):\n            result_frame = callback(frame, index)\n            sink.write_frame(frame=result_frame)\n</code></pre>"}]}